{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Libraries and Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score\n",
    "\n",
    "\n",
    "def transformToList(inputString):\n",
    "    s = inputString.translate(str.maketrans({'{': '[', '}': ']', '(': '[', ')': ']'}))\n",
    "    s = s.replace('[', ' [ ')\n",
    "    s = s.replace(']', ' ] ')\n",
    "    s = s.replace(',', ' , ')\n",
    "    words = s.split()\n",
    "    output = \"\"\n",
    "    for word in words:\n",
    "        if word==\"[\" or word==\"]\" or word==\",\": output+=word\n",
    "        else: output+='\"'+word+'\"'\n",
    "    out = json.loads(output)\n",
    "    try:\n",
    "        a = np.array(out).astype(np.float).tolist()\n",
    "    except:\n",
    "        a = out\n",
    "    return a\n",
    "\n",
    "class Activation:\n",
    "    def stepFunction(u):\n",
    "        if u>0: return 1\n",
    "        elif u<0: return 0\n",
    "        else: return 0.5\n",
    "\n",
    "\n",
    "class Utility:\n",
    "    def augmentArray(a, value=1, position=0):\n",
    "        return np.insert(a, position, value, axis=len(a.shape)-1)\n",
    "    \n",
    "    def sampleNorm(data, target, mainClass = 1):\n",
    "        if len(data) != len(target):\n",
    "            print(\"incompatible array sizes - sampleNorm\")\n",
    "            return\n",
    "        for i in range(len(target)):\n",
    "            if target[i] != mainClass:\n",
    "                data[i] = [-x for x in data[i]]\n",
    "        return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Confusion Matrix and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Week1:\n",
    "    def basic_metrics(y_true, y_pred, class_names, normalize=False):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        cm = cm[:,::-1][::-1]\n",
    "        np.set_printoptions(precision=4)\n",
    "\n",
    "        title='Confusion matrix'\n",
    "        cmap=plt.cm.Blues\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names, rotation=45)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names[::-1],digits=4))\n",
    "        plt.show()\n",
    "        \n",
    "    def setParams():\n",
    "        class_names = transformToList(input(\"Enter names of classes in descending order eg [Two, One, Zero]:  \"))\n",
    "        y_true = transformToList(input(\"Enter true value of labels eg [1,1,0,1,0,1,1,2] 1D:  \"))\n",
    "        y_pred = transformToList(input(\"Enter predicted value of labels eg [1,0,1,1,0,1,0,2] 1D:  \"))\n",
    "        return class_names, y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class_names, y_true, y_pred = Week1.setParams()\n",
    "Week1.basic_metrics(y_true, y_pred, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Batch Perceptron Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BatchPerceptronLearning:\n",
    "    def fit(X, Y, a, n):\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Applying Sample Normalisation:\n",
    "        Norm_Y = []\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "\n",
    "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
    "            if y != 1:\n",
    "                x = [i * -1 for i in x]\n",
    "                x.insert(0, -1)\n",
    "                Norm_Y.append(x)\n",
    "            else:\n",
    "                x.insert(0, 1)\n",
    "                Norm_Y.append(x)\n",
    "\n",
    "        print(\"Vectors used in Batch Perceptron Learning Algorithm:\\n {}\\n\".format(Norm_Y))\n",
    "\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Batch Perceptron Learning Algorithm:\n",
    "\n",
    "        epoch = 1\n",
    "\n",
    "        while True:\n",
    "\n",
    "            updating_samples = []\n",
    "            print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "            for count, i in enumerate(range(len(Norm_Y))):\n",
    "\n",
    "                # Knowing which value of a to use. If it is the first iteration, than use the given parameters in the \n",
    "                # question:\n",
    "                a_prev = a\n",
    "                print(\"The value of a used is {}\".format(a_prev))\n",
    "                y_input = Norm_Y[i]\n",
    "                print(\"y Value used for this iteration is: {}\".format(y_input))\n",
    "\n",
    "                # Equation -> g(x) = a^{t}y\n",
    "                ay = np.dot(a, y_input)\n",
    "                print(\"The value of a^t*y for this iteration is: {}\".format(ay))\n",
    "\n",
    "\n",
    "                # Checking if the sample is misclassified or not:\n",
    "\n",
    "                # If sample is misclassified:\n",
    "                if ay <= 0:\n",
    "\n",
    "                    # If this is the first sample in the epoch, add the previous value of a to the list of samples used \n",
    "                    # for the update to perform summation at the end of the epoch:\n",
    "                    if count == 0:\n",
    "                        print(\"This sample is misclassified. This sample will be used in update.\\n\")\n",
    "                        updating_samples.append(np.array(a))\n",
    "                        updating_samples.append(np.array(y_input))\n",
    "\n",
    "                    # If sample is misclassified and IS NOT the first sample in the epoch:\n",
    "                    else:\n",
    "                        print(\"This sample is misclassified. This sample will be used in update.\\n\")\n",
    "                        updating_samples.append(np.array(y_input))\n",
    "\n",
    "                # If sample is classified correctly:\n",
    "                else: \n",
    "\n",
    "                    # If first sample in the epoch, append the previous value of a to the updating samples list:\n",
    "                    if count == 0:\n",
    "                        updating_samples.append(np.array(a))\n",
    "                        print(\"This sample is classified correctly.\\n\")\n",
    "                    else:\n",
    "                        print(\"This sample is classified correctly.\\n\")\n",
    "\n",
    "            # Calculating new value of a after having gone through all of the samples in the dataset since it is Batch Learning.\n",
    "            a_update_val = n * sum(updating_samples)\n",
    "\n",
    "            # If Block to check whether learning has converged. If we have gone through all the data without needing \n",
    "            # to update the parameters, we can conclude that learning has converged.\n",
    "            if len(updating_samples) <= 1:\n",
    "                print(\"\\nLearning has converged.\")\n",
    "                print(\"Required parameters of a are: {}\".format(a))\n",
    "                break\n",
    "\n",
    "            # Updating a using our new value of a:\n",
    "            a = a_update_val\n",
    "            print(\"\\nNew Value of a^t is: {}.\\n\".format(a))\n",
    "\n",
    "            epoch += 1\n",
    "        \n",
    "    def setParams():\n",
    "        X_train = transformToList(input(\"Enter features eg [[1, 5], [2, 5], [4, 1], [5, 1]] 2D:  \"))\n",
    "        y_train = transformToList(input(\"Enter labels eg [1, 1, 2, 2] 1D:  \"))\n",
    "        a = transformToList(input(\"Enter a. Usually [1, w1, w2...] eg [-25, 6, 3] 1D:  \"))\n",
    "        lr = float(input(\"Enter the learning rate eg 1.  \"))\n",
    "        return X_train, y_train, a, lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, a, lr = BatchPerceptronLearning.setParams()\n",
    "BatchPerceptronLearning.fit(X_train, y_train, a, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sequential Perceptron Learing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SequentialPerceptronLearning:\n",
    "    def fit(X, Y, a, n):\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Applying Sample Normalisation:\n",
    "        Norm_Y = []\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "\n",
    "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
    "            if y != 1:\n",
    "                x = [i * -1 for i in x]\n",
    "                x.insert(0, -1)\n",
    "                Norm_Y.append(x)\n",
    "            else:\n",
    "                x.insert(0, 1)\n",
    "                Norm_Y.append(x)\n",
    "\n",
    "        print(\"Vectors used in Sequential Perceptron Learning Algorithm:\\n {}\\n\".format(Norm_Y))\n",
    "\n",
    "\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Sequential Perceptron Learning Algorithm:\n",
    "\n",
    "        epoch = 1\n",
    "\n",
    "        while True:\n",
    "\n",
    "            updating_samples = []\n",
    "            print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "            # Keeping track of how many samples are correctly classified. If this variable reaches \n",
    "            # the value that is equal to the size of the dataset (len), than we know that learning \n",
    "            # has converged:\n",
    "            correctly_classified_counter = 0\n",
    "\n",
    "            # Going through all of the samples in the dataset one-by-one:\n",
    "            for i in range(len(Norm_Y)):\n",
    "\n",
    "                # This chooses which weight to use for an iteration. If first iteration, uses given starting weight \n",
    "                # as described in question:\n",
    "                a_prev = a\n",
    "                print(\"The value of a used is {}\".format(a_prev))\n",
    "\n",
    "                # Selecting sample to use:\n",
    "                y_input = Norm_Y[i]\n",
    "                print(\"y Value used for this iteration is: {}\".format(y_input))\n",
    "\n",
    "                # Equation -> g(x) = a^{t}y\n",
    "                ay = np.dot(a, y_input)\n",
    "                print(\"The value of a^t*y for this iteration is: {}\".format(ay))\n",
    "\n",
    "\n",
    "                # Checking if the sample is misclassified or not:\n",
    "\n",
    "                # If sample is misclassified:\n",
    "                if ay <= 0:\n",
    "\n",
    "                    print(\"This sample is misclassified. This sample will be used in update.\\n\")\n",
    "                    updating_samples.append(np.array(a))\n",
    "                    updating_samples.append(np.array(y_input))\n",
    "\n",
    "                    # Calculating new value of a using update rule for Sequential Perceptron Learning Algorithm:\n",
    "                    a_update_val = n * sum(updating_samples)\n",
    "\n",
    "                    a = a_update_val\n",
    "                    print(\"\\nNew Value of a^t is: {}.\\n\".format(a))\n",
    "\n",
    "                # If the sample is correctly classified:\n",
    "                else: \n",
    "                    print(\"This sample is classified correctly.\\n\")\n",
    "                    correctly_classified_counter += 1\n",
    "                    pass\n",
    "\n",
    "                # Reset sample to add for update to occur:\n",
    "                updating_samples = []\n",
    "\n",
    "            # If Block to check whether learning has converged. If we have gone through all the data without needing \n",
    "            # to update the parameters, we can conclude that learning has converged.\n",
    "            if correctly_classified_counter == len(Norm_Y):\n",
    "                print(\"\\nLearning has converged.\")\n",
    "                print(\"Required parameters of a are: {}.\".format(a))\n",
    "                break\n",
    "\n",
    "            epoch += 1\n",
    "        \n",
    "    def setParams():\n",
    "        X_train = transformToList(input(\"Enter features eg [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]] 2D:  \"))\n",
    "        y_train = transformToList(input(\"Enter labels eg [1, 1, 1, -1, -1, -1] 1D:  \"))\n",
    "        a = transformToList(input(\"Enter a. Usually [1, w1, w2...] eg [1, 0, 0] 1D:  \"))\n",
    "        lr = float(input(\"Enter the learning rate eg 1.  \"))\n",
    "        return X_train, y_train, a, lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, a, lr = SequentialPerceptronLearning.setParams()\n",
    "SequentialPerceptronLearning.fit(X_train, y_train, a, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### MultiClass perceptron learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SequentialMultiClassPerceptronLearning:\n",
    "    def fit(X,y, a, eta):\n",
    "    # N is the number of exemplars provided in the question\n",
    "    # augmented_matrix is the augmented feature vector from the question\n",
    "    # eta is the learning rate given in the question\n",
    "    # omega is an array containing all the output classes of the feature vectors\n",
    "        augmented_matrix = np.array(X).T\n",
    "    # counter which keeps track of cases where winner_class == omega[index]\n",
    "        N_counter = 0\n",
    "        \n",
    "        omega = np.array(y).astype(int).tolist()\n",
    "        N = augmented_matrix.shape[1]\n",
    "        number_of_classes = len(set(omega))\n",
    "        number_of_features = augmented_matrix.shape[0]\n",
    "        \n",
    "        print(f'class nb:{number_of_classes} and number of features {number_of_features} and number of examplar {N}')\n",
    "        # Step 2. Initialise aj for each class\n",
    "        at = np.array(a)\n",
    "        #at = np.zeros((number_of_classes, number_of_features))\n",
    "        \n",
    "        for i in range(0, 15):\n",
    "            print('Iteration: ', i+1)\n",
    "            # Step 3. Find values of g1, g2 and g3 and then select the arg max of g\n",
    "            index = i % N\n",
    "\n",
    "            # Print updated a^t value\n",
    "            print('a^t:')\n",
    "            print(at)\n",
    "\n",
    "            # Compute g value\n",
    "            g = np.empty([number_of_classes])\n",
    "            for i in range(len(g)):\n",
    "                print('Calculation of g values..........')\n",
    "                print('a^t is:', at[i])\n",
    "                print('Index is:', index)\n",
    "                print('Aug matrix is:', augmented_matrix[:, index])\n",
    "                g[i] = at[i] @ augmented_matrix[:, index]\n",
    "\n",
    "            print('g1 | g2 | g3')\n",
    "            print(g)\n",
    "\n",
    "            # Step 4. Select the winner\n",
    "            # Logic for 0,0,0 case and similar ones where 2 gs can produce max value\n",
    "            seen = []\n",
    "            bRepeated = False\n",
    "            # Check if there are multiple max values, and assign the winner class accordingly\n",
    "            for number in g:\n",
    "                if number in seen:\n",
    "                    bRepeated = True\n",
    "                    print(\"Number repeated!\")\n",
    "                    m = max(g)\n",
    "                    temp = [index for index, j in enumerate(g) if j == m]\n",
    "                    winner_class = max(temp) + 1\n",
    "                else:\n",
    "                    seen.append(number)\n",
    "            # If all g values are unique, simply select the max value's class as the winner\n",
    "            if(bRepeated == False):\n",
    "                g = g.tolist()\n",
    "                arg_max = max(g)\n",
    "                winner_class = g.index(arg_max) + 1\n",
    "\n",
    "            print('Winner class = ', winner_class,\n",
    "                  ', and actual class is:', omega[index])\n",
    "\n",
    "            # Compare winnner to actual class\n",
    "            if(winner_class != omega[index]):\n",
    "                # Step 4. Apply the update rule as per the algorithm\n",
    "\n",
    "                # Increment the actual class value which is incorrectly classified\n",
    "                at[omega[index]-1] = at[omega[index]-1] + \\\n",
    "                    eta * augmented_matrix[:, index]\n",
    "                print('New loser value:', at[omega[index]-1])\n",
    "\n",
    "                # Penalize the wrongly predicted Winner class\n",
    "                at[winner_class-1] = at[winner_class-1] - \\\n",
    "                    eta * augmented_matrix[:, index]\n",
    "                print('New winner value:', at[winner_class-1])\n",
    "\n",
    "                # Reset counter to 0\n",
    "                N_counter = 0\n",
    "            else:\n",
    "                print('No update is performed!')\n",
    "                # Increment convergence counter which keeps track of cases where winner_class == omega[index]\n",
    "                N_counter += 1\n",
    "                if(N_counter == N):  # check for convergence\n",
    "                    print('Value of N = ', N)\n",
    "                    print('Value of N_counter = ', N_counter)\n",
    "                    print('Learning has converged, so stopping...')\n",
    "                    print('Final values of a^t after update....')\n",
    "                    print('at')\n",
    "                    print(at)\n",
    "                    break\n",
    "                print('N counter value = ', N_counter)\n",
    "            print('at')\n",
    "            print(at)\n",
    "            print('=========================================================')\n",
    "\n",
    "\n",
    "    def setParams():\n",
    "        X_train = transformToList(input(\"Enter augmented features (1 at first index) eg [[1,0, 2], [1, 1, 2], [1, 2, 1], [1, -3, 1], [1, -2, -1], [1, -3, -2]] 2D:  \"))\n",
    "        y_train = transformToList(input(\"Enter labels eg [1, 1, 1, -1, -1, -1] 1D:  \"))\n",
    "        a = transformToList(input(\"Enter a eg [[0, 0, 0],[0,0,0],[0,0,0]] 2D with the number of discriminant function equal to the number of classes:  \"))\n",
    "        lr = float(input(\"Enter the learning rate eg 1.  \"))\n",
    "        print(X_train, y_train, a, lr)\n",
    "        return X_train, y_train, a, lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, a, lr = SequentialMultiClassPerceptronLearning.setParams()\n",
    "SequentialMultiClassPerceptronLearning.fit(X_train, y_train, a, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Moore-Penrose Pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Pseudoinverse:\n",
    "    def fit(X, Y, b):\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Applying Sample Normalisation:\n",
    "        Norm_Y = []\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "\n",
    "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
    "            if y == -1 or y == 2:\n",
    "                x = [i * -1 for i in x]\n",
    "                x.insert(0, y)\n",
    "                Norm_Y.append(x)\n",
    "            else:\n",
    "                x.insert(0, y)\n",
    "                Norm_Y.append(x)\n",
    "\n",
    "        print(\"Vectors used in Pseudoinverse operation to calculate parameters of linear discriminant function:\\n {}\\n\".format(Norm_Y))\n",
    "\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Initialising Y Matrix:\n",
    "        Y_matrix = []\n",
    "\n",
    "        # Adding each normalised sample in dataset to Y Matrix:\n",
    "        for i in range(len(Norm_Y)):\n",
    "            Y_matrix.append(Norm_Y[i])\n",
    "        Y_matrix = np.array(Y_matrix)\n",
    "        print(\"y Matrix being used:\\n {}\\n\".format(Y_matrix))\n",
    "\n",
    "        # Calculating pseudo-inverse of Y Matrix:\n",
    "        pseudo_inv_matrix = np.linalg.pinv(Y_matrix)\n",
    "        print(\"Pseudo-inverse Matrix is:\\n {}\\n\".format(pseudo_inv_matrix))\n",
    "\n",
    "        # Multiplying Pseudo-inverse matrix by given margin vector in question:\n",
    "        a = np.matmul(pseudo_inv_matrix, b)\n",
    "        print(\"a is equal to:\\n {}\\n\".format(a))\n",
    "\n",
    "        correct_classification = 0\n",
    "\n",
    "        # Checking if classifications are correct:\n",
    "\n",
    "        for sample in Norm_Y:\n",
    "            ay = np.dot(sample, a)\n",
    "            print(\"\\ng(x) for sample {} is {}\".format(sample, ay))\n",
    "\n",
    "            # Sample is correctly classified if ay is positive:    \n",
    "            if ay > 0:\n",
    "                print(\"Sample has been correctly classified.\")\n",
    "                correct_classification += 1\n",
    "\n",
    "        if correct_classification == len(Norm_Y):\n",
    "            print(\"\\nAll samples are classified correctly which means that discriminant function parameters are correct.\")\n",
    "\n",
    "        else:\n",
    "            print(\"\\nSome samples are misclassified.\")\n",
    "        \n",
    "    def setParams():\n",
    "        X_train = transformToList(input(\"Enter features eg [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]] 2D:  \"))\n",
    "        y_train = transformToList(input(\"Enter labels eg [1, 1, 1, -1, -1, -1] 1D:  \"))\n",
    "        b = transformToList(input(\"Enter b eg [1, 1, 1, 1, 1, 1] 1D:  \"))\n",
    "        return X_train, y_train, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, b = Pseudoinverse.setParams()\n",
    "Pseudoinverse.fit(X_train, y_train, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sequential Widrow Hoff Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SequentialWidrowHoff:\n",
    "    def fit(X, Y, a, b, n, iterations):\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Applying Sample Normalisation:\n",
    "        Norm_Y = []\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "\n",
    "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
    "            if y == -1 or y == 2:\n",
    "                x = [i * -1 for i in x]\n",
    "                x.insert(0, y)\n",
    "                Norm_Y.append(x)\n",
    "            else:\n",
    "                x.insert(0, y)\n",
    "                Norm_Y.append(x)\n",
    "\n",
    "        print(\"Vectors used in Sequential Widrow-Hoff Learning Algorithm:\\n {}\\n\".format(Norm_Y))\n",
    "\n",
    "\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Sequential Widrow-Hoff Learning Algorithm\n",
    "\n",
    "        # Epoch for-loop:\n",
    "        for o in range(int(iterations / len(Norm_Y))):\n",
    "\n",
    "            # This for-loop goes through each sample one-by-one:\n",
    "            for i in range(len(Norm_Y)):\n",
    "\n",
    "                # Value of a to use. If first iteration, then uses parameters given in question:\n",
    "\n",
    "                a_prev = a\n",
    "\n",
    "                # Which sample to use:\n",
    "                y_input = Norm_Y[i]\n",
    "                print(\"Sample used for this iteration is: {}\".format(y_input))\n",
    "\n",
    "                # Equation -> g(x) = a^{t}y\n",
    "                ay = np.dot(a, y_input)\n",
    "                print(\"g(x) = {}\".format(ay))\n",
    "\n",
    "                # Calculating the values for update:\n",
    "                update = np.zeros(len(y_input))\n",
    "                for j in range(len(y_input)): \n",
    "\n",
    "                    # Applying Update Rule of Sequential Widrow-Hoff Learning Algorithm:\n",
    "                    update[j] = n * (b[i] - ay) * y_input[j]\n",
    "\n",
    "                # Adding update to a:\n",
    "                a = np.add(a, update)\n",
    "                print(\"New Value of a^t is: {}\\n\".format(a))\n",
    "\n",
    "        print(\"Gone through all of the iterations as asked for in question.\")\n",
    "        \n",
    "    def setParams():\n",
    "        X_train = transformToList(input(\"Enter features eg [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]] 2D:  \"))\n",
    "        y_train = transformToList(input(\"Enter labels eg [1, 1, 1, -1, -1, -1] 1D:  \"))\n",
    "        a = transformToList(input(\"Enter a. Usually [1, w1, w2...] eg [1, 0, 0] 1D:  \"))\n",
    "        b = transformToList(input(\"Enter b eg [1, 0.5, 1.5, 1.5, 1.5, 1] 1D:  \"))\n",
    "        lr = float(input(\"Enter the learning rate eg 0.1:  \"))\n",
    "        epochs = int(input(\"Enter the number of epochs eg 12:  \"))\n",
    "        return X_train, y_train, a, b, lr, epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, a, b, lr, epochs = SequentialWidrowHoff.setParams()\n",
    "SequentialWidrowHoff.fit(X_train, y_train, a, b, lr, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Neuron Output (with heavy side function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NeuronOutput:\n",
    "    def fit(weight, threshold, x):\n",
    "        summation = []\n",
    "        for i in range(len(x)):\n",
    "            summation.append(weight[i] * x[i])\n",
    "\n",
    "        summation = np.sum(summation, 0) - threshold\n",
    "\n",
    "        # Find output of neuron by applying heaviside function with given threshold:\n",
    "        output = np.heaviside(summation, threshold)\n",
    "        print(\"Output of neuron with input {} is {}.\".format(x, output))\n",
    "        \n",
    "    def setParams():\n",
    "        #This script is based off of Question 2 in Tutorial 3\n",
    "        x = transformToList(input(\"Enter single sample/input eg [0.1, -0.5, 0.4] 1D:  \"))\n",
    "        weights = transformToList(input(\"Enter weigths from one layer to another eg [0.1, -5, 0.4] 1D:  \"))\n",
    "        threshold = float(input(\"Enter the threshold value eg 0:  \"))\n",
    "        return weights, threshold, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights, threshold, x = NeuronOutput.setParams()\n",
    "NeuronOutput.fit(weights, threshold, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Batch and Sequential Delta Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DeltaLearningModel:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, features, target, weights, threshold, lr, epochs, type = \"S\", display=True):\n",
    "        x = Utility.augmentArray(features.astype(np.float), 1)\n",
    "        self.w = Utility.augmentArray(weights.astype(np.float), -threshold)\n",
    "        t = target.astype(np.float)\n",
    "        print(\"Score Before Training : \", round(self.score(features, target)*100, 1), \"%\")\n",
    "        \n",
    "        count = 0\n",
    "        for e in range(0,epochs):\n",
    "            error=0\n",
    "            if count>=epochs: break;\n",
    "            for i in range(0,len(x)):\n",
    "                y = Activation.stepFunction(x[i].dot(self.w))\n",
    "                error += (t[i]-y)*x[i]\n",
    "                \n",
    "                if type.upper()==\"S\":\n",
    "                    self.w = self.w+lr*error\n",
    "                    if display:\n",
    "                        count+=1\n",
    "                        print(count, \"\\t H(wx) = \", round(y,4), \"\\t delta w or n(t-y)x = \", np.round(lr*error,4), \"\\t w = \", np.round(self.w,4))\n",
    "                    error=0\n",
    "            if type.upper()==\"B\":\n",
    "                count+=1\n",
    "                self.w = self.w+lr*error\n",
    "                if display:\n",
    "                    print(count, \"\\t Weight change = \", np.round(error,4), \"\\t w = \", np.round(self.w,4))\n",
    "                \n",
    "                \n",
    "        \n",
    "        print(\"Score After Training : \", round(self.score(features, target)*100, 1), \"%\")\n",
    "        print(\"Final Weights (w) = \", self.w)\n",
    "        \n",
    "    def predict(self, X_values):\n",
    "        x = Utility.augmentArray(X_values.astype(np.float), 1)\n",
    "        myfunc = lambda t: Activation.stepFunction(t.dot(self.w))\n",
    "        return np.apply_along_axis(myfunc, 1, x)\n",
    "    \n",
    "    def score(self, X_test, y_target):\n",
    "        y_test = self.predict(X_test)\n",
    "        return np.sum(y_test == y_target)/len(y_target)\n",
    "    \n",
    "    def setParams(self):\n",
    "        batchOrSeq = str(input(\"'s' for sequential or 'b' for batch.  \"))\n",
    "        X_train = np.array(transformToList(input(\"Enter features eg [[0, 0], [0, 1], [1, 0], [1, 1]] 2D:  \"))).astype(np.float)\n",
    "        y_train = np.array(transformToList(input(\"Enter labels eg [0, 0, 0, 1] 1D:  \"))).astype(np.float)\n",
    "        initWeights = np.array(transformToList(input(\"Enter weights [w1, w2, ..., wd] eg [1, 1] 1D:  \"))).astype(np.float)\n",
    "        threshold = float(input(\"Enter the threshold value eg -0.5.  \"))\n",
    "        learningRate = float(input(\"Enter the learning rate eg 1.  \"))\n",
    "        epochs = int(input(\"Enter the number of epochs eg 4.  \"))\n",
    "        return X_train, y_train, initWeights, threshold, learningRate, epochs, batchOrSeq\n",
    "    \n",
    "    def newPred(self):\n",
    "        ispredict = str(input(\"\\nDo you want to predict using trained model? y/n.  \"))\n",
    "        while ispredict.upper()==\"Y\":\n",
    "            x_pred = np.array(transformToList(input(\"Enter features eg [[0, 0], [0, 1], [1, 0], [1, 1]] 2D:  \")))\n",
    "            print(\"Predicted values: \", self.predict(x_pred))\n",
    "            ispredict = str(input(\"\\nDo you want to predict using trained model? y/n.  \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = DeltaLearningModel()\n",
    "X_train, y_train, initWeights, threshold, learningRate, epochs, batchOrSeq = model.setParams()\n",
    "model.fit(X_train, y_train, initWeights, threshold, learningRate, epochs, batchOrSeq)\n",
    "model.newPred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Softmax (For Competitive Learning Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def fit(x, b=1):\n",
    "        print(\"\\nSoftmax of array: \", x, \"\\n\")\n",
    "        print(np.exp(x*b) / np.sum(np.exp(x*b), axis=0))\n",
    "    \n",
    "    def setParams():\n",
    "        a = np.array(transformToList(input(\"Enter array eg [0.34, 0.73, -0.61] 1D:  \"))).astype(np.float)\n",
    "        beta = float(input(\"Enter Beta value (1 if not specified).  \"))\n",
    "        return a, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a, beta = Softmax.setParams()\n",
    "Softmax.fit(a, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Negative Feedback Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NegativeFeddbackNetwork:\n",
    "    def fit(weights, iterations, x, alpha, activations):\n",
    "        prev_activations = activations\n",
    "        iteration = 1\n",
    "\n",
    "        for i in range(iterations):\n",
    "\n",
    "            print(\"Iteration {}\".format(iteration))\n",
    "\n",
    "            # Following block deals with calculating first equation: e = x - W^{T}y\n",
    "            wT = np.array(weights).T\n",
    "            wTy = np.dot(wT, activations)\n",
    "            print(\"value of wTy {}\".format(wTy))\n",
    "\n",
    "            eT = x - wTy\n",
    "            print(\"eT: {}\".format(eT))\n",
    "            e = np.array(eT).reshape((eT.shape[0], 1))\n",
    "\n",
    "            # The following lines deal with calculating the update: y <- y + \\alpha*W*e\n",
    "            We = np.dot(weights, e)\n",
    "            We = [j for i in We for j in i]\n",
    "            print(\"We: {} \".format(We))\n",
    "\n",
    "            alphaWe = np.dot(alpha, We)\n",
    "\n",
    "            # Doing the actual update using the second equation:\n",
    "            y = activations + alphaWe\n",
    "            print(\"Value of y: {}\".format(y))\n",
    "\n",
    "            activations = y\n",
    "\n",
    "            \n",
    "            wTy = np.dot(wT, activations)\n",
    "            print(\"value of wTy with new y{}\\n\".format(wTy))\n",
    "            iteration += 1\n",
    "\n",
    "        print(\"\\nAfter {} iterations, the activation of the output neurons is equal to {}\".format(iterations, activations))\n",
    "        \n",
    "    def setParams():\n",
    "        print(\"This script is based off of Question 7 in Tutorial 3\")\n",
    "        x = transformToList(input(\"Enter single sample/input eg [1, 1, 0] 1D:  \"))\n",
    "        weights = transformToList(input(\"Enter weigths from one layer to another eg [[1, 1, 0], [1, 1, 1]] 2D:  \"))\n",
    "        activation = transformToList(input(\"Enter output layer initial activation eg [0, 0] 1D:  \"))\n",
    "        lr = float(input(\"Enter the learning rate/alpha eg 0.25.  \"))\n",
    "        epochs = int(input(\"Enter the number of epochs eg 5.  \"))\n",
    "        return weights, epochs, x, lr, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights, epochs, x, lr, activation = NegativeFeddbackNetwork.setParams()\n",
    "NegativeFeddbackNetwork.fit(weights, epochs, x, lr, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Radial Basis Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --------------------------------INPUT--------------------------------#\n",
    "X = np.array(\n",
    "    [\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [1, 0],\n",
    "        [1, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "Y = np.array([0, 1, 1, 0])\n",
    "\n",
    "C = np.array(\n",
    "    [\n",
    "        [0, 0],\n",
    "        [1, 1],\n",
    "    ]\n",
    ")\n",
    "# ----------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "# ---------------------------------TEST---------------------------------#\n",
    "X_predict = np.array(\n",
    "    [\n",
    "        [0.5, -0.1],\n",
    "        [-0.2, 1.2],\n",
    "        [0.8, 0.3],\n",
    "        [1.8, 0.6]\n",
    "    ]\n",
    ")\n",
    "# ----------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "# --------------Calculate rho and sigma for radial basis---------------#\n",
    "center_dist = []\n",
    "for i in range(0, len(C) - 1):\n",
    "    for j in range(i + 1, len(C)):\n",
    "        print(i, j)\n",
    "        center_dist.append(np.sqrt(np.sum((C[i, :] - C[j, :]) ** 2)))\n",
    "\n",
    "rho_max = np.max(center_dist)\n",
    "rho_avg = np.average(center_dist)\n",
    "nH = len(C)\n",
    "print(\"Rho max: \", rho_max)\n",
    "print(\"Rho average: \", rho_avg)\n",
    "\n",
    "sigma = rho_max / np.sqrt(2 * nH)       # Using rho-max\n",
    "# sigma = 2 * rho_avg                   # Using rho-average\n",
    "print(\"Sigma: \", sigma)\n",
    "\n",
    "\n",
    "# -----------------------------CALCULATE OUTPUTS---------------------------#\n",
    "\n",
    "def get_hidden_output(X, C, sigma):\n",
    "    radial_basis_output = []\n",
    "    for i in range(0, len(X)):\n",
    "        hidden_node_outputs = []\n",
    "        for j in range(0, len(C)):\n",
    "            # Using Gaussian function\n",
    "            hidden_node_outputs.append(np.exp(-np.sum((X[i] - C[j]) ** 2)) / (2 * sigma * sigma))\n",
    "        radial_basis_output.append(hidden_node_outputs)\n",
    "\n",
    "    print(\"Radial basis layer output: \")\n",
    "    print(np.round(radial_basis_output, 2))\n",
    "    return radial_basis_output\n",
    "\n",
    "# Get output from hidden rbf layer\n",
    "radial_basis_output = get_hidden_output(X, C, sigma)\n",
    "\n",
    "# Add bias to hidden layer output\n",
    "radial_basis_output = np.c_[radial_basis_output, np.ones(len(radial_basis_output))]\n",
    "radial_basis_output_transposed = np.transpose(radial_basis_output)\n",
    "\n",
    "# Least squares method to calculate weights\n",
    "weights = np.dot(\n",
    "    np.dot(\n",
    "        np.linalg.inv(\n",
    "            np.dot(radial_basis_output_transposed, radial_basis_output)\n",
    "        ), radial_basis_output_transposed\n",
    "    ), Y\n",
    ")\n",
    "print(\"Weights between hidden-output layer: \", np.round(weights, 2))\n",
    "\n",
    "# Get output of final layer\n",
    "calculated_output = np.dot(radial_basis_output, weights)\n",
    "print(\"Output of network: \", np.round(calculated_output, 2))\n",
    "\n",
    "# Apply basic sign function with 0.5 threshold or call any other activation function\n",
    "final_output = np.where(calculated_output > 0.5, 1, 0)\n",
    "print(\"Signed output of network: \", final_output)\n",
    "\n",
    "# ---------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "# ------------------------------PREDICT TEST OUTPUT---------------------------#\n",
    "\n",
    "# Get output of hidden rbf layer\n",
    "hidden_output = get_hidden_output(X_predict, C, sigma)\n",
    "\n",
    "# Get output of final output layer\n",
    "predicted_output = np.dot(np.c_[hidden_output, np.ones(len(hidden_output))], weights)\n",
    "print(\"Output of test samples: \", np.round(predicted_output, 2))\n",
    "\n",
    "test_output = np.where(predicted_output > 0.5, 1, 0)\n",
    "print(\"Signed Output of test samples: \", test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Multi Layer NN - Feedforward and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "class MLN():\n",
    "    \"\"\" \n",
    "        MLN: This stands for Multi layer Network of Neurons. \n",
    "        This MLN classifier is modular. So it is not built with any set\n",
    "        amount of layers or any specific activation function in mind.\n",
    "        When an MLN object is created you can add any amount of layers\n",
    "        and any activation function implimented (Sigmoid, Tanh and Softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_seed=0):\n",
    "        \"\"\" \n",
    "            Initalizing the class simply sets a random seed so that we can get \n",
    "            consistent values when trying to tweak our hyper parameters\n",
    "            it also initalizes all the lists that we will use for our MLN\n",
    "            I decided to have every node have its own bias rather than share one with \n",
    "            the entire layer, This should result in better performace.\n",
    "            \n",
    "            Parameters:\n",
    "            random_seed (int): seed value for numpy random.\n",
    "        \n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        self.layersInfo = []        # Stores the size and activation of all layers.\n",
    "        self.layersValue = []       # Stores the output of all layers\n",
    "        self.weights = []           # Stores the weights of all connections\n",
    "        self.biases = []            # Stores the biases of all nodes\n",
    "        self.deltaWeights = []      # Stores the change of the weights when learning\n",
    "        self.deltaBiases = []       # Stores the change of the biases when learning\n",
    "        self.deltaNet = []          # Stores delta of the activation output (net)\n",
    "        self.deltaActivation = []   # Stores delta of the activation input\n",
    "    \n",
    "    def addLayer(self, size, activation, weights=None, bias=None, input_size=0):\n",
    "        \"\"\"\n",
    "            This funtion allows for any number of layers to be added to the netowrk.\n",
    "            Calling this fucntion once adds one layer. It does this by populating our lists\n",
    "            eg. layerInfo, layerValue, weights and biases. weights and biases are randomized\n",
    "            between -1 and 1 while layerValue is set to 0 for all nodes in the layer.\n",
    "            \n",
    "            Parameters:\n",
    "            size (int): the number of nodes in this layer.\n",
    "            activation (string): the type of activation function. (\"sigmoid\", \"tanh\", \"softmax\")\n",
    "            input_size (int): the size of input coming into the layer. Needed for first layer only.\n",
    "        \"\"\"\n",
    "        if input_size == 0:\n",
    "            input_size = self.layersInfo[-1][0]\n",
    "\n",
    "        self.layersInfo.append([size, activation])\n",
    "        self.layersValue.append(np.zeros(size))\n",
    "        \n",
    "        if weights == None:\n",
    "            self.weights.append(np.random.uniform(-1,1,(size,input_size)))\n",
    "        else:\n",
    "            self.weights.append(np.array(weights))\n",
    "        if bias == None:\n",
    "            self.biases.append(np.random.uniform(-1, 1, size))\n",
    "        else:\n",
    "            self.biases.append(np.array(bias))\n",
    "\n",
    "\n",
    "    def oneHotEncode(self, a):\n",
    "        \"\"\"\n",
    "            Simple function takes an array and performs \n",
    "            one hot ecoding on it.\n",
    "            \n",
    "            Parameters:\n",
    "            a (list/array): a list or np.array to be encoded\n",
    "            \n",
    "            Returns:\n",
    "            b (array): one hot encoded array of a\n",
    "        \"\"\"\n",
    "        b = np.zeros((a.size, a.max()+1))\n",
    "        b[np.arange(a.size),a] = 1\n",
    "        return b\n",
    "\n",
    "    def logSigmoid(self, u):\n",
    "        \"\"\"\n",
    "            Simple function takes an array and performs sigmoid \n",
    "            function on all numbers in array.\n",
    "            \n",
    "            Parameters:\n",
    "            u (list/array): a list or np.array to be tranformed\n",
    "            \n",
    "            Returns:\n",
    "            (array): sigmoid transformation of u\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-u))\n",
    "    \n",
    "    def symmetricSigmoid(self, u):\n",
    "        return (2.0 / (1.0 + np.exp(-2*u))) - 1\n",
    "    \n",
    "    def gradSymmetricSigmoid(self, u):\n",
    "        return (4.0 * np.exp(-2*u)) / (1.0 + np.exp(-2*u))**2\n",
    "\n",
    "    def gradLogSigmoid(self, u):\n",
    "        \"\"\"\n",
    "            Simple function takes an array and finds sigmoid gradient \n",
    "            of all numbers in array.\n",
    "            \n",
    "            Parameters:\n",
    "            x (list/array): a list or np.array to be tranformed\n",
    "            \n",
    "            Returns:\n",
    "            dt (array): sigmoid gradient transformation of x\n",
    "        \"\"\"\n",
    "        return u*(1-u)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "            Simple function takes an array and performs tanh \n",
    "            function on all numbers in array.\n",
    "            \n",
    "            Parameters:\n",
    "            x (list/array): a list or np.array to be tranformed\n",
    "            \n",
    "            Returns:\n",
    "            (array): tanh transformation of x\n",
    "        \"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def gradTanh(self, x):\n",
    "        \"\"\"\n",
    "            Simple function takes an array and finds tanh gradient \n",
    "            of all numbers in array.\n",
    "            \n",
    "            Parameters:\n",
    "            x (list/array): a list or np.array to be tranformed\n",
    "            \n",
    "            Returns:\n",
    "            dt (array): tanh gradient transformation of x\n",
    "        \"\"\"\n",
    "        t = self.tanh(x)\n",
    "        dt=1-t**2\n",
    "        return dt\n",
    "\n",
    "    def softmax(self, u):\n",
    "        \"\"\"\n",
    "            Simple function takes an array and performs softmax \n",
    "            function on all numbers in array.\n",
    "            \n",
    "            Parameters:\n",
    "            u (list/array): a list or np.array to be tranformed\n",
    "            \n",
    "            Returns:\n",
    "            (array): sofmax transformation of u\n",
    "        \"\"\"\n",
    "        e = np.exp(u - np.max(u))\n",
    "        return e / e.sum(axis=0)\n",
    "\n",
    "    def cross_entropy(self, t, y):\n",
    "        \"\"\"\n",
    "            Cross entropy loss function, used because it is \n",
    "            a multicalssification problem\n",
    "            \n",
    "            Parameters:\n",
    "            t (list/array): the expected ouput\n",
    "            y (list/array): the predicted ouput\n",
    "            \n",
    "            Returns:\n",
    "            error (array): cross enthropy loss values for all classes\n",
    "        \"\"\"\n",
    "        error=np.multiply(y,t)\n",
    "        error=np.mean(-np.log(error[error!=0]))\n",
    "        return error\n",
    "    \n",
    "    def mse(self,t, y):\n",
    "        return mean_squared_error(t, y)\n",
    "        \n",
    "    def forwardPass(self, x, printPass=True):\n",
    "        \"\"\"\n",
    "            This performs a single forward pass through the network\n",
    "            It will update the layer values which are stored in layersValue\n",
    "            \n",
    "            Paramters:\n",
    "            x (list/array):the input to run through our network\n",
    "        \"\"\"\n",
    "        if printPass: \n",
    "            print(\"Forward Pass: \")\n",
    "            inputlayerprint=[len(x), \"linear\"]\n",
    "            print(inputlayerprint, x)\n",
    "        for i in range(len(self.layersInfo)):\n",
    "            if i != 0: x = self.layersValue[i-1]\n",
    "            \n",
    "            net = self.weights[i].dot(x)+self.biases[i]\n",
    "\n",
    "            if self.layersInfo[i][1].lower() == \"logsigmoid\":\n",
    "                self.layersValue[i] = self.logSigmoid(net)\n",
    "            elif self.layersInfo[i][1].lower() == \"symmetricsigmoid\":\n",
    "                self.layersValue[i] = self.symmetricSigmoid(net)\n",
    "            elif self.layersInfo[i][1].lower() == \"tanh\":\n",
    "                self.layersValue[i] = self.tanh(net)\n",
    "            elif self.layersInfo[i][1].lower() == \"linear\":\n",
    "                self.layersValue[i] = net\n",
    "            elif self.layersInfo[i][1].lower() == \"softmax\":\n",
    "                self.layersValue[i] = self.softmax(net)\n",
    "            \n",
    "            if printPass: print(self.layersInfo[i], self.layersValue[i])\n",
    "        if printPass: print(\"\\n\")\n",
    "\n",
    "    def backpropagation(self, x, y):\n",
    "        \"\"\"\n",
    "            This performs iterative backpropagation through the network\n",
    "            This will only perform one full step of backpropagation but will not update\n",
    "            the weights, rather the change for the weights and biases will be stored \n",
    "            in deltaWeights and deltaBiases.\n",
    "            \n",
    "            Parameters:\n",
    "            x (list/array): the input value of the last foward pass\n",
    "            y (list/array): one hot encoded list of expected output for x\n",
    "        \"\"\"\n",
    "        noLayers = len(self.layersValue)-1\n",
    "        self.deltaNet[noLayers] = (self.layersValue[noLayers] - y)\n",
    "        for k in range(noLayers, -1, -1):\n",
    "            if k == 0: prevValue = x\n",
    "            else: prevValue = self.layersValue[k-1]\n",
    "\n",
    "            self.deltaWeights[k] += prevValue * self.deltaNet[k][:, None]\n",
    "            self.deltaBiases[k] += self.deltaNet[k]\n",
    "            \n",
    "            if k != 0:\n",
    "                self.deltaActivation[k-1] = self.deltaNet[k][:,None] * self.weights[k]\n",
    "                if self.layersInfo[k-1][1].lower() == \"tanh\":\n",
    "                    gradAactivation = self.gradTanh(self.layersValue[k-1])\n",
    "                elif self.layersInfo[k-1][1].lower() == \"symmetricsigmoid\":\n",
    "                    gradAactivation = self.gradSymmetricSigmoid(self.layersValue[k-1])\n",
    "                elif self.layersInfo[k-1][1].lower() == \"logsigmoid\":\n",
    "                    gradAactivation = self.gradLogSigmoid(self.layersValue[k-1])\n",
    "                self.deltaNet[k-1] = np.sum(np.multiply(self.deltaActivation[k-1], gradAactivation), axis = 0)\n",
    "        print(\"Delta weights: \", self.deltaWeights)\n",
    "        print(\"Delta biases: \", self.deltaBiases)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def resetDeltaWeights(self):\n",
    "        \"\"\"\n",
    "            This resets all the values of all delta lists to 0\n",
    "            because they will need to be reset after every weight and bias \n",
    "            update.\n",
    "        \"\"\"\n",
    "        self.deltaWeights = []\n",
    "        self.deltaBiases = []\n",
    "        self.deltaNet = []\n",
    "        self.deltaActivation = []\n",
    "        for i in self.weights:\n",
    "            self.deltaWeights.append(np.zeros(i.shape))\n",
    "        for i in self.biases:\n",
    "            self.deltaBiases.append(np.zeros(i.shape))\n",
    "        for i in self.layersValue:\n",
    "            self.deltaNet.append(np.zeros(i.shape))\n",
    "            self.deltaActivation.append(np.zeros(i.shape))\n",
    "\n",
    "    def fit(self, X_train, y_train, lr, epochs, batch=True, oneHot=False):\n",
    "        \"\"\"\n",
    "            This function is what is called to perform learning on the network.\n",
    "            It iteratively calls forward pass then backpropagation. Once backproagation gives the \n",
    "            changes to weights and biases we can update them either using batch or stacastic\n",
    "            gradient decent.\n",
    "            \n",
    "            There is learning rate decay implimented, if the accuracy has not changed by more than 1% then\n",
    "            it most likely means we are bouncing around within a loss minima so we reduce the learing rate\n",
    "            to take smaller steps in the gradient decent. This saves alot of time by reducing the amount of epochs\n",
    "            needed to reach the minima.\n",
    "            \n",
    "            There is early stopping implimented, which is performed using the accuracy metric.\n",
    "            \n",
    "            Parameters:\n",
    "            X_train (np.array): Data to train Network on\n",
    "            y_train (np.array): labels of X_train data\n",
    "            lr (float): the learning rate of the network\n",
    "            epochs (int): the number of iterations to learn for\n",
    "            batch (bool): if true then use batch gradient decent to learn\n",
    "                          if false use stocastic gradient decent to learn\n",
    "            printStats (bool): if true print network statistics while learning\n",
    "            checkFreq (int): how often should we print stats and how often should we check for early stopping criteria\n",
    "            earlyStopAcc (int): number of consecutive same accuracy value before early stopping is executed\n",
    "                                eg if earlyStopAcc = 4 then if we get 80 accuracy 4 times in a row then stop learning\n",
    "            \n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        if oneHot:\n",
    "            Y = self.oneHotEncode(y_train)\n",
    "        else:\n",
    "            Y = y_train\n",
    "        X = X_train\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.resetDeltaWeights()\n",
    "            for x, y in zip(X, Y):\n",
    "                self.forwardPass(x)\n",
    "                self.backpropagation(x, y)\n",
    "                if not batch:\n",
    "                    self.weights = list(np.array(self.weights) - lr*np.array(self.deltaWeights))\n",
    "                    self.biases = list(np.array(self.biases) - lr*np.array(self.deltaBiases))\n",
    "                    self.resetDeltaWeights()\n",
    "            if batch:\n",
    "                self.weights = list(np.array(self.weights) - lr*np.array(self.deltaWeights)/len(X))\n",
    "                self.biases = list(np.array(self.biases) - lr*np.array(self.deltaBiases)/len(X))\n",
    "\n",
    "            \n",
    "            if oneHot:\n",
    "                loss = self.cross_entropy(Y, self.predict(X, oneHot))\n",
    "            else:\n",
    "                loss = self.mse(Y, self.predict(X, oneHot))\n",
    "                \n",
    "            if oneHot:\n",
    "                acc = self.accuracy(X, y_train)\n",
    "            else:\n",
    "                acc = 0\n",
    "            print (\"Epoch: \", epoch, \"Loss: \", round(loss, 5), \"accuracy: \",  round(acc*100, 5), \"%\")\n",
    "            \n",
    "\n",
    "        end = time.time()\n",
    "        print (\"Time to train network (seconds): \", round(end - start, 3))\n",
    "\n",
    "    def predict(self, X_pred, oneHot=False, printPass=False):\n",
    "        \"\"\"\n",
    "            This funtion takes a list of lists and returns the \n",
    "            predicted output for all of them.\n",
    "            \n",
    "            Parameters:\n",
    "            X_pred (list of lists): a 2D list which you want to feed to the network to get the predictions\n",
    "            onHot (bool): if True the it will return the prediction as a one hot encoding\n",
    "            \n",
    "            Returns:\n",
    "            (list of lists): predicted output of X_pred\n",
    "        \"\"\"\n",
    "        pred=[]\n",
    "        if printPass: print(\"\\n\\nPrediction:\")\n",
    "        for x in X_pred:\n",
    "            self.forwardPass(x, printPass)\n",
    "            pred.append(self.layersValue[-1])\n",
    "        if not oneHot: return pred\n",
    "        return np.argmax(pred, axis=1)\n",
    "\n",
    "    def accuracy(self, X_pred, t):\n",
    "        \"\"\"\n",
    "            Simply returns the accuracy of the network\n",
    "            \n",
    "            Parameters:\n",
    "            X_pred (list of lists): a 2D list which you want to feed to the network to get the predictions\n",
    "            t (list/array): target outcomes of X_pred (not one hot encoded)\n",
    "            \n",
    "            Returns: \n",
    "            (float): X_pred accuracy of the model\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X_pred)\n",
    "        return accuracy_score(t, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run Feedforward + Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = [[0.1, 0.9]]\n",
    "target = [0.5]\n",
    "\n",
    "outputNo = 2\n",
    "noFeatures = 4\n",
    "\n",
    "lr = 0.25\n",
    "epoch = 1\n",
    "\n",
    "model = MLN()\n",
    "model.addLayer(3, \"symmetricsigmoid\",weights=[[0.5, 0], [0.3, -0.7]], bias=[0.2, 0], input_size=noFeatures)\n",
    "model.addLayer(outputNo, \"symmetricsigmoid\",weights=[[0.8, 1.6]], bias=[-0.4])\n",
    "model.fit(data, target, lr, epoch, batch=False)\n",
    "# model.predict(data, printPass=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run Feedforward only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = [[1,0,1,0],[1,1,1,0]] # Example of a single 4D sample to predict\n",
    "target = [1, 0] # Its target class is 1.\n",
    "\n",
    "outputNo = 2 # Nodes in the output layer\n",
    "noFeatures = 4 # Features in the sample \n",
    "\n",
    "### This example is a 4-3-2 #input-hidden-out\n",
    "model = MLN() # Create a NN\n",
    "\n",
    "# [-0.7057, 0.49, 0.9438], [4.9061, 1.9324, -5.4160], [2.6605, -0.4269, -0.3431], [-1.1359, -5.1570, -0.2931]]\n",
    "# [[-0.7057, 4.9061, 2.6605, -1.1359],[0.49, 1.9324, -0.4269, -5.1570],[0.9438, -5.4160, -0.3431, -0.2931]]\n",
    "# Add an hidden layer. Since you are only doing FeedForward, you will need to specify the weights between each node\n",
    "# in the layer and each input node (feature). Every row in weights correspond to a row in W\n",
    "model.addLayer(3, \"symmetricsigmoid\", weights=[[-0.7057, 4.9061, 2.6605, -1.1359],[0.49, 1.9324, -0.4269, -5.1570],[0.9438, -5.4160, -0.3431, -0.2931]], \n",
    "               bias=[4.8432, 0.3973, 2.1761], input_size=noFeatures)\n",
    "\n",
    "# [[-1.1444, 0.0106], [0.3115, 11.5477], [-9.9812, 2.6479]]\n",
    "# [[-1.1444, 0.3115, -9.9812], [0.0106, 11.5477, 2.6479]]\n",
    "# Add the ouptu layer. Since you are only doing FeedForward, you will need to specify the weights between each node\n",
    "# in the output layer and each node in the preceiding hidden layer. Every row in weights correspond to a row in W\n",
    "model.addLayer(outputNo, \"logsigmoid\",weights=[[-1.1444, 0.3115, -9.9812], [0.0106, 11.5477, 2.6479]], bias=[2.5230, 2.6463])\n",
    "\n",
    "# Call predict\n",
    "model.predict(data, printPass=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Batch Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=8)\n",
    "\n",
    "def batch_normalization(batch, beta, gamma, epsilon):\n",
    "    mean = np.zeros(batch[0].shape)\n",
    "    for X in batch:\n",
    "        mean = mean + X\n",
    "    mean = mean / len(batch)\n",
    "\n",
    "    variance = np.zeros(batch[0].shape)\n",
    "    for X in batch:\n",
    "        variance = variance + (X - mean) ** 2\n",
    "    variance = variance / len(batch)\n",
    "\n",
    "    batch_normalized = list()\n",
    "    for X in batch:\n",
    "        batch_normalized.append(beta + gamma * (X - mean) / np.sqrt(variance + epsilon))\n",
    "\n",
    "    return batch_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X1 = np.array([[1, 0.5, 0.2], [-1, -0.5, -0.2], [0.1, -0.1, 0]])\n",
    "X2 = np.array([[1, -1, 0.1], [0.5, -0.5, -0.1], [0.2, -0.2, 0]])\n",
    "X3 = np.array([[0.5, -0.5, -0.1], [0, -0.4, 0], [0.5, 0.5, 0.2]])\n",
    "X4 = np.array([[0.2, 1, -0.2], [-1, -0.6, -0.1], [0.1, 0, 0.1]])\n",
    "beta = 0\n",
    "gamma = 1\n",
    "epsilon = 0.1\n",
    "\n",
    "# ---------------------\n",
    "for i, a in enumerate(batch_normalization((X1, X2, X3, X4), beta, gamma, epsilon)):\n",
    "    print(f\"X{i+1}:\\n {a}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Convolution Layer (padding, stride, dilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=8)\n",
    "\n",
    "def convolution_layer(feature_maps, mask_channels, padding, stride, dilation):\n",
    "    nh, nw = feature_maps[0].shape\n",
    "    mh, mw = mask_channels[0].shape\n",
    "    nh_out, nw_out = int((nh + 2 * padding - mh ) / stride) + 1 - (dilation - 1), int((nw + 2 * padding - mw ) / stride) + 1 - (dilation - 1)\n",
    "    out = np.zeros((nh_out, nw_out))\n",
    "    for i in range(nh_out):\n",
    "        for j in range(nw_out):\n",
    "            for X, H in zip(feature_maps, mask_channels):\n",
    "                X = np.pad(X, padding)\n",
    "                if stride != 0:\n",
    "                    i_start = i * stride\n",
    "                    j_start = j * stride\n",
    "                else:\n",
    "                    i_start = i\n",
    "                    j_start = j\n",
    "                i_end = i_start + mh * dilation - dilation + 1\n",
    "                j_end = j_start + mw * dilation - dilation + 1\n",
    "                out[i, j] = out[i, j] + np.sum(X[i_start:i_end:dilation, j_start:j_end:dilation] * H)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initialise your matrices that needs to go through the convolutional layer\n",
    "feature_maps = list()\n",
    "feature_maps.append(np.array([[0.2, 1, 0], [-1, 0, -0.1], [0.1, 0, 0.1]]))\n",
    "feature_maps.append(np.array([[1, 0.5, 0.2], [-1, -0.5, -0.2], [0.1, -0.1, 0]]))\n",
    "\n",
    "# Initialise the masks\n",
    "mask_channels = list()\n",
    "mask_channels.append(np.array([[1, -0.1], [1, -0.1]]))\n",
    "mask_channels.append(np.array([[0.5, 0.5], [-0.5, -0.5]]))\n",
    "\n",
    "#Set the desired padding, stride, and dilation\n",
    "padding = 0 # NOTE padding should be set to 0 if not provided\n",
    "stride = 1 # NOTE stride should be set to 1 if not provided\n",
    "dilation = 1 # NOTE dilation should be set to 1 if not provided\n",
    "\n",
    "#----------------------------------------------------------------------------#\n",
    "convolution_layer(feature_maps, mask_channels, padding=padding, stride=stride, dilation=dilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  Convolution Layer (pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=8)\n",
    "\n",
    "def pooling_layer(feature_maps, pool_size, stride, pooling='max'):\n",
    "    nh, nw = feature_maps[0].shape\n",
    "    nh_out, nw_out = int((nh - pool_size[0]) / stride) + 1, int((nw - pool_size[1]) / stride) + 1\n",
    "    out_maps = list()\n",
    "    for X in feature_maps:\n",
    "        out = np.zeros((nh_out, nw_out))\n",
    "        for i in range(nh_out):\n",
    "            for j in range(nw_out):\n",
    "                i_start = i * stride\n",
    "                j_start = j * stride\n",
    "                i_end = i_start + pool_size[0]\n",
    "                j_end = j_start + pool_size[1]\n",
    "                if pooling == 'avg':\n",
    "                    out[i, j] = np.average(X[i_start:i_end, j_start:j_end])\n",
    "                else:\n",
    "                    out[i, j] = np.max(X[i_start:i_end, j_start:j_end])\n",
    "        out_maps.append(out)\n",
    "    return(out_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Enter the matrix to apply pooling on.\n",
    "feature_maps = list()\n",
    "feature_maps.append(np.array([[0.2, 1, 0, 0.4], [-1, 0, -0.1, -0.1], [0.1, 0, -1, -0.5], [0.4, -0.7, -0.5, 1]]))\n",
    "\n",
    "#Set the desired stride and pooling\n",
    "stride = 2 # NOTE stride should be set to 1 if not provided\n",
    "pooling = 'avg' # It can be either 'avg' or 'max'\n",
    "\n",
    "# Set the dimension of the pooling\n",
    "x1 = 2\n",
    "x2 = 2 # This is a 2x2 pooling\n",
    "\n",
    "#-------------------------------------------------------------------#\n",
    "output = pooling_layer(feature_maps, (x1, x2), stride=stride, pooling=pooling)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Convolution Layer - output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_output_size(input_size, mask_size, padding, stride, dilation=1):\n",
    "    (nh, nw, nc) = input_size\n",
    "    (mh, mw, mc) = mask_size\n",
    "    nh_out, nw_out = int((nh + 2 * padding - mh) / stride) + 1 - (dilation - 1), int(\n",
    "        (nw + 2 * padding - mw) / stride) + 1 - (dilation - 1)\n",
    "    return (nh_out, nw_out, nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# NOTE This only works for single masks and not for concatenations of CNN. For that, look at the image below\n",
    "\n",
    "\n",
    "input_size = (11, 15, 6) # hight x width x feature maps \n",
    "mask_size = (3, 3, 6) # hight x width x channels\n",
    "\n",
    "output_size = get_output_size(input_size, mask_size, padding=0, stride=2)\n",
    "print(output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"download.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Deep Discriminative Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Deep Discriminative Neural Networks [week 5]\n",
    "from scipy import signal\n",
    "\n",
    "ERR_MSG = 'Please use proper activation function name. (relu,' \\\n",
    "          'lrelu,tanh,heaviside'\n",
    "\n",
    "\n",
    "# Returns values after applying activation function.\n",
    "# Params: net - a 2D numpy ndarray, activation - activation function, kwargs to\n",
    "# specify additional value for specific activation function.\n",
    "def calc_activation(net, activation, **kwargs):\n",
    "    return {\n",
    "        'relu': np.array([apply(row, relu, **kwargs) for row in net]),\n",
    "        'lrelu': np.array([apply(row, lrelu, **kwargs) for row in net]),\n",
    "        'tanh': np.array([apply(row, tanh, **kwargs) for row in net]),\n",
    "        'heaviside': np.array([apply(row, heaviside, **kwargs) for row in net]),\n",
    "    }.get(activation, ERR_MSG)\n",
    "\n",
    "\n",
    "def apply(row, activation_fun, **kwargs):\n",
    "    result = []\n",
    "    for value in row:\n",
    "        result.append(activation_fun(value, **kwargs))\n",
    "    return result\n",
    "\n",
    "\n",
    "def relu(x, **kwargs):\n",
    "    return x if x >= 0 else 0\n",
    "\n",
    "\n",
    "def lrelu(x, a=0.1, **kwargs):\n",
    "    return x if x >= 0 else a * x\n",
    "\n",
    "\n",
    "def tanh(x, **kwargs):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def heaviside(x, threshold=0.1, **kwargs):\n",
    "    return np.heaviside(x - threshold, 0.5)\n",
    "\n",
    "\n",
    "# Returns samples after batch normalization.\n",
    "# Parameters: samples - collections of samples in which every sample needs to\n",
    "# have the same shape, beta, gamma, epsilon are parameters of the batch\n",
    "# normalization\n",
    "def batch_normalization(samples, beta, gamma, epsilon):\n",
    "    output = []\n",
    "    shape = samples[0].shape\n",
    "\n",
    "    means = np.zeros(shape)\n",
    "    variances = np.zeros(shape)\n",
    "    for y in range(shape[1]):\n",
    "        for x in range(shape[0]):\n",
    "            values_at_x_y = np.array([sample[y][x] for sample in samples])\n",
    "            mean = np.mean(values_at_x_y)\n",
    "            variance = np.var(values_at_x_y)\n",
    "\n",
    "            means[y][x] = mean\n",
    "            variances[y][x] = variance\n",
    "\n",
    "    for sample in samples:\n",
    "        b_normalized_sample = np.zeros(shape)\n",
    "        for y in range(shape[1]):\n",
    "            for x in range(shape[0]):\n",
    "                value = sample[y][x]\n",
    "                b_normalized_sample[y][x] = calc_b_norm(beta, epsilon, gamma,\n",
    "                                                        means[y][x], value,\n",
    "                                                        variances[y][x])\n",
    "        output.append(b_normalized_sample)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Returns batch normalization for a single value\n",
    "def calc_b_norm(beta, epsilon, gamma, mean, value, variance):\n",
    "    return beta + gamma * ((value - mean) / np.sqrt(variance + epsilon))\n",
    "\n",
    "\n",
    "# Below 4 functions for Q6 to Q8 is provided\n",
    "# through the good offices of Hankun Wang\n",
    "def strided_len(x_len, H_len, stride):\n",
    "    return np.ceil((x_len - H_len + 1) / stride).astype(int)\n",
    "\n",
    "\n",
    "def H_dilated_len(H_len, dilation):\n",
    "    return (H_len - 1) * (dilation - 1) + H_len\n",
    "\n",
    "\n",
    "def dilate_H(H, dilation):\n",
    "    H_rows, H_cols = H[0].shape\n",
    "    H_dilated = np.zeros((H.shape[0], H_dilated_len(H_rows, dilation),\n",
    "                          H_dilated_len(H_cols, dilation)))\n",
    "    H_dilated[:, ::dilation, ::dilation] = H\n",
    "    return H_dilated\n",
    "\n",
    "\n",
    "def apply_mask(X, H, padding=0, stride=1, dilation=1):\n",
    "    # x and H can have multiple channels in the 0th dimension\n",
    "    if padding > 0:\n",
    "        X = np.pad(X, pad_width=padding, mode='constant')[1:-1]\n",
    "\n",
    "    if dilation > 1:\n",
    "        H = dilate_H(H, dilation)\n",
    "\n",
    "    H_rows, H_cols = H[0].shape\n",
    "    x_rows, x_cols = X[0].shape\n",
    "\n",
    "    fm_rows = strided_len(x_rows, H_rows, stride)\n",
    "    fm_cols = strided_len(x_cols, H_cols, stride)\n",
    "\n",
    "    feature_map = np.empty((fm_rows, fm_cols))\n",
    "    for xf in range(fm_rows):\n",
    "        for yf in range(fm_cols):\n",
    "            xi, yi = xf * stride, yf * stride\n",
    "            receptive_region = X[:, xi: xi + H_rows, yi: yi + H_cols]\n",
    "            feature_map[xf, yf] = np.sum(H * receptive_region)\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "def pooling(x, pooling_function, region=(2, 2), stride=1):\n",
    "    x_rows, x_cols = x[0].shape\n",
    "    pool_rows = strided_len(x_rows, region[0], stride)\n",
    "    pool_cols = strided_len(x_cols, region[1], stride)\n",
    "\n",
    "    pool = np.empty((x.shape[0], pool_rows, pool_cols))\n",
    "    for xp in range(pool_rows):\n",
    "        for yp in range(pool_cols):\n",
    "            xi = xp * stride\n",
    "            yi = yp * stride\n",
    "            pooling_region = x[:, xi: xi + region[0], yi: yi + region[1]]\n",
    "            pool[:, xp, yp] = pooling_function(pooling_region)\n",
    "    return pool\n",
    "\n",
    "\n",
    "# Returns output of the height, width and output size based on input shape,\n",
    "# mask shape, number of masks, stride and padding. When defining shape for\n",
    "# input or mask specify it as (height,width, n_channels)\n",
    "def calc_output_dim(input_shape, mask_shape, n_masks, stride, padding):\n",
    "    output_h = int(calc_dim(input_shape[0], mask_shape[0], padding, stride))\n",
    "    output_w = int(calc_dim(input_shape[1], mask_shape[1], padding, stride))\n",
    "\n",
    "    return output_h, output_w, output_h * output_w * n_masks\n",
    "\n",
    "\n",
    "# Returns output size for specific dimension, like height or width in case of\n",
    "# 2D arrays, based on input dimension, mask dimension, padding and stride\n",
    "def calc_dim(input_dim, mask_dim, padding, stride):\n",
    "    return 1 + ((input_dim - mask_dim + 2 * padding) / stride)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Task4 The following array show the output produced by a mask in a\n",
    "    # convolutional layer of a CNN\n",
    "    #\n",
    "    # Calculate the values produced by the application of the following\n",
    "    # activation functions:\n",
    "    # a. ReLU,\n",
    "    # b. LReLU when a=0.1,\n",
    "    # c. tanh,\n",
    "    # d. Heaviside function where each neuron has a threshold of 0.1\n",
    "    # (define H(0) as 0.5).\n",
    "    print('Task 4 ------------------------------------------------------------')\n",
    "    net = np.array([[1, 0.5, 0.2],\n",
    "                    [-1, -0.5, -0.2],\n",
    "                    [0.1, -0.1, 0]])\n",
    "\n",
    "    print('relu:')\n",
    "    print(calc_activation(net, 'relu'))\n",
    "\n",
    "    print('lrelu with a=0.1:')\n",
    "    print(calc_activation(net, 'lrelu', a=0.1))\n",
    "\n",
    "    print('tanh:')\n",
    "    print(calc_activation(net, 'tanh'))\n",
    "\n",
    "    print('heaviside with threshold = 0.1:')\n",
    "    print(calc_activation(net, 'heaviside', threshold=0.1))\n",
    "\n",
    "    # Task5 The following arrays show the output produced by a convolutional\n",
    "    # layer\n",
    "    # to all 4 samples in a batch\n",
    "    #\n",
    "    # Calculate the corresponding outputs produced after the application of\n",
    "    # batch normalisation, assuming the following parameter values β = 0,\n",
    "    # γ = 1, and ε = 0.1 which are the same for all neurons\n",
    "    print('Task 5 ------------------------------------------------------------')\n",
    "    print('batch normalization')\n",
    "    X1 = np.array([[1, 0.5, 0.2],\n",
    "                   [-1, -0.5, -0.2],\n",
    "                   [0.1, -0.1, 0]])\n",
    "\n",
    "    X2 = np.array([[1, -1, 0.1],\n",
    "                   [0.5, -0.5, -0.1],\n",
    "                   [0.2, -0.2, 0]])\n",
    "\n",
    "    X3 = np.array([[0.5, -0.5, -0.1],\n",
    "                   [0, -0.4, 0],\n",
    "                   [0.5, 0.5, 0.2]])\n",
    "\n",
    "    X4 = np.array([[0.2, 1, -0.2],\n",
    "                   [-1, -0.6, -0.1],\n",
    "                   [0.1, 0, 0.1]])\n",
    "\n",
    "    for output in batch_normalization([X1, X2, X3, X4], 0, 1, 0.1):\n",
    "        print(str(output) + '\\n')\n",
    "\n",
    "    # Task6 The following arrays show the feature maps that provide the input\n",
    "    # to a convolutional layer of a CNN\n",
    "    print('Task 6 ------------------------------------------------------------')\n",
    "    X1 = [[0.2, 1., 0.],\n",
    "          [-1., 0., -0.1],\n",
    "          [0.1, 0., 0.1]]\n",
    "\n",
    "    X2 = [[1., 0.5, 0.2],\n",
    "          [-1., -0.5, -0.2],\n",
    "          [0.1, -0.1, 0.]]\n",
    "\n",
    "    X = np.array([X1, X2])\n",
    "\n",
    "    h1 = [[1., -0.1],\n",
    "          [1., -0.1]]\n",
    "\n",
    "    h2 = [[0.5, 0.5],\n",
    "          [-0.5, -0.5]]\n",
    "\n",
    "    H = np.array([h1, h2])\n",
    "    # Calculate the output produced by mask H when using:\n",
    "    print('padding=0 and stride=1')\n",
    "    print(apply_mask(X, H))\n",
    "\n",
    "    print('padding=1 and stride=1')\n",
    "    print(apply_mask(X, H, padding=1))\n",
    "\n",
    "    print('padding=1 and stride=2')\n",
    "    print(apply_mask(X, H, padding=1, stride=2))\n",
    "\n",
    "    print('padding=0 and stride=1 dilation=2')\n",
    "    print(apply_mask(X, H, padding=0, stride=1, dilation=2))\n",
    "\n",
    "    # Task7 The following arrays show the feature maps that provide the input\n",
    "    # to a convolutional layer of a CNN\n",
    "    print('Task 7 ------------------------------------------------------------')\n",
    "    X1 = [[0.2, 1., 0.],\n",
    "          [-1., 0., -0.1],\n",
    "          [0.1, 0., 0.1]]\n",
    "\n",
    "    X2 = [[1., 0.5, 0.2],\n",
    "          [-1., -0.5, -0.2],\n",
    "          [0.1, -0.1, 0.]]\n",
    "\n",
    "    X3 = [[0.5, -0.5, -0.1],\n",
    "          [0, -0.4, 0],\n",
    "          [0.5, 0.5, 0.2]]\n",
    "\n",
    "    X = np.array([X1, X2, X3])\n",
    "\n",
    "    # Calculate the output produced by 1x1 convolution when the 3 channels of\n",
    "    # the 1x1 mask are [1,-1,0.5]\n",
    "    h1 = [[1]]\n",
    "    h2 = [[-1]]\n",
    "    h3 = [[0.5]]\n",
    "\n",
    "    H = np.array([h1, h2, h3])\n",
    "    print('3 channel 1x1 masks applied to X1 X2 X3:')\n",
    "    print(apply_mask(X, H))\n",
    "\n",
    "    # Task 8  The following array shows the input to a pooling layer of a CNN\n",
    "    print('Task 8 ------------------------------------------------------------')\n",
    "    X1 = [[0.2, 1., 0, 0.4],\n",
    "          [-1., 0., -0.1, -0.1],\n",
    "          [0.1, 0., -1, -0.5],\n",
    "          [0.4, -0.7, -0.5, 1]]\n",
    "\n",
    "    # Calculate the output produced by the pooling when using:\n",
    "    print('average pooling with a pooling region of 2x2 and stride=2')\n",
    "    print(pooling(np.array([X1]), np.mean, (2, 2), stride=2))\n",
    "\n",
    "    print('max pooling with a pooling region of 2x2 and stride=2')\n",
    "    print(pooling(np.array([X1]), np.max, (2, 2), stride=2))\n",
    "\n",
    "    print('max pooling with a pooling region of 3x3 and stride=1')\n",
    "    print(pooling(np.array([X1]), np.max, (3, 3), stride=1))\n",
    "\n",
    "    # Task9 The input to a convolutional layer of a CNN consists of 6 feature\n",
    "    # maps each of which has a height of 11 and width of 15 (i.e., input is\n",
    "    # 11 × 15 × 6). What size will the output produced by a single mask with\n",
    "    # 6 channels and a width of 3 and a height of 3 (i.e., 3×3×6) when using\n",
    "    # a stride of 2 and padding of 0.\n",
    "    #\n",
    "    # when defining shape please use template (height,width,n_channels)\n",
    "    print('Task 9 ------------------------------------------------------------')\n",
    "    input_shape = (11, 15, 6)\n",
    "    mask_shape = (3, 3, 6)\n",
    "    n_masks = 1\n",
    "    stride = 2\n",
    "    padding = 0\n",
    "    print('calculate output dimension')\n",
    "    out_h, out_w, out_size = calc_output_dim(input_shape, mask_shape, n_masks,\n",
    "                                             stride, padding)\n",
    "    print('output shape is: ' + str(out_h) + 'x' + str(out_w) + 'x' +\n",
    "          str(n_masks) + '=' + str(out_size))\n",
    "\n",
    "    # Task 10 A CNN processes an image of size 200x200x3 using the following\n",
    "    # sequence of layers:\n",
    "    print('Task 10 -----------------------------------------------------------')\n",
    "\n",
    "    print('convolution with 40 masks of size 5x5x3 with stride=1, padding=0')\n",
    "    input_shape = (200, 200, 3)\n",
    "    mask_shape = (5, 5, 3)\n",
    "    n_masks = 40\n",
    "    stride = 1\n",
    "    padding = 0\n",
    "    out_h, out_w, out_size = calc_output_dim(input_shape, mask_shape, n_masks,\n",
    "                                             stride, padding)\n",
    "    print('output shape is: ' + str(out_h) + 'x' + str(out_w) + 'x' +\n",
    "          str(n_masks) + '=' + str(out_size))\n",
    "\n",
    "    print('pooling with 2x2 pooling regions stride=2')\n",
    "    input_shape = (out_h, out_w, 3)\n",
    "    mask_shape = (2, 2, 3)\n",
    "    stride = 2\n",
    "\n",
    "    out_h, out_w, out_size = calc_output_dim(input_shape, mask_shape, n_masks,\n",
    "                                             stride, padding)\n",
    "    print('output shape is: ' + str(out_h) + 'x' + str(out_w) + 'x' +\n",
    "          str(n_masks) + '=' + str(out_size))\n",
    "\n",
    "    print('convolution with 80 masks of size 4x4 with stride=2, padding=1')\n",
    "    input_shape = (out_h, out_w, 3)\n",
    "    mask_shape = (4, 4, 3)\n",
    "    n_masks = 80\n",
    "    padding = 1\n",
    "\n",
    "    out_h, out_w, out_size = calc_output_dim(input_shape, mask_shape, n_masks,\n",
    "                                             stride, padding)\n",
    "    print('output shape is: ' + str(out_h) + 'x' + str(out_w) + 'x' +\n",
    "          str(n_masks) + '=' + str(out_size))\n",
    "\n",
    "    print('1x1 convolution with 20 masks')\n",
    "    input_shape = (out_h, out_w, 3)\n",
    "    mask_shape = (1, 1, 3)\n",
    "    n_masks = 20\n",
    "    # this was not clearly stated in description, but from solution looks\n",
    "    # like they meant to set padding and stride to default values\n",
    "    stride = 1\n",
    "    padding = 0\n",
    "\n",
    "    out_h, out_w, out_size = calc_output_dim(input_shape, mask_shape, n_masks,\n",
    "                                             stride, padding)\n",
    "    print('output shape is: ' + str(out_h) + 'x' + str(out_w) + 'x' +\n",
    "          str(n_masks) + '=' + str(out_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Activation Function Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ActivationFunctionOperations:\n",
    "    '''\n",
    "    Based on Question 4 in Tutorial 5:\n",
    "    The following array show the output produced by a mask in a convolutional layer of a CNN.\n",
    "              [[1, 0.5, 0.2], \n",
    "     net_j =  [-1, -0.5, -0.2], \n",
    "              [0.1, -0.1, 0]]\n",
    "    Calculate the values produced by the application of the following activation functions:\n",
    "    '''\n",
    "\n",
    "    def fit(net_j, activation_function, a = 0.1, threshold = 0.1, heaviside_0 = 0.5):\n",
    "\n",
    "        new_array = []\n",
    "\n",
    "        if activation_function == 'ReLU':\n",
    "\n",
    "            for row in net_j:\n",
    "                temp_array = []\n",
    "                for i in row:\n",
    "\n",
    "                    # threshold:\n",
    "                    if i >= 0:\n",
    "                        temp_array.append(i)\n",
    "                    else:\n",
    "                        temp_array.append(0)\n",
    "                new_array.append(temp_array)\n",
    "\n",
    "            print(activation_function, \":  \", new_array)\n",
    "\n",
    "        elif activation_function == 'LReLU':\n",
    "\n",
    "            for row in net_j:\n",
    "                temp_array = []\n",
    "                for i in row:\n",
    "\n",
    "                    # threshold:\n",
    "                    if i >= 0:\n",
    "                        temp_array.append(i)\n",
    "                    else:\n",
    "                        temp_array.append(round(a * i, 2))\n",
    "                new_array.append(temp_array)\n",
    "\n",
    "            print(activation_function, \":  \", new_array)\n",
    "\n",
    "        elif activation_function == 'tanh':\n",
    "\n",
    "            for row in net_j:\n",
    "                temp_array = []\n",
    "                for i in row:\n",
    "\n",
    "                    # Using equation of tanh activation function:\n",
    "                    temp_array.append(round((math.e**i - math.e ** -i) / (math.e**i + math.e ** -i), 5)) \n",
    "                new_array.append(temp_array)\n",
    "\n",
    "            print(activation_function, \":  \", new_array)\n",
    "\n",
    "        elif activation_function == 'heaviside':\n",
    "\n",
    "            for row in net_j:\n",
    "                temp_array = []\n",
    "                for i in row:\n",
    "\n",
    "                    # subtracts threshold away from each value:\n",
    "                    i = i - threshold\n",
    "\n",
    "                    # applies heaviside function to value:\n",
    "                    temp_array.append(np.heaviside(i, heaviside_0))\n",
    "\n",
    "                new_array.append(temp_array)\n",
    "\n",
    "            print(activation_function, \":  \", new_array)\n",
    "            \n",
    "    def setParams():\n",
    "        \n",
    "        activation_function = \"ReLU\"\n",
    "        a = 0.1\n",
    "        threshold = 0.1\n",
    "        heaviside_0 = 0.5\n",
    "        \n",
    "        net_j = transformToList(input(\"Enter matrix to perform activation on eg [[1, 0.5, 0.2], [-1, -0.5, -0.2], [0.1, -0.1, 0]] 2D:  \"))\n",
    "        typeActivation = str(input(\"'ReLU', 'LReLU', 'tanh' or 'heaviside' \"))\n",
    "        if typeActivation.upper()==\"RELU\":\n",
    "            activation_function = \"ReLU\"\n",
    "        elif typeActivation.upper()==\"LRELU\":\n",
    "            activation_function = \"LReLU\"\n",
    "            a = float(input(\"Enter the a value eg 0.1.  \"))\n",
    "        elif typeActivation.lower()==\"tanh\": \n",
    "            activation_function = \"tanh\"\n",
    "        elif typeActivation.lower()==\"heaviside\": \n",
    "            activation_function = \"heaviside\"\n",
    "            threshold = float(input(\"Enter the threshold value eg 0.1.  \"))\n",
    "            heaviside_0 = float(input(\"Enter the H(0) value eg 0.5.  \"))\n",
    "        else:\n",
    "            print(\"wrong input\")\n",
    "            \n",
    "        return net_j, activation_function, a, threshold, heaviside_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_j, activation_function, a, threshold, heaviside_0 = ActivationFunctionOperations.setParams()\n",
    "ActivationFunctionOperations.fit(net_j, activation_function, a, threshold, heaviside_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Deep Gen Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math as math\n",
    "import sympy as sy\n",
    "\n",
    "#\n",
    "# NOTE: This script only works for k=1. Examples with different values\n",
    "#       of k are not provided in the lecture or the tutorial.\n",
    "#\n",
    "\n",
    "# Change this variables according to the question\n",
    "training_iterations = 1 # Number of training iterations.\n",
    "x_real = [[1,2],[3,4]] # Real samples\n",
    "x_fake = [[5,6],[7,8]] # Generated samples\n",
    "    # Both x_real and x_fake must have the same number of \n",
    "    # samples for part b to work\n",
    "\n",
    "theta = [0.1,0.2] # Initial parameters of discriminator\n",
    "\n",
    "prob_real = 0.5 # Probability of real samples to be selected\n",
    "prob_fake = 0.5 # Probability of generated samples to be selected\n",
    "    # In Tutorial 7 Q.3 each sample has the same probability,\n",
    "    # that is why the current value is 0.5 for both\n",
    "\n",
    "learning_rate = 0.02 # Part b Q.3 says that learning rate is 0.02 but\n",
    "                    # it can be changed to the desired value\n",
    "\n",
    "# DO NOT change this variables\n",
    "# ----------------------------\n",
    "# Here, symbols are created for each variable so that\n",
    "# the discriminator function can be differentiated and\n",
    "# evaluated with different values\n",
    "n = len(x_real[0]) \n",
    "x = [None] * n\n",
    "t = [None] * n\n",
    "for i in range(n):\n",
    "    name = 'x'+str(i)\n",
    "    x[i] = sy.Symbol(name)\n",
    "    name = 't'+str(i)\n",
    "    t[i] = sy.Symbol(name)\n",
    "# ----------------------------\n",
    "\n",
    "# Change this according to the problem's discriminator function\n",
    "discriminator_function = 1/(1+ math.e**-(t[0]*x[0] - t[1]*x[1] - 2))\n",
    "    # The discriminator function can be changed to any function needed.\n",
    "    # \n",
    "    # Note that the variables MUST be expressed as t[i] and x[i] \n",
    "    # where:\n",
    "    # 't' is a list containig variables for the parameters of Discriminator (Theta)\n",
    "    # 'x' is a list containig variables for each attribute of each sample\n",
    "    # 'i' is the number of the attribute and parameter used in that case\n",
    "    # \n",
    "    # i.e. for the equation: \"θ1*x1\" \n",
    "    # then above it should be written: \"t[0]*x[0]\"\n",
    "    # because python lists start in 0\n",
    "\n",
    "# *****DO NOT CHANGE CODE BELOW THIS LINE********\n",
    "#\n",
    "m = len(x_real)\n",
    "m_fake = len(x_fake)\n",
    "\n",
    "#\n",
    "# Because it is discrete we don't need to find integrals here when computing expectations\n",
    "#\n",
    "print('************')\n",
    "print('*****GAN****')\n",
    "print('************')\n",
    "\n",
    "#\n",
    "# Tutorial 7 Question 3 part a\n",
    "#\n",
    "for iter in range(training_iterations):\n",
    "    print ('\\n--Start of',str(iter+1)+\"°\",'training iteration.--')\n",
    "    V_D = 0 # Discriminator Value \n",
    "\n",
    "    # This for is to obtain the discriminator value\n",
    "    for i in range(m):\n",
    "        xx = x_real[i]\n",
    "        res_discr_funct = discriminator_function\n",
    "        for j in range(n):\n",
    "            # .subs() is a function that replaces the x[i] and t[i] variables of the \n",
    "            # discriminant function with the actual values of the theta and real samples\n",
    "            # given in the problem\n",
    "            res_discr_funct = res_discr_funct.subs(x[j],xx[j]).subs(t[j],theta[j])\n",
    "        V_D += prob_real*math.log(res_discr_funct)\n",
    "\n",
    "\n",
    "    print('\\n***** PART A ****\\n')\n",
    "\n",
    "    print('\\nThe Discriminator value is', V_D)\n",
    "\n",
    "    V_G = 0 # Generator Value\n",
    "\n",
    "    # This for is to obtain the generator value\n",
    "    for i in range(m_fake):\n",
    "        xx = x_fake[i]\n",
    "        res_discr_funct = discriminator_function\n",
    "        for j in range(n):\n",
    "            # .subs() is a function that replaces the x[i] and t[i] variables of the \n",
    "            # discriminant function with the actual values of the theta and real samples\n",
    "            # given in the problem\n",
    "            res_discr_funct = res_discr_funct.subs(x[j],xx[j]).subs(t[j],theta[j])\n",
    "        V_G += prob_fake*math.log(1 - res_discr_funct)\n",
    "        \n",
    "    print('\\nThe Generator value is', V_G)\n",
    "\n",
    "    V_DG = V_D + V_G\n",
    "\n",
    "    print('\\nThe Computed V_DG is ', V_DG)\n",
    "\n",
    "    #\n",
    "    # Tutorial 7 Question 3 part b\n",
    "    #\n",
    "\n",
    "    print('\\n\\n*********************')\n",
    "    print('***** PART B ****\\n')\n",
    "\n",
    "    alpha_beta = [[0] * n] * m\n",
    "    for i in range(m):\n",
    "        xx = x_real[i]\n",
    "        xx_bar = x_fake[i]\n",
    "        # Two different equations are needed. One with x_real[i] and another with\n",
    "        # x_fake[i]. This is explained in the tutorial solutions.\n",
    "        discriminator_function_xx = discriminator_function\n",
    "        discriminator_function_xx_bar = discriminator_function\n",
    "        for j in range(n):\n",
    "            discriminator_function_xx = discriminator_function_xx.subs(x[j],xx[j])\n",
    "            discriminator_function_xx_bar = discriminator_function_xx_bar.subs(x[j],xx_bar[j])\n",
    "\n",
    "        learning_equation = sy.log(discriminator_function_xx) \n",
    "        learning_equation += sy.log(1 - discriminator_function_xx_bar)\n",
    "        \n",
    "        differential_equation = [None] * n\n",
    "        # The learning equation is differentiated in this for\n",
    "        for j in range(n):\n",
    "            differential_equation[j] = learning_equation.diff(t[j])\n",
    "            # After differentiating the learning equation, the next for replaces\n",
    "            # thetha variables with the actual values to obtain the result\n",
    "            for k in range(n):\n",
    "                differential_equation[j] = differential_equation[j].subs(t[k],theta[k])\n",
    "        # The result of the differentiated equations is added to a list\n",
    "        alpha_beta[i] = differential_equation\n",
    "\n",
    "    # Alpha and beta are added together to obtain delta\n",
    "    delta_theta = [0] * n\n",
    "    for i in range(n):\n",
    "        delta_theta[i] = 0\n",
    "        for j in range(m):\n",
    "            print( 'Alpha and Beta', i+1,j+1, 'is', alpha_beta[j][i])\n",
    "            delta_theta[i] += (1/m)*alpha_beta[j][i]\n",
    "\n",
    "    print( '\\nDelta is ', delta_theta,'\\n')\n",
    "\n",
    "    # Lastly, delta is added to the current theta values to obtain the new\n",
    "    # theta values after training\n",
    "    for i in range(n):\n",
    "        theta[i] = theta[i] + learning_rate*delta_theta[i]\n",
    "        print('New theta',i+1   ,'is',theta[i])\n",
    "    print ('\\n--End of',str(iter+1)+\"°\",'training iteration.--')\n",
    "print ('\\n\\n---------End of the script.---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Oja's rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "import numpy as np\n",
    "\n",
    "# Oja's Learning rule \n",
    "# through Mizan \n",
    "# this code works out zero-mean data so please enter the raw feature vectors as given in the question\n",
    "# Tutorial 7, question 7\n",
    "\n",
    "## -----------------------------------------------------------------------\n",
    "# ONLY CHANGE THESE 3 INPUTS AND CHANGE THE NUMBER OF EPOCH YOU WANT WHEN APPLYING THE FUNCTION(Oja_learning_rule)\n",
    "input_from_question = np.array([[[0,1]],[[3,5]],[[5,4]],[[5,6]],[[8,7]],[[9,7]]])\n",
    "weight_x = np.array([[-1,0]])\n",
    "learning_rate = 0.01\n",
    "epochs = 6\n",
    "## -----------------------------------------------------------------------\n",
    "\n",
    "# we are going to perform Oja's learning on the input_vectors\n",
    "input_vectors = []\n",
    "\n",
    "mean_of_data = input_from_question.mean(axis=0)\n",
    "for i in input_from_question:\n",
    "    zero_mean_data = i - mean_of_data\n",
    "    input_vectors.append(zero_mean_data)\n",
    "    \n",
    "\n",
    "def Oja_learning_rule(epoch):\n",
    "    weight_update = np.copy(weight_x)  \n",
    "    for i in range(1,epoch+1):\n",
    "        df = pd.DataFrame({\"x\": [i for i in input_vectors]})\n",
    "        df['y'] = df['x'].apply(lambda x: np.dot(x,weight_update.T))\n",
    "        df['x - yw'] = df['x'].apply(lambda x: np.round(x, 4)) - df['y'].apply(lambda y: y * (weight_update))  \n",
    "        df['ny(x -yw)'] = learning_rate * df['y'].apply(lambda y: y) * df['x - yw'].apply(lambda x: x)\n",
    "        #Rounding the numbers         \n",
    "        df['y'] = df['y'].apply(lambda y: np.round(y,4))\n",
    "        df['x - yw'] =  df['x - yw'].apply(lambda x: np.round(x,4))\n",
    "        df['ny(x -yw)']  = df['ny(x -yw)'].apply(lambda x: np.round(x,4))\n",
    "        sum_of_weights = df['ny(x -yw)'].sum()\n",
    "        weight_update = weight_update + sum_of_weights   \n",
    "        display(df)\n",
    "        print(f'after {i} epoch Total weight change is: {sum_of_weights}')\n",
    "        print(f'after {i} epoch our weights are: {weight_update}')\n",
    "\n",
    "\n",
    "Oja_learning_rule(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "\n",
    "def _PCA(ip, n_components, data_to_project=None):\n",
    "    ip = np.array(ip)\n",
    "    ip_mean = np.mean(ip, axis=1)\n",
    "    ip_prime = ip - np.vstack(ip_mean)\n",
    "    C = (ip_prime @ ip_prime.T) / ip.shape[1]\n",
    "    V, D, VT = svd(C)\n",
    "    ans = VT @ ip_prime\n",
    "    print(\"-\"*100)\n",
    "    print(\"READ THE ROWS FROM THE TOP\")\n",
    "    print(ans[:n_components])\n",
    "    print(\"-\"*100)\n",
    "    if data_to_project:\n",
    "        data_to_project = np.array(data_to_project)\n",
    "        print(\"-\"*100)\n",
    "        print(f\"PROJECTION OF {data_to_project}\")\n",
    "        print((VT@data_to_project)[:n_components])\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# REPLACE ACCORDING TO THE QUESTION\n",
    "ip = [[4, 0, 2, -2], [2, -2, 4, 0], [2, 2, 2, 2]]\n",
    "n_components = 2\n",
    "data_to_project = [3, -2, 5]\n",
    "_PCA(ip=ip, n_components=n_components, data_to_project=data_to_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fisher's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def fishers(ip, weights, classes):\n",
    "    ip = np.array(ip)\n",
    "    N, D = ip.shape\n",
    "    weights = np.array(weights)\n",
    "    m1 = []\n",
    "    m2 = []\n",
    "    for idx in range(N):\n",
    "        if classes[idx] == 1:\n",
    "            m1.append(ip[idx])\n",
    "        else:\n",
    "            m2.append(ip[idx])\n",
    "    m1 = np.mean(m1, axis=0)\n",
    "    m2 = np.mean(m2, axis=0)\n",
    "\n",
    "    # between cluster distance\n",
    "    sb = []\n",
    "    sw = []\n",
    "    for w in (weights):\n",
    "        d = (w @ (m1-m2)) ** 2\n",
    "        sb.append(d)\n",
    "    # calculate within cluster distance\n",
    "    sw = []\n",
    "    for w in weights:\n",
    "        running_sw = 0\n",
    "        for idx in range(len(ip)):\n",
    "            if classes[idx] == 1:\n",
    "                running_sw += (w.T @ (ip[idx] - m1)) ** 2\n",
    "\n",
    "            elif classes[idx] == 2:\n",
    "                running_sw += (w.T @ (ip[idx] - m2)) ** 2\n",
    "        sw.append(running_sw)\n",
    "        # print(running_sw)\n",
    "    print(\"SB: \")\n",
    "    print(sb)\n",
    "    print(\"SW: \")\n",
    "    print(sw)\n",
    "    cost = []\n",
    "    for _sb, _sw in zip(sb, sw):\n",
    "        cost.append(_sb/_sw)\n",
    "    print(\"Cost: \")\n",
    "    print(cost)\n",
    "\n",
    "    print(\"-\"*100)\n",
    "    print(f\"{weights[np.argmax(cost)]} has high PROJECTION COST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ip = [[1, 2], [2, 1], [3, 3], [6, 5], [7, 8]]\n",
    "classes = [1, 1, 1, 2, 2]\n",
    "weights = [[-1, 5], [2, -3]]\n",
    "fishers(ip, weights, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sparse Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def main(p, VT, x, _lambda):\n",
    "    p = np.array(p)\n",
    "    VT = np.array(VT)\n",
    "    x = np.array(x)\n",
    "    r_error = []\n",
    "    for p in projections:\n",
    "        val = x - VT @ p\n",
    "        r_error.append(np.linalg.norm(val) + _lambda*np.count_nonzero(p))\n",
    "    print(\"RECONSTRUCTION ERRORS: \")\n",
    "    print(r_error)\n",
    "    print(projections[np.argmin(r_error)], \" for sparse coding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# REPLACE ACCORDING TO THE QUESTION\n",
    "# projections are nothing but y\n",
    "\n",
    "projections = [[1, 2, 0, -1], [0, 0.5, 1, 0]]\n",
    "x = [[2, 3]]\n",
    "VT = [[1, 1, 2, 1], [-4, 3, 2, -1]]\n",
    "main(p=projections, VT=VT, x=x, _lambda=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SVM(to find lambda, weights and margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def fit(X, y, support_vectors, support_vector_class):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        print(\"-\"*100)\n",
    "        w = []\n",
    "        for idx in range(len(support_vectors)):\n",
    "            w.append(support_vectors[idx] * support_vector_class[idx])\n",
    "        w = np.array(w)\n",
    "        eq_arr = []\n",
    "        for idx, sv in enumerate(support_vectors):\n",
    "            tmp = ((w @ sv) * support_vector_class[idx])\n",
    "            tmp = np.append(tmp, [support_vector_class[idx]])\n",
    "            eq_arr.append(tmp)\n",
    "        eq_arr.append(np.append(support_vector_class, [0]))\n",
    "        rhs_arr = [1] * len(support_vector_class)\n",
    "        rhs_arr.extend([0])\n",
    "        rhs_arr = np.array(rhs_arr)\n",
    "        ans = rhs_arr @ np.linalg.inv(eq_arr)\n",
    "        print(\"lambda and w_0 values are \", ans)\n",
    "        final_weight = []\n",
    "        for idx in range(w.shape[0]):\n",
    "            final_weight.append(w[idx] * ans[idx])\n",
    "        final_weight = np.array(final_weight)\n",
    "        final_weight = np.sum(final_weight, axis=0)\n",
    "        print(\"Weights: \")\n",
    "        print(final_weight)\n",
    "        print(\"Margin: \")\n",
    "        print(2/np.linalg.norm(final_weight))\n",
    "        print(\"-\"*100)\n",
    "            \n",
    "    def setParams():\n",
    "        X = transformToList(input(\"Enter features eg [[3, 1], [3, -1], [7, 1], [8, 0], [1, 0], [0, 1], [-1, 0], [-2, 0]] 2D:  \"))\n",
    "        y = transformToList(input(\"Enter labels eg [1, 1, 1, 1, -1, -1, -1, -1] 1D:  \"))\n",
    "        support_vectors = np.array(transformToList(input(\"Enter features eg [[3, 1], [3, -1], [1, 0]] 2D:  \"))).astype(np.float)\n",
    "        support_vector_class = np.array(transformToList(input(\"Enter labels eg [1, 1, -1] 1D:  \"))).astype(np.float)\n",
    "            \n",
    "        return X, y, support_vectors, support_vector_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X, y, support_vectors, support_vector_class = SVM.setParams()\n",
    "SVM.fit(X, y, support_vectors, support_vector_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# NOTE you can use if for Q1 c in tutorial9\n",
    "\n",
    "def run_adaBooster_setup():\n",
    "    \"\"\" Ask the user to enter the needed parameters\n",
    "    \"\"\"  \n",
    "    print(\"Please insert the dataset. Each sample should be divided by a space and each coordinate within a sample should be divided by a coma. \\nBe careful not to enter spaces after the coma that separates the samples' coordinates.\\n\")\n",
    "    print(\"I.E.: x1=[1,2], x2=[-3,4], x3=[5,3] would be ---> 1,2 -3,4 5,3 \\n\")\n",
    "    dataset_input = str(input())\n",
    "    dataset = np.array([]).reshape(0,2)\n",
    "    samples = dataset_input.split(' ')\n",
    "    for sample in samples:\n",
    "        coordinates = sample.split(',')\n",
    "        coordinates = np.array([float(c) for c in coordinates])\n",
    "        dataset = np.concatenate((dataset, [coordinates]))\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the labels (y) of each sample in the dataset. \\nYou should provide a sequential list of y where each y is separated by a single space.\")\n",
    "    print(\"I.E. +1 -1 +1 -1\")\n",
    "    labels = [ int(o) for o in str(input()).split(' ')] \n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the number of weak classifiers:\")\n",
    "    n_classifiers = int(input())\n",
    "    print(\"\\n\")\n",
    "    thresholds = np.array([])\n",
    "    for i in range(0, n_classifiers):\n",
    "        print(f\"Please insert the decision threshold for weak classifier {i+1} so that it classify a sample to be +1:\")\n",
    "        print(\"I.E.: x1 > 0\")\n",
    "        print(\"I.E.: x2 > 3\")\n",
    "        print(\"I.E.: x1 >= -4\")\n",
    "        thresholds = np.concatenate((thresholds, [str(input())]))\n",
    "        print('\\n')\n",
    "    print(\"Please enter the target training error (when the adaboost should terminate). This should be normalised to the total number of samples:\")\n",
    "    print(\"I.E.: 0 -> if you want to stop it when the classifier classifies correctly all the training samples.\")\n",
    "    print(\"I.E.: 0.25 -> if you want to stop it when the classifier classifies correctly 75\\% of the samples.\")\n",
    "    target_error = int(input())\n",
    "    print(\"\\n\")\n",
    "    print(\"Please enter the maximum number of iterations you want the algorithm to run for:\")\n",
    "    max_iterations = int(input())\n",
    "    print(\"\\n\")\n",
    "    return dataset, labels, n_classifiers, thresholds, target_error, max_iterations\n",
    "\n",
    "\n",
    "# Decision stump used as weak classifier\n",
    "class DecisionStump():\n",
    "    def __init__(self, id, threshold=None):\n",
    "        self.threshold = threshold\n",
    "        self.id = id\n",
    "\n",
    "\n",
    "    def predict(self, sample):\n",
    "        if '>' in self.threshold:\n",
    "            terms = self.threshold.split(' ')\n",
    "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
    "            if sample[axis_in_condition[0]] > float(terms[2]):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        elif '<' in self.threshold:\n",
    "            terms = self.threshold.split(' ')\n",
    "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
    "            if sample[axis_in_condition[0]] < float(terms[2]):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        elif '<=' in self.threshold:\n",
    "            terms = self.threshold.split(' ')\n",
    "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
    "            if sample[axis_in_condition[0]] <= float(terms[2]):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        elif '>=' in self.threshold:\n",
    "            terms = self.threshold.split(' ')\n",
    "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
    "            if sample[axis_in_condition[0]] >= float(terms[2]):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "\n",
    "class Adaboost():\n",
    "\n",
    "    def __init__(self, n_clf, thresholds, target_error, max_iterations=10):\n",
    "        self.n_clf = n_clf\n",
    "        self.clfs = np.array([])\n",
    "        for i in range (0, len(thresholds)):\n",
    "            self.clfs = np.concatenate(( self.clfs, [DecisionStump(i+1, thresholds[i])] ))\n",
    "        # self.alpha = 0\n",
    "        self.alpha = []\n",
    "        self.target_error = target_error\n",
    "        self.max_iterations = max_iterations\n",
    "        self.best_classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights to 1/N\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "        iteration = 1\n",
    "        while True:\n",
    "            print(f\"Iteration {iteration}: \\nWeights: {w}\")\n",
    "\n",
    "            # Iterate through classifiers and find the best one\n",
    "            lowest_error = 100000000\n",
    "            best_classifier = 0\n",
    "            for i in range(0, self.n_clf):\n",
    "                clf = self.clfs[i]\n",
    "                # Calculate Error\n",
    "                err = 0\n",
    "                for j, sample in enumerate(X):\n",
    "                    prediction = clf.predict(sample)\n",
    "                    error = 0 if prediction == y[j] else 1\n",
    "                    err += error * w[j]\n",
    "                if err < lowest_error:\n",
    "                    lowest_error = err\n",
    "                    best_classifier = i\n",
    "\n",
    "            self.best_classifiers.append(self.clfs[best_classifier])\n",
    "            print(f\"Best classifier: {best_classifier+1}\")\n",
    "\n",
    "            # Get predictions from best classifer for each sample\n",
    "            predictions = []\n",
    "            for j, sample in enumerate(X):\n",
    "                prediction = self.clfs[best_classifier].predict(sample)\n",
    "                predictions.append(prediction)\n",
    "\n",
    "            # Calculate weighted training error of best classifier\n",
    "            weighted_error = 0\n",
    "            for i, prediction in enumerate(predictions):\n",
    "                # error += 0 if prediction == y[i] else 1\n",
    "                w_e = 0 if prediction == y[i] else 1\n",
    "                w_e *= w[i]\n",
    "                weighted_error += w_e\n",
    "            error /= len(y)\n",
    "            print(f\"Best classifier's weighted training error: {weighted_error}\")\n",
    "\n",
    "\n",
    "            # Calculate Alpha\n",
    "            EPS = 1e-10\n",
    "            alpha = 0.5 * np.log((1.0 - lowest_error) / (lowest_error))\n",
    "            self.alpha.append(alpha)\n",
    "            print(f\"Alpha: {alpha}\")\n",
    "\n",
    "            # Calculate weights for next iteration\n",
    "            new_w = []\n",
    "            for i, weight in enumerate(w):\n",
    "                new_w.append(w[i] * (np.exp(- alpha * y[i] * predictions[i])))\n",
    "                print(f\"Update weight: W{iteration}(sample{i+1})*e^-alpha{iteration}*y{i+1}*h{iteration}(sample{i+1}) ----> {w[i] * (np.exp(- alpha * y[i] * predictions[i]))}\")\n",
    "            # Normalize to one\n",
    "            Z_normalisation = 0\n",
    "            for i, weight in enumerate(new_w):\n",
    "                Z_normalisation += weight\n",
    "            for i, weight in enumerate(new_w):\n",
    "                new_w[i] /= Z_normalisation\n",
    "            print(f\"Normalisation Z{iteration} when updating new weights: {Z_normalisation}\")\n",
    "            # Update weights for next iteration\n",
    "            w = new_w\n",
    "\n",
    "\n",
    "\n",
    "            # Check if the classifier has reached the desired target error\n",
    "            # Find the output*alpha of each classifier for each sample\n",
    "            tot_error = 0\n",
    "            decision_formula = ''\n",
    "            sample_classifications = np.zeros((X.shape[0], len(self.alpha)))\n",
    "            for i, alpha in enumerate(self.alpha):  \n",
    "                clf = self.best_classifiers[i]\n",
    "                for j, sample in enumerate(X):\n",
    "                    prediction = clf.predict(sample)\n",
    "                    sample_classifications[j][i] = alpha if prediction == y[j] else -alpha\n",
    "                decision_formula += f\"{alpha} * h{clf.id}(x) + \"\n",
    "            # Calculate the AdaBooster classification error in this round\n",
    "            sample_classifications = sample_classifications.sum(axis=1)\n",
    "            for i, classification in enumerate(sample_classifications):\n",
    "                classification = 1 if classification >= 0 else -1\n",
    "                tot_error += 1/X.shape[0] if classification == y[j] else 0\n",
    "            print(f\"AdaBoost Classifier in this round: {decision_formula[:-2]}\")\n",
    "            print(f\"AdaBoost Classifier (unweighted) error in this round: {tot_error}\")\n",
    "            # If the error is below our target error stop the execution\n",
    "            if tot_error <= self.target_error:\n",
    "                print('\\n')\n",
    "                print(f\"The final hard classifier is: sgn({decision_formula[:-2]})\")\n",
    "                return\n",
    "\n",
    "            # If we have reached the max iterations stop the execution\n",
    "            if iteration >= self.max_iterations:\n",
    "                return\n",
    "            iteration += 1\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, sample):\n",
    "        tot_error = 0\n",
    "        sample_classifications = np.zeros((1, len(self.alpha)))\n",
    "        for i, alpha in enumerate(self.alpha):  \n",
    "            clf = self.best_classifiers[i]\n",
    "            prediction = clf.predict(sample)\n",
    "            sample_classifications[0][i] = alpha if prediction == 1 else -alpha\n",
    "        # Calculate the AdaBooster classification error in this round\n",
    "        sample_classifications = sample_classifications.sum(axis=1)\n",
    "        return 1 if sample_classifications[0] >= 0 else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset, labels, n_classifiers, thresholds, target_error, max_iterations = run_adaBooster_setup()\n",
    "    classifier = Adaboost(n_classifiers, thresholds, target_error, max_iterations)\n",
    "    classifier.fit(dataset, labels)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nEnter a new sample (separating its coordinates with a coma and not including spaces) to predict or simply enter quit()\")\n",
    "        sample = str(input())\n",
    "        if sample == 'quit()':\n",
    "            quit()\n",
    "        result = classifier.predict([float(c) for c in sample.split(',')])\n",
    "        print(f\"The AdaBoost classifier classified it as {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Competitive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def run_competitive_learning_setup():\n",
    "    \"\"\" Ask the user to enter the needed parameters for competitive learning.\n",
    "    Returns:\n",
    "        [string, np.array, np.array, float, np.array]: \n",
    "                             mode -> Specify if using normalisation or not.\n",
    "                             dataset -> The dataset to cluster.\n",
    "                             clusters -> The coordinates of the initial centroids\n",
    "                             lr -> The learning Rate \n",
    "                             order_of_indexes -> The order to follow when selecting samples in the algorithm\n",
    "    \"\"\"    \n",
    "    print(\"How you want to run the algorithm? (Enter the corresponding number and press ENTER)\")\n",
    "    print(\"1) With normalisation and argumentation\")\n",
    "    print(\"2) Without normalisation and argumentation\")\n",
    "    mode = int(input())\n",
    "    algorithm_variants = {1: \"with norm\", 2:\"without norm\"}\n",
    "    mode = algorithm_variants.get(mode)\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the dataset. Each sample should be divided by a space and each coordinate within a sample should be divided by a coma. \\nBe careful not to enter spaces after the coma that separates the samples' coordinates.\\n\")\n",
    "    print(\"I.E.: x1=[1,2], x2=[-3,4], x3=[5,3] would be ---> 1,2 -3,4 5,3 \\n\")\n",
    "    dataset_input = str(input())\n",
    "    print(\"\\n\")\n",
    "    dataset = np.array([]).reshape(0,2)\n",
    "    samples = dataset_input.split(' ')\n",
    "    for sample in samples:\n",
    "        coordinates = sample.split(',')\n",
    "        coordinates = np.array([float(c) for c in coordinates])\n",
    "        dataset = np.concatenate((dataset, [coordinates]))\n",
    "    print(\"Please insert the initial centroids. Each centroid should be divided by a space and each coordinate within a centroid should be divided by a coma. \\nBe careful not to enter spaces after the coma that separates the centroids' coordinates.\\n\")\n",
    "    print(\"I.E.: c1=[1,2], c2=[-3,4], c3=[5,3] would be ---> 1,2 -3,4 5,3 \\n\")\n",
    "    centroids_input = str(input())\n",
    "    print(\"\\n\")\n",
    "    clusters = np.array([]).reshape(0,2)\n",
    "    centroids = centroids_input.split(' ')\n",
    "    for centroid in centroids:\n",
    "        coordinates = centroid.split(',')\n",
    "        coordinates = np.array([float(c) for c in coordinates])\n",
    "        clusters = np.concatenate((clusters, [coordinates]))\n",
    "    print(\"Please insert the value of the Learning Rate and then press ENTER (make sure it is a float number - I.E. 0.1)\")\n",
    "    lr = float(input())\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the order to follow when selecting the samples within the algorithm. \\nYou should provide a sequential list of the indexes of the samples to select separated by single spaces.\")\n",
    "    print(\"I.E. 1 2 1 5 3\")\n",
    "    print(\"NOTE: indexes start at 1!\")\n",
    "    order_of_indexes = [ int(o) - 1 for o in str(input()).split(' ')]\n",
    "    print (\"\\n\")\n",
    "    return mode, dataset, clusters, lr, order_of_indexes\n",
    "\n",
    "\n",
    "class CompetitiveLearning:\n",
    "    \"\"\" This class can be used to execute problems regarding Competitive Learning\n",
    "    \"\"\" \n",
    "    def __init__(self, mode, dataset, clusters, lr, order_of_samples):\n",
    "        self.mode = mode # Indicates wether to use normalisation or not. Either \"with norm\" or \"without norm\"\n",
    "        self.dataset = dataset # Dataset of type np.array([[x11, x12], [x12, x22],...,[x1n, x2n]])\n",
    "        self.centroids = clusters # Initial clusters of type np.array([[x11, x12], [x12, x22],...,[x1n, x2n]])\n",
    "        self.lr = lr # Learning rate. Must be a float > 0\n",
    "        self.order_of_samples = order_of_samples # Indicates what order to select the samples in the algorithm. \n",
    "                                                # Of type np.array([int, int, int]) where each int is the corresponding index to the sample in self.dataset\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs the algorithm on the samples listed in self.order_of_samples.\n",
    "            After that, the user can select some more operations to do with the updated clusters.\n",
    "        \"\"\"        \n",
    "        if self.mode == \"with norm\":\n",
    "            self.run_with_normalisation()\n",
    "        elif self.mode == \"without norm\":\n",
    "            self.run_without_normalisation()\n",
    "        user_input = 0\n",
    "        while user_input != 3:\n",
    "            # While the user doesnt selects exit show some options.\n",
    "            # NOTE only applicable when in \"without norm\" mode.\n",
    "            if user_input == 1:\n",
    "                self.classify_samples()\n",
    "            elif user_input == 2:\n",
    "                self.classify_new_data()\n",
    "\n",
    "            print(\"What do you want to do now?\")\n",
    "            print(\"1) Classify all the existing samples\")\n",
    "            print(\"2) Classify a new sample\")\n",
    "            print(\"3) Exit\")\n",
    "            user_input = int(input())\n",
    "\n",
    "    def run_with_normalisation(self):\n",
    "        augmented_dataset = np.array([np.insert(sample, 0, 1) for sample in self.dataset])\n",
    "        normalised_dataset = np.array([np.divide(sample, np.linalg.norm(sample)) for sample in augmented_dataset])\n",
    "        augmented_centroids = np.array([np.insert(sample, 0, 1) for sample in self.centroids])\n",
    "\n",
    "        print(f\"The augmented dataset is :{augmented_dataset}\")\n",
    "        print(f\"The normalised dataset is :{normalised_dataset}\\n\")\n",
    "        for iteration, i in enumerate(self.order_of_samples):\n",
    "            x = normalised_dataset[i]\n",
    "            print(f\"Iteration {iteration+1}:\")\n",
    "            print(f\"Selected x{i+1} {self.dataset[i]} which normalised is --> {x}\")\n",
    "\n",
    "            net_inner_products = np.array([np.multiply(c.transpose(), x) for c in augmented_centroids])\n",
    "            print(f\"The inner products to each centroid with respect to x{i+1} are {net_inner_products}\")\n",
    "            j = np.argmax(np.sum(net_inner_products, axis=1))\n",
    "\n",
    "            rhino_centroid = augmented_centroids[j]\n",
    "            print(f\"The selected centroid is c{j+1} {rhino_centroid}, with a net inner product to {x} of {net_inner_products[j]}\")\n",
    "            \n",
    "            # Update Rhino Centroid\n",
    "            rhino_centroid = np.add(rhino_centroid, np.multiply([self.lr], x))\n",
    "            print(f\"Updated Centroid c{j+1} with respect to x{i+1}: {rhino_centroid}\")\n",
    "            # Normalise Rhino Centroid\n",
    "            rhino_centroid = np.divide(rhino_centroid, np.linalg.norm(rhino_centroid))\n",
    "            print(f\"Normalised Centroid c{j+1} with respect to x{i+1}: {rhino_centroid} \\n\")\n",
    "            augmented_centroids[j] = rhino_centroid\n",
    "\n",
    "        self.centroids = augmented_centroids\n",
    "        print(f\"The final Centroids are: {self.centroids}\\n\")\n",
    "    \n",
    "    def run_without_normalisation(self):\n",
    "        for iteration, i in enumerate(self.order_of_samples):\n",
    "            print(f\"Iteration {iteration+1}:\")\n",
    "            x = self.dataset[i]\n",
    "\n",
    "            distances_to_centroids = np.array([np.linalg.norm(x - c) for c in self.centroids])\n",
    "            j = distances_to_centroids.argmin()\n",
    "\n",
    "            rhino_centroid = self.centroids[j]\n",
    "            print(f\"The selected centroid is c{j+1} {rhino_centroid}, with a distance of {distances_to_centroids[j]} to {x}\")\n",
    "            \n",
    "            # Update Rhino Centroid\n",
    "            rhino_centroid = np.add(rhino_centroid, np.multiply([self.lr], np.subtract(x, rhino_centroid)))\n",
    "            self.centroids[j] = rhino_centroid\n",
    "            print(f\"Updated Centroid with respect to x{i+1}: {rhino_centroid} \\n\")\n",
    "        print(f\"The final Centroids are: {self.centroids}\\n\")\n",
    "    \n",
    "    def classify_samples(self):\n",
    "        \"\"\"Prints what cluster each sample belongs to.\n",
    "            NOTE Only works in 'without norm' mode\n",
    "        \"\"\" \n",
    "        if self.mode == 'without norm':\n",
    "            for j, sample in enumerate(self.dataset):\n",
    "                minimum_distance = None\n",
    "                closest_centroid_index = None\n",
    "                for i, centroid in enumerate(self.centroids):\n",
    "                    dist = np.linalg.norm(sample - centroid)\n",
    "                    if not closest_centroid_index or minimum_distance > dist:\n",
    "                        minimum_distance = dist\n",
    "                        closest_centroid_index = i\n",
    "                print(f\"Sample x{j+1} {sample} belongs to cluster c{closest_centroid_index+1} {self.centroids[closest_centroid_index]}\")\n",
    "            print(\"\\n\")\n",
    "        elif self.mode == 'with norm':\n",
    "            print(\"This feature is not available in mode \\\"with normalisation\\\". No examples were given.\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    \n",
    "    def classify_new_data(self):\n",
    "        \"\"\"Asks the user to input a new sample to classify and says what cluster it belongs to.\n",
    "            NOTE Only works in 'without norm' mode\n",
    "        \"\"\"        \n",
    "        if self.mode == 'without norm':\n",
    "            print(\"Please insert the new sample to classify. Do not insert spaces and divide its coordinates with a coma\")\n",
    "            new_sample = np.array([float(c) for c in str(input()).split(',')])\n",
    "            print('\\n')\n",
    "            minimum_distance = None\n",
    "            closest_centroid_index = None\n",
    "            for i, centroid in enumerate(self.centroids):\n",
    "                dist = np.linalg.norm(new_sample - centroid)\n",
    "                if not closest_centroid_index or minimum_distance > dist:\n",
    "                    minimum_distance = dist\n",
    "                    closest_centroid_index = i\n",
    "            print(f\"The new sample {new_sample} belongs to cluster c{closest_centroid_index+1} {self.centroids[closest_centroid_index]}\")\n",
    "        elif self.mode == 'with norm':\n",
    "            print(\"This feature is not available in mode \\\"with normalisation\\\". No examples were given.\")\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Competitive Algorithm. NOTE youll be asked to enter all the parameters when executing the script\n",
    "    mode, dataset, clusters, lr, order_of_indexes = run_competitive_learning_setup()\n",
    "    cluster = CompetitiveLearning(mode, dataset, clusters, lr, order_of_indexes)\n",
    "    cluster.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def run_hierarchical_clustering_setup():\n",
    "    \"\"\" Ask the user to enter the needed parameters for hierarchical clustering\n",
    "    Returns:\n",
    "        [int, np.array, string]: c -> the number of classes to cluster.\n",
    "                             dataset -> The dataset to cluster.\n",
    "                             similarity_method -> the distancing method to use.\n",
    "    \"\"\"    \n",
    "    print(\"Please enter the number of clusters you want to divide the dataset in and then press ENTER: \")\n",
    "    c = int(input())\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the dataset. Each sample should be divided by a space and each coordinate within a sample should be divided by a coma. \\n Be careful not to enter spaces after the coma that separates the samples' coordinates.\")\n",
    "    print(\"I.E.: x1=[1,2], x2=[-3,4], x3=[5,3] would be ---> 1,2 -3,4 5,3\")\n",
    "    dataset_input = str(input())\n",
    "    print(\"\\n\")\n",
    "    dataset = []\n",
    "    samples = dataset_input.split(' ')\n",
    "    for sample in samples:\n",
    "        coordinates = sample.split(',')\n",
    "        coordinates = [float(c) for c in coordinates]\n",
    "        dataset.append(coordinates)\n",
    "    print(\"Please type the number corresponding to the similarity method to use and press ENTER:\")\n",
    "    print(\"1) Single-link\")\n",
    "    print(\"2) Complete-link\")\n",
    "    print(\"3) Group-average\")\n",
    "    print(\"4) Centroid\")\n",
    "    similarity_method = int(input())\n",
    "    print(\"\\n\")\n",
    "    similarity_options = {1: \"single-link\", 2:\"complete-link\", 3:\"group-average\", 4: \"centroid\"}\n",
    "    similarity_method = similarity_options.get(similarity_method)\n",
    "    return c, dataset, similarity_method\n",
    "\n",
    "\n",
    "class HierarchicalClustering():\n",
    "    \"\"\" This class can be used to execute problems regarding Hierarchical Clustering\n",
    "    \"\"\"    \n",
    "    def __init__(self, n_classes, dataset, similarity_method=\"single-link\"):\n",
    "        self.n = n_classes # Number of classes that need to be found\n",
    "        self.dataset = dataset # Dataset of type np.array([[x11, x12], [x12, x22],...,[x1n, x2n]])\n",
    "        self.similarity_method = similarity_method # either \"single-link\", \"complete-link\", \"group-average\", or \"centroid\"\n",
    "        self.clusters = [np.array([s]) for s in self.dataset] # The initial clusters of type np.array([[x11, x12], [x12, x22],...,[x1n, x2n]])\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run hierarchical clustering on the dataset to find self.n number of clusters.\n",
    "        \"\"\"        \n",
    "        iteration = 1\n",
    "        while(len(self.clusters) != self.n):\n",
    "            closest_clusters = None\n",
    "            closest_distance = None\n",
    "            # Find the two closest clusters\n",
    "            for i, cluster in enumerate(self.clusters):\n",
    "                closest_cluster_to_i, distance_to_i = self.find_closest_cluster(i) #TODO\n",
    "                if not closest_distance or distance_to_i < closest_distance:\n",
    "                    closest_clusters = [i, closest_cluster_to_i]\n",
    "                    closest_distance = distance_to_i\n",
    "            \n",
    "            # Merge closest_clusters\n",
    "            merged_clusters = [self.clusters[closest_clusters[0]], self.clusters[closest_clusters[1]]]\n",
    "            self.merge_clusters(closest_clusters[0], closest_clusters[1])\n",
    "            self.print_iteration(iteration, merged_clusters, closest_distance)\n",
    "            iteration += 1\n",
    "        self.print_final()\n",
    "\n",
    "    def find_closest_cluster(self, cluster_index):\n",
    "        \"\"\"Find the the cluster closest to self.clusters[cluster_index] according to a given similarity method.\n",
    "        Args:\n",
    "            cluster_index (int): The index of the cluster in self.cluster\n",
    "        Returns:\n",
    "            ((int, float)): A pair containing the index of the cluster closest to self.clusters[cluster_index] and its distance.\n",
    "        \"\"\"        \n",
    "        closest_cluster_index = None\n",
    "        closest_distance = None\n",
    "        for i, cluster in enumerate(self.clusters):\n",
    "            similarity_method_options = {\"single-link\": lambda: self.get_single_link_distance(self.clusters[cluster_index], self.clusters[i]),\n",
    "                                            \"complete-link\": lambda: self.get_complete_link_distance(self.clusters[cluster_index], self.clusters[i]),\n",
    "                                            \"group-average\": lambda: self.get_average_link_distance(self.clusters[cluster_index], self.clusters[i]),\n",
    "                                            \"centroid\": lambda: self.get_centroid_distance(self.clusters[cluster_index], self.clusters[i])}\n",
    "            if cluster_index != i:\n",
    "                func = similarity_method_options.get(self.similarity_method, lambda: \"Invalid\")\n",
    "                distance = func()\n",
    "                if not closest_distance or distance < closest_distance:\n",
    "                    closest_cluster_index = i\n",
    "                    closest_distance = distance\n",
    "\n",
    "        return closest_cluster_index, closest_distance\n",
    "\n",
    "\n",
    "            \n",
    "    def get_single_link_distance(self, cluster_a, cluster_b):\n",
    "        minimum_distance = None\n",
    "        for a in cluster_a:\n",
    "            for b in cluster_b:\n",
    "                dist = np.linalg.norm(a-b)\n",
    "                if not minimum_distance or minimum_distance > dist:\n",
    "                    minimum_distance = dist\n",
    "        return minimum_distance \n",
    "\n",
    "    def get_complete_link_distance(self, cluster_a, cluster_b):\n",
    "        maximum_distance = None\n",
    "        for a in cluster_a:\n",
    "            for b in cluster_b:\n",
    "                dist = np.linalg.norm(a-b)\n",
    "                if not maximum_distance or maximum_distance < dist:\n",
    "                    maximum_distance = dist\n",
    "        return maximum_distance \n",
    "    \n",
    "    def get_average_link_distance(self, cluster_a, cluster_b):\n",
    "        distances = np.array([])\n",
    "        for a in cluster_a:\n",
    "            for b in cluster_b:\n",
    "                dist = np.linalg.norm(a-b)\n",
    "                distances = np.append(distances, np.array([dist]))\n",
    "        return np.average(distances) \n",
    "\n",
    "    def get_centroid_distance(self, cluster_a, cluster_b):\n",
    "        centroid_a = np.average(cluster_a) \n",
    "        centroid_b = np.average(cluster_b) \n",
    "        dist = np.linalg.norm(centroid_a - centroid_b)\n",
    "        return dist \n",
    "    \n",
    "    def merge_clusters(self, cluster_index_a, cluster_index_b):\n",
    "        merged_cluster = np.concatenate((self.clusters[cluster_index_a], self.clusters[cluster_index_b]))\n",
    "        self.clusters = [self.clusters[i] for i in range(0, len(self.clusters)) if i != cluster_index_a and i != cluster_index_b]\n",
    "        self.clusters.append(merged_cluster)\n",
    "\n",
    "    def print_iteration(self, i, merged_clusters, distance):\n",
    "        print(f\"End of iteration {str(i)} \" )\n",
    "        print(f\"Merged clusters {merged_clusters[0]} and {merged_clusters[1]}. Distance between the clusters was {distance}.\")\n",
    "        print(f\"There are {str(len(self.clusters))} clusters:\")\n",
    "        for j, c in enumerate(self.clusters):\n",
    "            print(f\"Cluster {j} -> {self.clusters[j]}\")\n",
    "        print (\"\\n\")\n",
    "    \n",
    "    def print_final(self):\n",
    "        print(\"FINAL CLUSTERS:\")\n",
    "        for j, c in enumerate(self.clusters):\n",
    "            print(f\"Cluster {j} -> {self.clusters[j]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Hierarchical Clustering Exercise. NOTE youll be asked to enter all the parameters when executing the script\n",
    "    c, dataset, similarity_method = run_hierarchical_clustering_setup()\n",
    "    cluster = HierarchicalClustering(c, dataset, similarity_method)\n",
    "    cluster.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialisation - Parameters from question\n",
    "k = 2 # Number of clusters to form\n",
    "centers = {  # Initial clusters' centers\n",
    "    0: [-1, 3],\n",
    "    1: [5, 1],\n",
    "}\n",
    "\n",
    "x = np.array([  # Dataset\n",
    "    [-1, 3],\n",
    "    [1, 4],\n",
    "    [0, 5],\n",
    "    [4, -1],\n",
    "    [3, 0],\n",
    "    [5, 1]\n",
    "])\n",
    "\n",
    "try:\n",
    "    assert(k == len(centers))\n",
    "except AssertionError as e:\n",
    "    e.args += ('The number of initialised clusers doesn\\'t match k',\n",
    "               len(centers), k)\n",
    "    raise\n",
    "\n",
    "print(\n",
    "    f'The parameters are: k = {k} and the centers are {centers}')\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def distance(feature, centers, method='euclidian'):\n",
    "    if method == 'euclidian':\n",
    "        return [np.linalg.norm(feature-centers[center])  # euclidian norm\n",
    "                for center in centers]\n",
    "    if method == 'manhatan':\n",
    "        return [np.linalg.norm(feature-centers[center], 1)  # manhatan dist\n",
    "                for center in centers]\n",
    "\n",
    "\n",
    "previous = {}\n",
    "for i in range(5):  # Max number of iterations (I don't think we'd be expected to perform more than 5 iterations)\n",
    "    print(f'\\n Iteration number {i+1}: \\n')\n",
    "    classes = {}  # Dict to hold a list of data points that are closest to the cluster number\n",
    "    for j in range(k):\n",
    "        classes[j] = []  # Instantiate empty list at every iteration\n",
    "    for feature in x:  # For each data point do:\n",
    "        # Compute the distance to each cluster (options: euclidian or manhatan)\n",
    "        distances = distance(feature, centers, 'euclidian')\n",
    "\n",
    "        classification = distances.index(\n",
    "            min(distances))  # Find the lowest distance\n",
    "        # Assign the datapoint to that cluster\n",
    "        classes[classification].append(feature)\n",
    "\n",
    "    # Print to which cluster each data point belongs to\n",
    "    print(f'The classification is: {classes}')\n",
    "\n",
    "    previous = centers.copy()  # Copy the cluster centers dict\n",
    "    print(f'The previous centers are:{previous}')\n",
    "    for classification in classes:\n",
    "        # Compute the new cluster average by taking the mean of all the data point assigned to that cluster\n",
    "        centers[classification] = np.average(classes[classification], axis=0)\n",
    "    print(f'The new centers are:{centers}')\n",
    "\n",
    "    opti = True\n",
    "    for center in centers:\n",
    "        prev = previous[center]  # Previous cluster\n",
    "        curr = centers[center]  # Update cluster\n",
    "        # termination criteria #TODO: IMPORTANT: This can be changed in the exam question!!\n",
    "        if np.sum(curr-prev) != 0:\n",
    "            opti = False\n",
    "    if opti:\n",
    "        break\n",
    "\n",
    "print('\\n The algorithm converged!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### K-Fuzzy Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def fuzzy_k_means(data, K, b, iterations, weights):\n",
    "    data = np.array(data)\n",
    "    weights = np.array(weights)\n",
    "    centers = [None] * K\n",
    "    # centers = np.array(centers)\n",
    "    data = np.array(data)\n",
    "    for _iter in range(iterations):\n",
    "        # calculate centers\n",
    "        for idx, w in enumerate(weights):\n",
    "            new_center = [None] * data.shape[1]\n",
    "            for idy in range(data.shape[1]):\n",
    "                new_center[idy] = w[idy]**b * data[:, idy]\n",
    "            centers[idx] = (np.array(new_center).sum(axis=0)/(w**b).sum())\n",
    "        # print(\"Centers\", centers)\n",
    "        new_weight_container = [None] * data.shape[1]\n",
    "        for idx in range(data.shape[1]):\n",
    "            new_weights = []\n",
    "            for c in centers:\n",
    "                val = np.linalg.norm(c - data[:, idx])\n",
    "\n",
    "                val = (1 / val) ** (2/(b-1))\n",
    "                new_weights.append(val)\n",
    "            new_weights = np.array(new_weights)\n",
    "            nw_sum = new_weights.sum()\n",
    "            for idy, nw in enumerate(new_weights):\n",
    "                new_weights[idy] = nw/nw_sum\n",
    "            new_weight_container[idx] = new_weights\n",
    "\n",
    "        new_weight_container = np.array(new_weight_container)\n",
    "        for col_idx in range(new_weight_container.shape[1]):\n",
    "            weights[col_idx] = new_weight_container[:, col_idx]\n",
    "        print(F\"CENTERS AFTER ITERATION {_iter+1} [READ COLUMN WISE]\")\n",
    "        print(np.array(centers).T)\n",
    "        print(\"#\"*100)\n",
    "        print(f\"WEIGHTS AFTER ITERATION {_iter + 1} [READ COLUMN WISE]\")\n",
    "        print(weights.T)\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# REPLACE A VALUES AS GIVEN IN THE QUESTION\n",
    "data = [[-1, 1, 0, 4, 3, 5], [3, 4, 5, -1, 0, 1]]\n",
    "fuzzy_k_means(data=data, K=2, weights=[\n",
    "              [1, 0.5, 0.5, 0.5, 0.5, 0], [0, 0.5, 0.5, 0.5, 0.5, 1]], b=2, iterations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Leader-Follower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def run_leader_follower_setup():\n",
    "    \"\"\" Ask the user to enter the needed parameters\n",
    "    Returns:\n",
    "        [string, np.array, float, float, np.array]: \n",
    "                             mode -> Specify if using normalisation or not.\n",
    "                             dataset -> The dataset to cluster.\n",
    "                             lr -> The learning Rate \n",
    "                             theta -> The threshold theta.\n",
    "                             order_of_indexes -> The order to follow when selecting samples in the algorithm\n",
    "    \"\"\"    \n",
    "    print(\"How you want to run the algorithm? (Enter the corresponding number and press ENTER)\")\n",
    "    print(\"1) With normalisation and argumentation\")\n",
    "    print(\"2) Without normalisation and argumentation\")\n",
    "    mode = int(input())\n",
    "    algorithm_variants = {1: \"with norm\", 2:\"without norm\"}\n",
    "    mode = algorithm_variants.get(mode)\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the dataset. Each sample should be divided by a space and each coordinate within a sample should be divided by a coma. \\nBe careful not to enter spaces after the coma that separates the samples' coordinates.\\n\")\n",
    "    print(\"I.E.: x1=[1,2], x2=[-3,4], x3=[5,3] would be ---> 1,2 -3,4 5,3 \\n\")\n",
    "    dataset_input = str(input())\n",
    "    print(\"\\n\")\n",
    "    dataset = np.array([]).reshape(0,2)\n",
    "    samples = dataset_input.split(' ')\n",
    "    for sample in samples:\n",
    "        coordinates = sample.split(',')\n",
    "        coordinates = np.array([float(c) for c in coordinates])\n",
    "        dataset = np.concatenate((dataset, [coordinates]))\n",
    "    print(\"Please insert the value of the learning rate and then press ENTER (make sure it is a float number - I.E. 0.1)\")\n",
    "    lr = float(input())\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the value of the the threshold theta and then press ENTER (make sure it is a float number - I.E. 2.0)\")\n",
    "    theta = float(input())\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the order to follow when selecting the samples within the algorithm. \\nYou should provide a sequential list of the indexes of the samples to select separated by single spaces.\")\n",
    "    print(\"I.E. 1 2 1 5 3\")\n",
    "    print(\"NOTE: indexes start at 1!\")\n",
    "    order_of_indexes = [ int(o) - 1 for o in str(input()).split(' ')]\n",
    "    print (\"\\n\")\n",
    "    return mode, dataset, lr, theta, order_of_indexes\n",
    "\n",
    "\n",
    "class LeaderFollower:\n",
    "    \"\"\" This class can be used to execute problems regarding Leader Follower\n",
    "    \"\"\" \n",
    "    def __init__(self, mode, dataset, lr, theta, order_of_samples):\n",
    "        self.mode = mode # Indicates wether to use normalisation or not. Either \"with norm\" or \"without norm\"\n",
    "        self.dataset = dataset # Dataset of type np.array([[x11, x12], [x12, x22],...,[x1n, x2n]])\n",
    "        self.lr = lr # Learning rate. Must be a float > 0\n",
    "        self.theta = theta # The threshold to use in the algorithm.\n",
    "        self.order_of_indexes = order_of_indexes # Indicates what order to select the samples in the algorithm. \n",
    "                                                # Of type np.array([int, int, int]) where each int is the corresponding index to the sample in self.dataset\n",
    "        self.centroids = np.array([]) # Empty np.array of initial centroids\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Runs the algorithm on the samples listed in self.order_of_samples.\n",
    "            After that, the user can select some more operations to do with the updated clusters.\n",
    "        \"\"\"        \n",
    "        if self.mode == \"with norm\":\n",
    "            self.run_with_normalisation()\n",
    "        elif self.mode == \"without norm\":\n",
    "            self.run_without_normalisation()\n",
    "        user_input = 0\n",
    "        while user_input != 3:\n",
    "            # While the user doesnt selects exit show some options.\n",
    "            # NOTE only applicable when in \"without norm\" mode.\n",
    "            if user_input == 1:\n",
    "                self.classify_samples()\n",
    "            elif user_input == 2:\n",
    "                self.classify_new_data()\n",
    "\n",
    "            print(\"What do you want to do now?\")\n",
    "            print(\"1) Classify all the existing samples\")\n",
    "            print(\"2) Classify a new sample\")\n",
    "            print(\"3) Exit\")\n",
    "            user_input = int(input())\n",
    "\n",
    "    def run_without_normalisation(self):\n",
    "        # Shape the list of centroid to the right shape\n",
    "        self.centroids = np.array([]).reshape(0, self.dataset.shape[1])\n",
    "        # Initialise first centroid\n",
    "        self.centroids = np.concatenate( (self.centroids, [self.dataset[self.order_of_indexes[0]]]) )\n",
    "        for iteration, i in enumerate(self.order_of_indexes):\n",
    "            x = self.dataset[i]\n",
    "            print(f\"Iteration {iteration}:\")\n",
    "            print(f\"Selected x{i+1} {x}\")\n",
    "\n",
    "            distances_to_centroids = np.array([np.linalg.norm(x - c) for c in self.centroids])\n",
    "            print(f\"Distances to each centroids are {distances_to_centroids}\")\n",
    "            j = distances_to_centroids.argmin()\n",
    "            rhino_centroid = self.centroids[j]\n",
    "            print(f\"The closest centroid is c{j+1} {rhino_centroid}\")\n",
    "\n",
    "            if np.linalg.norm(x - rhino_centroid) < self.theta:\n",
    "                print(f\"C{j+1} is within the threshold\")\n",
    "                self.centroids[j] = np.add( rhino_centroid, np.multiply(self.lr, np.subtract(x, rhino_centroid)) )\n",
    "                print(f\"Updated centroid c{j+1} to be {self.centroids[j]}\\n\")\n",
    "            else:\n",
    "                print(f\"C{j+1} is not within the threshold\")\n",
    "                self.centroids = np.concatenate( (self.centroids, [x]) )\n",
    "                print(f\"Added new Centroid {x}\\n\")\n",
    "        \n",
    "        print(f\"The final Centroids are: {self.centroids}\\n\")\n",
    "\n",
    "    def run_with_normalisation(self):\n",
    "\n",
    "        augmented_dataset = np.array([np.insert(sample, 0, 1) for sample in self.dataset])\n",
    "        normalised_dataset = np.array([np.divide(sample, np.linalg.norm(sample)) for sample in augmented_dataset])\n",
    "\n",
    "        print(f\"The augmented dataset is :{augmented_dataset}\")\n",
    "        print(f\"The normalised dataset is :{normalised_dataset}\\n\")\n",
    "\n",
    "        # Shape the list of centroid to the right shape (it must be augmented)\n",
    "        self.centroids = np.array([]).reshape(0, normalised_dataset.shape[1])\n",
    "\n",
    "        # Initialise first centroid\n",
    "        self.centroids = np.concatenate( (self.centroids, [normalised_dataset[self.order_of_indexes[0]]]) )\n",
    "\n",
    "        for iteration, i in enumerate(self.order_of_indexes):\n",
    "            x = normalised_dataset[i]\n",
    "            print(f\"Iteration {iteration + 1}:\")\n",
    "            print(f\"Selected x{i+1} {self.dataset[i]} which normalised is --> {x}\")\n",
    "\n",
    "            net_inner_products = np.array([np.multiply(c.transpose(), x) for c in self.centroids])\n",
    "            print(f\"The inner products to each centroid with respect to x{i+1} are {net_inner_products}\")\n",
    "            j = np.argmax(np.sum(net_inner_products, axis=1))\n",
    "            rhino_centroid = self.centroids[j]\n",
    "            print(f\"The closest centroid is c{j+1} {rhino_centroid}\")\n",
    "\n",
    "            if np.linalg.norm(x - rhino_centroid) < self.theta:\n",
    "                print(f\"C{j+1} is within the threshold, as {np.linalg.norm(x - rhino_centroid)} < {self.theta}\")\n",
    "                rhino_centroid = np.add(rhino_centroid, np.multiply([self.lr], x)) # Update cluster center\n",
    "                print(f\"Updated Centroid C{j+1} with respect to x{i+1}: {rhino_centroid}\") \n",
    "                rhino_centroid = np.divide(rhino_centroid, np.linalg.norm(rhino_centroid)) # Normalise updated cluster center\n",
    "                print(f\"Normalised Centroid c{j+1} with respect to x{i+1}: {rhino_centroid} \\n\")\n",
    "                self.centroids[j] = rhino_centroid # Actually update the new cetroid\n",
    "            else:\n",
    "                print(f\"C{j+1} is not within the threshold, as {np.linalg.norm(x - rhino_centroid)} > {self.theta}\")\n",
    "                new_centroid = np.divide(x, np.linalg.norm(x)) # Create new centroid\n",
    "                self.centroids = np.concatenate( (self.centroids, [new_centroid]) )\n",
    "                print(f\"Added new centroid c{len(self.centroids - 1)} {new_centroid}\")\n",
    "        \n",
    "        print(f\"The final Centroids are: {self.centroids}\\n\")\n",
    "\n",
    "    def classify_samples(self):\n",
    "        if self.mode == 'without norm':\n",
    "            for j, sample in enumerate(self.dataset):\n",
    "                minimum_distance = None\n",
    "                closest_centroid_index = None\n",
    "                for i, centroid in enumerate(self.centroids):\n",
    "                    dist = np.linalg.norm(sample - centroid)\n",
    "                    if not minimum_distance or minimum_distance > dist:\n",
    "                        minimum_distance = dist\n",
    "                        closest_centroid_index = i\n",
    "                print(f\"Sample x{j+1} {sample} belongs to cluster c{closest_centroid_index+1} {self.centroids[closest_centroid_index]}\")\n",
    "            print(\"\\n\")\n",
    "        elif self.mode == 'with norm':\n",
    "            print(\"This feature is not available in mode \\\"with normalisation\\\". No examples were given.\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    \n",
    "    def classify_new_data(self):\n",
    "        if self.mode == 'without norm':\n",
    "            print(\"Please insert the new sample to classify. Do not insert spaces and divide its coordinates with a coma\")\n",
    "            new_sample = np.array([float(c) for c in str(input()).split(',')])\n",
    "            print('\\n')\n",
    "            minimum_distance = None\n",
    "            closest_centroid_index = None\n",
    "            for i, centroid in enumerate(self.centroids):\n",
    "                dist = np.linalg.norm(new_sample - centroid)\n",
    "                if not closest_centroid_index or minimum_distance > dist:\n",
    "                    minimum_distance = dist\n",
    "                    closest_centroid_index = i\n",
    "            print(f\"The new sample {new_sample} belongs to cluster c{closest_centroid_index+1} {self.centroids[closest_centroid_index]}\")\n",
    "        elif self.mode == 'with norm':\n",
    "            print(\"This feature is not available in mode \\\"with normalisation\\\". No examples were given.\")\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    # Leader follower algorithm.  NOTE youll be asked to enter all the parameters when executing the script\n",
    "    mode, dataset, lr, theta, order_of_indexes = run_leader_follower_setup()\n",
    "    cluster = LeaderFollower(mode, dataset, lr, theta, order_of_indexes)\n",
    "    cluster.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "\n",
    "def _PCA(ip, n_components, data_to_project=None):\n",
    "    ip = np.array(ip)\n",
    "    ip_mean = np.mean(ip, axis=1)\n",
    "    ip_prime = ip - np.vstack(ip_mean)\n",
    "    C = (ip_prime @ ip_prime.T) / ip.shape[1]\n",
    "    V, D, VT = svd(C)\n",
    "    ans = VT @ ip_prime\n",
    "    print(\"-\"*100)\n",
    "    print(\"READ THE ROWS FROM THE TOP\")\n",
    "    print(ans[:n_components])\n",
    "    print(\"-\"*100)\n",
    "    if data_to_project:\n",
    "        data_to_project = np.array(data_to_project)\n",
    "        print(\"-\"*100)\n",
    "        print(f\"PROJECTION OF {data_to_project}\")\n",
    "        print((VT@data_to_project)[:n_components])\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# REPLACE ACCORDING TO THE QUESTION\n",
    "ip = [[4, 0, 2, -2], [2, -2, 4, 0], [2, 2, 2, 2]]\n",
    "n_components = 2\n",
    "data_to_project = [3, -2, 5]\n",
    "_PCA(ip=ip, n_components=n_components, data_to_project=data_to_project)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
