{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Libraries and Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score\n",
    "\n",
    "\n",
    "def transformToList(inputString):\n",
    "    s = inputString.translate(str.maketrans({'{': '[', '}': ']', '(': '[', ')': ']'}))\n",
    "    s = s.replace('[', ' [ ')\n",
    "    s = s.replace(']', ' ] ')\n",
    "    s = s.replace(',', ' , ')\n",
    "    words = s.split()\n",
    "    output = \"\"\n",
    "    for word in words:\n",
    "        if word==\"[\" or word==\"]\" or word==\",\": output+=word\n",
    "        else: output+='\"'+word+'\"'\n",
    "    out = json.loads(output)\n",
    "    try:\n",
    "        a = np.array(out).astype(np.float).tolist()\n",
    "    except:\n",
    "        a = out\n",
    "    return a\n",
    "\n",
    "class Activation:\n",
    "    def stepFunction(u):\n",
    "        if u>0: return 1\n",
    "        elif u<0: return 0\n",
    "        else: return 0.5\n",
    "\n",
    "\n",
    "class Utility:\n",
    "    def augmentArray(a, value=1, position=0):\n",
    "        return np.insert(a, position, value, axis=len(a.shape)-1)\n",
    "    \n",
    "    def sampleNorm(data, target, mainClass = 1):\n",
    "        if len(data) != len(target):\n",
    "            print(\"incompatible array sizes - sampleNorm\")\n",
    "            return\n",
    "        for i in range(len(target)):\n",
    "            if target[i] != mainClass:\n",
    "                data[i] = [-x for x in data[i]]\n",
    "        return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Confusion Matrix and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Week1:\n",
    "    def basic_metrics(y_true, y_pred, class_names, normalize=False):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        cm = cm[:,::-1][::-1]\n",
    "        np.set_printoptions(precision=4)\n",
    "\n",
    "        title='Confusion matrix'\n",
    "        cmap=plt.cm.Blues\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names, rotation=45)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names[::-1],digits=4))\n",
    "        plt.show()\n",
    "        \n",
    "    def setParams():\n",
    "        class_names = transformToList(input(\"Enter names of classes in descending order eg [Two, One, Zero]:  \"))\n",
    "        y_true = transformToList(input(\"Enter true value of labels eg [1,1,0,1,0,1,1,2] 1D:  \"))\n",
    "        y_pred = transformToList(input(\"Enter predicted value of labels eg [1,0,1,1,0,1,0,2] 1D:  \"))\n",
    "        return class_names, y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class_names, y_true, y_pred = Week1.setParams()\n",
    "Week1.basic_metrics(y_true, y_pred, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Batch Perceptron Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BatchPerceptronLearning:\n",
    "    def fit(X, Y, a, n):\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Applying Sample Normalisation:\n",
    "        Norm_Y = []\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "\n",
    "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
    "            if y != 1:\n",
    "                x = [i * -1 for i in x]\n",
    "                x.insert(0, -1)\n",
    "                Norm_Y.append(x)\n",
    "            else:\n",
    "                x.insert(0, 1)\n",
    "                Norm_Y.append(x)\n",
    "\n",
    "        print(\"Vectors used in Batch Perceptron Learning Algorithm:\\n {}\\n\".format(Norm_Y))\n",
    "\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Batch Perceptron Learning Algorithm:\n",
    "\n",
    "        epoch = 1\n",
    "\n",
    "        while True:\n",
    "\n",
    "            updating_samples = []\n",
    "            print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "            for count, i in enumerate(range(len(Norm_Y))):\n",
    "\n",
    "                # Knowing which value of a to use. If it is the first iteration, than use the given parameters in the \n",
    "                # question:\n",
    "                a_prev = a\n",
    "                print(\"The value of a used is {}\".format(a_prev))\n",
    "                y_input = Norm_Y[i]\n",
    "                print(\"y Value used for this iteration is: {}\".format(y_input))\n",
    "\n",
    "                # Equation -> g(x) = a^{t}y\n",
    "                ay = np.dot(a, y_input)\n",
    "                print(\"The value of a^t*y for this iteration is: {}\".format(ay))\n",
    "\n",
    "\n",
    "                # Checking if the sample is misclassified or not:\n",
    "\n",
    "                # If sample is misclassified:\n",
    "                if ay <= 0:\n",
    "\n",
    "                    # If this is the first sample in the epoch, add the previous value of a to the list of samples used \n",
    "                    # for the update to perform summation at the end of the epoch:\n",
    "                    if count == 0:\n",
    "                        print(\"This sample is misclassified. This sample will be used in update.\\n\")\n",
    "                        updating_samples.append(np.array(a))\n",
    "                        updating_samples.append(np.array(y_input))\n",
    "\n",
    "                    # If sample is misclassified and IS NOT the first sample in the epoch:\n",
    "                    else:\n",
    "                        print(\"This sample is misclassified. This sample will be used in update.\\n\")\n",
    "                        updating_samples.append(np.array(y_input))\n",
    "\n",
    "                # If sample is classified correctly:\n",
    "                else: \n",
    "\n",
    "                    # If first sample in the epoch, append the previous value of a to the updating samples list:\n",
    "                    if count == 0:\n",
    "                        updating_samples.append(np.array(a))\n",
    "                        print(\"This sample is classified correctly.\\n\")\n",
    "                    else:\n",
    "                        print(\"This sample is classified correctly.\\n\")\n",
    "\n",
    "            # Calculating new value of a after having gone through all of the samples in the dataset since it is Batch Learning.\n",
    "            a_update_val = n * sum(updating_samples)\n",
    "\n",
    "            # If Block to check whether learning has converged. If we have gone through all the data without needing \n",
    "            # to update the parameters, we can conclude that learning has converged.\n",
    "            if len(updating_samples) <= 1:\n",
    "                print(\"\\nLearning has converged.\")\n",
    "                print(\"Required parameters of a are: {}\".format(a))\n",
    "                break\n",
    "\n",
    "            # Updating a using our new value of a:\n",
    "            a = a_update_val\n",
    "            print(\"\\nNew Value of a^t is: {}.\\n\".format(a))\n",
    "\n",
    "            epoch += 1\n",
    "        \n",
    "    def setParams():\n",
    "        X_train = transformToList(input(\"Enter features eg [[1, 5], [2, 5], [4, 1], [5, 1]] 2D:  \"))\n",
    "        y_train = transformToList(input(\"Enter labels eg [1, 1, 2, 2] 1D:  \"))\n",
    "        a = transformToList(input(\"Enter a. Usually [1, w1, w2...] eg [-25, 6, 3] 1D:  \"))\n",
    "        lr = float(input(\"Enter the learning rate eg 1.  \"))\n",
    "        return X_train, y_train, a, lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, a, lr = BatchPerceptronLearning.setParams()\n",
    "BatchPerceptronLearning.fit(X_train, y_train, a, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sequential Perceptron Learing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SequentialPerceptronLearning:\n",
    "    def fit(X, Y, a, n):\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Applying Sample Normalisation:\n",
    "        Norm_Y = []\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "\n",
    "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
    "            if y != 1:\n",
    "                x = [i * -1 for i in x]\n",
    "                x.insert(0, -1)\n",
    "                Norm_Y.append(x)\n",
    "            else:\n",
    "                x.insert(0, 1)\n",
    "                Norm_Y.append(x)\n",
    "\n",
    "        print(\"Vectors used in Sequential Perceptron Learning Algorithm:\\n {}\\n\".format(Norm_Y))\n",
    "\n",
    "\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Sequential Perceptron Learning Algorithm:\n",
    "\n",
    "        epoch = 1\n",
    "\n",
    "        while True:\n",
    "\n",
    "            updating_samples = []\n",
    "            print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "            # Keeping track of how many samples are correctly classified. If this variable reaches \n",
    "            # the value that is equal to the size of the dataset (len), than we know that learning \n",
    "            # has converged:\n",
    "            correctly_classified_counter = 0\n",
    "\n",
    "            # Going through all of the samples in the dataset one-by-one:\n",
    "            for i in range(len(Norm_Y)):\n",
    "\n",
    "                # This chooses which weight to use for an iteration. If first iteration, uses given starting weight \n",
    "                # as described in question:\n",
    "                a_prev = a\n",
    "                print(\"The value of a used is {}\".format(a_prev))\n",
    "\n",
    "                # Selecting sample to use:\n",
    "                y_input = Norm_Y[i]\n",
    "                print(\"y Value used for this iteration is: {}\".format(y_input))\n",
    "\n",
    "                # Equation -> g(x) = a^{t}y\n",
    "                ay = np.dot(a, y_input)\n",
    "                print(\"The value of a^t*y for this iteration is: {}\".format(ay))\n",
    "\n",
    "\n",
    "                # Checking if the sample is misclassified or not:\n",
    "\n",
    "                # If sample is misclassified:\n",
    "                if ay <= 0:\n",
    "\n",
    "                    print(\"This sample is misclassified. This sample will be used in update.\\n\")\n",
    "                    updating_samples.append(np.array(a))\n",
    "                    updating_samples.append(np.array(y_input))\n",
    "\n",
    "                    # Calculating new value of a using update rule for Sequential Perceptron Learning Algorithm:\n",
    "                    a_update_val = n * sum(updating_samples)\n",
    "\n",
    "                    a = a_update_val\n",
    "                    print(\"\\nNew Value of a^t is: {}.\\n\".format(a))\n",
    "\n",
    "                # If the sample is correctly classified:\n",
    "                else: \n",
    "                    print(\"This sample is classified correctly.\\n\")\n",
    "                    correctly_classified_counter += 1\n",
    "                    pass\n",
    "\n",
    "                # Reset sample to add for update to occur:\n",
    "                updating_samples = []\n",
    "\n",
    "            # If Block to check whether learning has converged. If we have gone through all the data without needing \n",
    "            # to update the parameters, we can conclude that learning has converged.\n",
    "            if correctly_classified_counter == len(Norm_Y):\n",
    "                print(\"\\nLearning has converged.\")\n",
    "                print(\"Required parameters of a are: {}.\".format(a))\n",
    "                break\n",
    "\n",
    "            epoch += 1\n",
    "        \n",
    "    def setParams():\n",
    "        X_train = transformToList(input(\"Enter features eg [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]] 2D:  \"))\n",
    "        y_train = transformToList(input(\"Enter labels eg [1, 1, 1, -1, -1, -1] 1D:  \"))\n",
    "        a = transformToList(input(\"Enter a. Usually [1, w1, w2...] eg [1, 0, 0] 1D:  \"))\n",
    "        lr = float(input(\"Enter the learning rate eg 1.  \"))\n",
    "        return X_train, y_train, a, lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, a, lr = SequentialPerceptronLearning.setParams()\n",
    "SequentialPerceptronLearning.fit(X_train, y_train, a, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiClass perceptron learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialMultiClassPerceptronLearning:\n",
    "    def fit(X,y, a, eta):\n",
    "    # N is the number of exemplars provided in the question\n",
    "    # augmented_matrix is the augmented feature vector from the question\n",
    "    # eta is the learning rate given in the question\n",
    "    # omega is an array containing all the output classes of the feature vectors\n",
    "        augmented_matrix = np.array(X).T\n",
    "    # counter which keeps track of cases where winner_class == omega[index]\n",
    "        N_counter = 0\n",
    "        \n",
    "        omega = np.array(y).astype(int).tolist()\n",
    "        N = augmented_matrix.shape[1]\n",
    "        number_of_classes = len(set(omega))\n",
    "        number_of_features = augmented_matrix.shape[0]\n",
    "        \n",
    "        print(f'class nb:{number_of_classes} and number of features {number_of_features} and number of shit {N}')\n",
    "        # Step 2. Initialise aj for each class\n",
    "        at = np.array(a)\n",
    "        #at = np.zeros((number_of_classes, number_of_features))\n",
    "        \n",
    "        for i in range(0, 15):\n",
    "            print('Iteration: ', i+1)\n",
    "            # Step 3. Find values of g1, g2 and g3 and then select the arg max of g\n",
    "            index = i % N\n",
    "\n",
    "            # Print updated a^t value\n",
    "            print('a^t:')\n",
    "            print(at)\n",
    "\n",
    "            # Compute g value\n",
    "            g = np.empty([number_of_classes])\n",
    "            for i in range(len(g)):\n",
    "                print('Calculation of g values..........')\n",
    "                print('a^t is:', at[i])\n",
    "                print('Index is:', index)\n",
    "                print('Aug matrix is:', augmented_matrix[:, index])\n",
    "                g[i] = at[i] @ augmented_matrix[:, index]\n",
    "\n",
    "            print('g1 | g2 | g3')\n",
    "            print(g)\n",
    "\n",
    "            # Step 4. Select the winner\n",
    "            # Logic for 0,0,0 case and similar ones where 2 gs can produce max value\n",
    "            seen = []\n",
    "            bRepeated = False\n",
    "            # Check if there are multiple max values, and assign the winner class accordingly\n",
    "            for number in g:\n",
    "                if number in seen:\n",
    "                    bRepeated = True\n",
    "                    print(\"Number repeated!\")\n",
    "                    m = max(g)\n",
    "                    temp = [index for index, j in enumerate(g) if j == m]\n",
    "                    winner_class = max(temp) + 1\n",
    "                else:\n",
    "                    seen.append(number)\n",
    "            # If all g values are unique, simply select the max value's class as the winner\n",
    "            if(bRepeated == False):\n",
    "                g = g.tolist()\n",
    "                arg_max = max(g)\n",
    "                winner_class = g.index(arg_max) + 1\n",
    "\n",
    "            print('Winner class = ', winner_class,\n",
    "                  ', and actual class is:', omega[index])\n",
    "\n",
    "            # Compare winnner to actual class\n",
    "            if(winner_class != omega[index]):\n",
    "                # Step 4. Apply the update rule as per the algorithm\n",
    "\n",
    "                # Increment the actual class value which is incorrectly classified\n",
    "                at[omega[index]-1] = at[omega[index]-1] + \\\n",
    "                    eta * augmented_matrix[:, index]\n",
    "                print('New loser value:', at[omega[index]-1])\n",
    "\n",
    "                # Penalize the wrongly predicted Winner class\n",
    "                at[winner_class-1] = at[winner_class-1] - \\\n",
    "                    eta * augmented_matrix[:, index]\n",
    "                print('New winner value:', at[winner_class-1])\n",
    "\n",
    "                # Reset counter to 0\n",
    "                N_counter = 0\n",
    "            else:\n",
    "                print('No update is performed!')\n",
    "                # Increment convergence counter which keeps track of cases where winner_class == omega[index]\n",
    "                N_counter += 1\n",
    "                if(N_counter == N):  # check for convergence\n",
    "                    print('Value of N = ', N)\n",
    "                    print('Value of N_counter = ', N_counter)\n",
    "                    print('Learning has converged, so stopping...')\n",
    "                    print('Final values of a^t after update....')\n",
    "                    print('at')\n",
    "                    print(at)\n",
    "                    break\n",
    "                print('N counter value = ', N_counter)\n",
    "            print('at')\n",
    "            print(at)\n",
    "            print('=========================================================')\n",
    "\n",
    "\n",
    "    def setParams():\n",
    "        X_train = transformToList(input(\"Enter features eg [[1,0, 2], [1, 1, 2], [1, 2, 1], [1, -3, 1], [1, -2, -1], [1, -3, -2]] 2D:  \"))\n",
    "        y_train = transformToList(input(\"Enter labels eg [1, 1, 1, -1, -1, -1] 1D:  \"))\n",
    "        a = transformToList(input(\"Enter a. Usually [1, w1, w2...] eg [1, 0, 0] 1D:  \"))\n",
    "        lr = float(input(\"Enter the learning rate eg 1.  \"))\n",
    "        print(X_train, y_train, a, lr)\n",
    "        return X_train, y_train, a, lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, a, lr = SequentialMultiClassPerceptronLearning.setParams()\n",
    "SequentialMultiClassPerceptronLearning.fit(X_train, y_train, a, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Moore-Penrose Pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Pseudoinverse:\n",
    "    def fit(X, Y, b):\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Applying Sample Normalisation:\n",
    "        Norm_Y = []\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "\n",
    "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
    "            if y == -1 or y == 2:\n",
    "                x = [i * -1 for i in x]\n",
    "                x.insert(0, y)\n",
    "                Norm_Y.append(x)\n",
    "            else:\n",
    "                x.insert(0, y)\n",
    "                Norm_Y.append(x)\n",
    "\n",
    "        print(\"Vectors used in Pseudoinverse operation to calculate parameters of linear discriminant function:\\n {}\\n\".format(Norm_Y))\n",
    "\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Initialising Y Matrix:\n",
    "        Y_matrix = []\n",
    "\n",
    "        # Adding each normalised sample in dataset to Y Matrix:\n",
    "        for i in range(len(Norm_Y)):\n",
    "            Y_matrix.append(Norm_Y[i])\n",
    "        Y_matrix = np.array(Y_matrix)\n",
    "        print(\"y Matrix being used:\\n {}\\n\".format(Y_matrix))\n",
    "\n",
    "        # Calculating pseudo-inverse of Y Matrix:\n",
    "        pseudo_inv_matrix = np.linalg.pinv(Y_matrix)\n",
    "        print(\"Pseudo-inverse Matrix is:\\n {}\\n\".format(pseudo_inv_matrix))\n",
    "\n",
    "        # Multiplying Pseudo-inverse matrix by given margin vector in question:\n",
    "        a = np.matmul(pseudo_inv_matrix, b)\n",
    "        print(\"a is equal to:\\n {}\\n\".format(a))\n",
    "\n",
    "        correct_classification = 0\n",
    "\n",
    "        # Checking if classifications are correct:\n",
    "\n",
    "        for sample in Norm_Y:\n",
    "            ay = np.dot(sample, a)\n",
    "            print(\"\\ng(x) for sample {} is {}\".format(sample, ay))\n",
    "\n",
    "            # Sample is correctly classified if ay is positive:    \n",
    "            if ay > 0:\n",
    "                print(\"Sample has been correctly classified.\")\n",
    "                correct_classification += 1\n",
    "\n",
    "        if correct_classification == len(Norm_Y):\n",
    "            print(\"\\nAll samples are classified correctly which means that discriminant function parameters are correct.\")\n",
    "\n",
    "        else:\n",
    "            print(\"\\nSome samples are misclassified.\")\n",
    "        \n",
    "    def setParams():\n",
    "        X_train = transformToList(input(\"Enter features eg [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]] 2D:  \"))\n",
    "        y_train = transformToList(input(\"Enter labels eg [1, 1, 1, -1, -1, -1] 1D:  \"))\n",
    "        b = transformToList(input(\"Enter b eg [1, 1, 1, 1, 1, 1] 1D:  \"))\n",
    "        return X_train, y_train, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, b = Pseudoinverse.setParams()\n",
    "Pseudoinverse.fit(X_train, y_train, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sequential Widrow Hoff Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SequentialWidrowHoff:\n",
    "    def fit(X, Y, a, b, n, iterations):\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Applying Sample Normalisation:\n",
    "        Norm_Y = []\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "\n",
    "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
    "            if y == -1 or y == 2:\n",
    "                x = [i * -1 for i in x]\n",
    "                x.insert(0, y)\n",
    "                Norm_Y.append(x)\n",
    "            else:\n",
    "                x.insert(0, y)\n",
    "                Norm_Y.append(x)\n",
    "\n",
    "        print(\"Vectors used in Sequential Widrow-Hoff Learning Algorithm:\\n {}\\n\".format(Norm_Y))\n",
    "\n",
    "\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        # Sequential Widrow-Hoff Learning Algorithm\n",
    "\n",
    "        # Epoch for-loop:\n",
    "        for o in range(int(iterations / len(Norm_Y))):\n",
    "\n",
    "            # This for-loop goes through each sample one-by-one:\n",
    "            for i in range(len(Norm_Y)):\n",
    "\n",
    "                # Value of a to use. If first iteration, then uses parameters given in question:\n",
    "\n",
    "                a_prev = a\n",
    "\n",
    "                # Which sample to use:\n",
    "                y_input = Norm_Y[i]\n",
    "                print(\"Sample used for this iteration is: {}\".format(y_input))\n",
    "\n",
    "                # Equation -> g(x) = a^{t}y\n",
    "                ay = np.dot(a, y_input)\n",
    "                print(\"g(x) = {}\".format(ay))\n",
    "\n",
    "                # Calculating the values for update:\n",
    "                update = np.zeros(len(y_input))\n",
    "                for j in range(len(y_input)): \n",
    "\n",
    "                    # Applying Update Rule of Sequential Widrow-Hoff Learning Algorithm:\n",
    "                    update[j] = n * (b[i] - ay) * y_input[j]\n",
    "\n",
    "                # Adding update to a:\n",
    "                a = np.add(a, update)\n",
    "                print(\"New Value of a^t is: {}\\n\".format(a))\n",
    "\n",
    "        print(\"Gone through all of the iterations as asked for in question.\")\n",
    "        \n",
    "    def setParams():\n",
    "        X_train = transformToList(input(\"Enter features eg [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]] 2D:  \"))\n",
    "        y_train = transformToList(input(\"Enter labels eg [1, 1, 1, -1, -1, -1] 1D:  \"))\n",
    "        a = transformToList(input(\"Enter a. Usually [1, w1, w2...] eg [1, 0, 0] 1D:  \"))\n",
    "        b = transformToList(input(\"Enter b eg [1, 0.5, 1.5, 1.5, 1.5, 1] 1D:  \"))\n",
    "        lr = float(input(\"Enter the learning rate eg 0.1:  \"))\n",
    "        epochs = int(input(\"Enter the number of epochs eg 12:  \"))\n",
    "        return X_train, y_train, a, b, lr, epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, a, b, lr, epochs = SequentialWidrowHoff.setParams()\n",
    "SequentialWidrowHoff.fit(X_train, y_train, a, b, lr, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Neuron Output (with heavy side function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NeuronOutput:\n",
    "    def fit(weight, threshold, x):\n",
    "        summation = []\n",
    "        for i in range(len(x)):\n",
    "            summation.append(weight[i] * x[i])\n",
    "\n",
    "        summation = np.sum(summation, 0) - threshold\n",
    "\n",
    "        # Find output of neuron by applying heaviside function with given threshold:\n",
    "        output = np.heaviside(summation, threshold)\n",
    "        print(\"Output of neuron with input {} is {}.\".format(x, output))\n",
    "        \n",
    "    def setParams():\n",
    "        #This script is based off of Question 2 in Tutorial 3\n",
    "        x = transformToList(input(\"Enter single sample/input eg [0.1, -0.5, 0.4] 1D:  \"))\n",
    "        weights = transformToList(input(\"Enter weigths from one layer to another eg [0.1, -5, 0.4] 1D:  \"))\n",
    "        threshold = float(input(\"Enter the threshold value eg 0:  \"))\n",
    "        return weights, threshold, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights, threshold, x = NeuronOutput.setParams()\n",
    "NeuronOutput.fit(weights, threshold, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Delta Learning ALTERNATIVE\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def main(ip, targets, weights, threshold, learning_rate, epochs):\n",
    "    ip = np.array(ip)\n",
    "    targets = np.array(targets)\n",
    "    weights.insert(0, -threshold)\n",
    "    weights = np.array(weights)\n",
    "    weights = weights.reshape(-1, 1)\n",
    "    for _iter in range(epochs):\n",
    "        for idx, x in enumerate(ip):\n",
    "            data = np.insert(x, 0, 1)\n",
    "            data = data.reshape(-1, 1)\n",
    "            pred = (weights.T @ data).reshape(-1)[0]\n",
    "            pred = 1 if pred > 0 else 0\n",
    "            weights = weights - learning_rate * (pred - targets[idx]) * data\n",
    "            print(weights.reshape(-1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on tutorial question. Change as needed\n",
    "ip = [[0], [1]]\n",
    "targets = [1, 0]\n",
    "threshold = 1.5\n",
    "weights = [2]\n",
    "main(ip=ip, targets=targets, weights=weights,\n",
    "     threshold=threshold, learning_rate=1, epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Batch and Sequential Delta Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DeltaLearningModel:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, features, target, weights, threshold, lr, epochs, type = \"S\", display=True):\n",
    "        x = Utility.augmentArray(features.astype(np.float), 1)\n",
    "        self.w = Utility.augmentArray(weights.astype(np.float), -threshold)\n",
    "        t = target.astype(np.float)\n",
    "        print(\"Score Before Training : \", round(self.score(features, target)*100, 1), \"%\")\n",
    "        \n",
    "        count = 0\n",
    "        for e in range(0,epochs):\n",
    "            error=0\n",
    "            if count>=epochs: break;\n",
    "            for i in range(0,len(x)):\n",
    "                y = Activation.stepFunction(x[i].dot(self.w))\n",
    "                error += (t[i]-y)*x[i]\n",
    "                \n",
    "                if type.upper()==\"S\":\n",
    "                    self.w = self.w+lr*error\n",
    "                    if display:\n",
    "                        count+=1\n",
    "                        print(count, \"\\t H(wx) = \", round(y,4), \"\\t delta w or n(t-y)x = \", np.round(lr*error,4), \"\\t w = \", np.round(self.w,4))\n",
    "                    error=0\n",
    "            if type.upper()==\"B\":\n",
    "                count+=1\n",
    "                self.w = self.w+lr*error\n",
    "                if display:\n",
    "                    print(count, \"\\t Weight change = \", np.round(error,4), \"\\t w = \", np.round(self.w,4))\n",
    "                \n",
    "                \n",
    "        \n",
    "        print(\"Score After Training : \", round(self.score(features, target)*100, 1), \"%\")\n",
    "        print(\"Final Weights (w) = \", self.w)\n",
    "        \n",
    "    def predict(self, X_values):\n",
    "        x = Utility.augmentArray(X_values.astype(np.float), 1)\n",
    "        myfunc = lambda t: Activation.stepFunction(t.dot(self.w))\n",
    "        return np.apply_along_axis(myfunc, 1, x)\n",
    "    \n",
    "    def score(self, X_test, y_target):\n",
    "        y_test = self.predict(X_test)\n",
    "        return np.sum(y_test == y_target)/len(y_target)\n",
    "    \n",
    "    def setParams(self):\n",
    "        batchOrSeq = str(input(\"'s' for sequential or 'b' for batch.  \"))\n",
    "        X_train = np.array(transformToList(input(\"Enter features eg [[0, 0], [0, 1], [1, 0], [1, 1]] 2D:  \"))).astype(np.float)\n",
    "        y_train = np.array(transformToList(input(\"Enter labels eg [0, 0, 0, 1] 1D:  \"))).astype(np.float)\n",
    "        initWeights = np.array(transformToList(input(\"Enter weights [w1, w2, ..., wd] eg [1, 1] 1D:  \"))).astype(np.float)\n",
    "        threshold = float(input(\"Enter the threshold value eg -0.5.  \"))\n",
    "        learningRate = float(input(\"Enter the learning rate eg 1.  \"))\n",
    "        epochs = int(input(\"Enter the number of epochs eg 4.  \"))\n",
    "        return X_train, y_train, initWeights, threshold, learningRate, epochs, batchOrSeq\n",
    "    \n",
    "    def newPred(self):\n",
    "        ispredict = str(input(\"\\nDo you want to predict using trained model? y/n.  \"))\n",
    "        while ispredict.upper()==\"Y\":\n",
    "            x_pred = np.array(transformToList(input(\"Enter features eg [[0, 0], [0, 1], [1, 0], [1, 1]] 2D:  \")))\n",
    "            print(\"Predicted values: \", self.predict(x_pred))\n",
    "            ispredict = str(input(\"\\nDo you want to predict using trained model? y/n.  \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = DeltaLearningModel()\n",
    "X_train, y_train, initWeights, threshold, learningRate, epochs, batchOrSeq = model.setParams()\n",
    "model.fit(X_train, y_train, initWeights, threshold, learningRate, epochs, batchOrSeq)\n",
    "model.newPred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Softmax (For Competitive Learning Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def fit(x, b=1):\n",
    "        print(\"\\nSoftmax of array: \", x, \"\\n\")\n",
    "        print(np.exp(x*b) / np.sum(np.exp(x*b), axis=0))\n",
    "    \n",
    "    def setParams():\n",
    "        a = np.array(transformToList(input(\"Enter array eg [0.34, 0.73, -0.61] 1D:  \"))).astype(np.float)\n",
    "        beta = float(input(\"Enter Beta value (1 if not specified).  \"))\n",
    "        return a, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a, beta = Softmax.setParams()\n",
    "Softmax.fit(a, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Negative Feedback Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NegativeFeddbackNetwork:\n",
    "    def fit(weights, iterations, x, alpha, activations):\n",
    "        prev_activations = activations\n",
    "        iteration = 1\n",
    "\n",
    "        for i in range(iterations):\n",
    "\n",
    "            print(\"Iteration {}\".format(iteration))\n",
    "\n",
    "            # Following block deals with calculating first equation: e = x - W^{T}y\n",
    "            wT = np.array(weights).T\n",
    "            wTy = np.dot(wT, activations)\n",
    "            print(\"value of wTy {}\".format(wTy))\n",
    "\n",
    "            eT = x - wTy\n",
    "            print(\"eT: {}\".format(eT))\n",
    "            e = np.array(eT).reshape((3, 1))\n",
    "\n",
    "            # The following lines deal with calculating the update: y <- y + \\alpha*W*e\n",
    "            We = np.dot(weights, e)\n",
    "            We = [j for i in We for j in i]\n",
    "            print(\"We: {} \".format(We))\n",
    "\n",
    "            alphaWe = np.dot(alpha, We)\n",
    "\n",
    "            # Doing the actual update using the second equation:\n",
    "            y = activations + alphaWe\n",
    "            print(\"Value of y: {}\\n\".format(y))\n",
    "\n",
    "            activations = y\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        print(\"\\nAfter {} iterations, the activation of the output neurons is equal to {}\".format(iterations, activations))\n",
    "        \n",
    "    def setParams():\n",
    "        print(\"This script is based off of Question 7 in Tutorial 3\")\n",
    "        x = transformToList(input(\"Enter single sample/input eg [1, 1, 0] 1D:  \"))\n",
    "        weights = transformToList(input(\"Enter weigths from one layer to another eg [[1, 1, 0], [1, 1, 1]] 2D:  \"))\n",
    "        activation = transformToList(input(\"Enter output layer initial activation eg [0, 0] 1D:  \"))\n",
    "        lr = float(input(\"Enter the learning rate/alpha eg 0.25.  \"))\n",
    "        epochs = int(input(\"Enter the number of epochs eg 5.  \"))\n",
    "        return weights, epochs, x, lr, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights, epochs, x, lr, activation = NegativeFeddbackNetwork.setParams()\n",
    "NegativeFeddbackNetwork.fit(weights, epochs, x, lr, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Activation Function Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ActivationFunctionOperations:\n",
    "    '''\n",
    "    Based on Question 4 in Tutorial 5:\n",
    "    The following array show the output produced by a mask in a convolutional layer of a CNN.\n",
    "              [[1, 0.5, 0.2], \n",
    "     net_j =  [-1, -0.5, -0.2], \n",
    "              [0.1, -0.1, 0]]\n",
    "    Calculate the values produced by the application of the following activation functions:\n",
    "    '''\n",
    "\n",
    "    def fit(net_j, activation_function, a = 0.1, threshold = 0.1, heaviside_0 = 0.5):\n",
    "\n",
    "        new_array = []\n",
    "\n",
    "        if activation_function == 'ReLU':\n",
    "\n",
    "            for row in net_j:\n",
    "                temp_array = []\n",
    "                for i in row:\n",
    "\n",
    "                    # threshold:\n",
    "                    if i >= 0:\n",
    "                        temp_array.append(i)\n",
    "                    else:\n",
    "                        temp_array.append(0)\n",
    "                new_array.append(temp_array)\n",
    "\n",
    "            print(activation_function, \":  \", new_array)\n",
    "\n",
    "        elif activation_function == 'LReLU':\n",
    "\n",
    "            for row in net_j:\n",
    "                temp_array = []\n",
    "                for i in row:\n",
    "\n",
    "                    # threshold:\n",
    "                    if i >= 0:\n",
    "                        temp_array.append(i)\n",
    "                    else:\n",
    "                        temp_array.append(round(a * i, 2))\n",
    "                new_array.append(temp_array)\n",
    "\n",
    "            print(activation_function, \":  \", new_array)\n",
    "\n",
    "        elif activation_function == 'tanh':\n",
    "\n",
    "            for row in net_j:\n",
    "                temp_array = []\n",
    "                for i in row:\n",
    "\n",
    "                    # Using equation of tanh activation function:\n",
    "                    temp_array.append(round((math.e**i - math.e ** -i) / (math.e**i + math.e ** -i), 5)) \n",
    "                new_array.append(temp_array)\n",
    "\n",
    "            print(activation_function, \":  \", new_array)\n",
    "\n",
    "        elif activation_function == 'heaviside':\n",
    "\n",
    "            for row in net_j:\n",
    "                temp_array = []\n",
    "                for i in row:\n",
    "\n",
    "                    # subtracts threshold away from each value:\n",
    "                    i = i - threshold\n",
    "\n",
    "                    # applies heaviside function to value:\n",
    "                    temp_array.append(np.heaviside(i, heaviside_0))\n",
    "\n",
    "                new_array.append(temp_array)\n",
    "\n",
    "            print(activation_function, \":  \", new_array)\n",
    "            \n",
    "    def setParams():\n",
    "        \n",
    "        activation_function = \"ReLU\"\n",
    "        a = 0.1\n",
    "        threshold = 0.1\n",
    "        heaviside_0 = 0.5\n",
    "        \n",
    "        net_j = transformToList(input(\"Enter matrix to perform activation on eg [[1, 0.5, 0.2], [-1, -0.5, -0.2], [0.1, -0.1, 0]] 2D:  \"))\n",
    "        typeActivation = str(input(\"'ReLU', 'LReLU', 'tanh' or 'heaviside' \"))\n",
    "        if typeActivation.upper()==\"RELU\":\n",
    "            activation_function = \"ReLU\"\n",
    "        elif typeActivation.upper()==\"LRELU\":\n",
    "            activation_function = \"LReLU\"\n",
    "            a = float(input(\"Enter the a value eg 0.1.  \"))\n",
    "        elif typeActivation.lower()==\"tanh\": \n",
    "            activation_function = \"tanh\"\n",
    "        elif typeActivation.lower()==\"heaviside\": \n",
    "            activation_function = \"heaviside\"\n",
    "            threshold = float(input(\"Enter the threshold value eg 0.1.  \"))\n",
    "            heaviside_0 = float(input(\"Enter the H(0) value eg 0.5.  \"))\n",
    "        else:\n",
    "            print(\"wrong input\")\n",
    "            \n",
    "        return net_j, activation_function, a, threshold, heaviside_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter matrix to perform activation on eg [[1, 0.5, 0.2], [-1, -0.5, -0.2], [0.1, -0.1, 0]] 2D:  [[1, 0.5, 0.2], [-1, -0.5, -0.2], [0.1, -0.1, 0]]\n",
      "'ReLU', 'LReLU', 'tanh' or 'heaviside' LReLU\n",
      "Enter the a value eg 0.1.  0.1\n",
      "LReLU :   [[1.0, 0.5, 0.2], [-0.1, -0.05, -0.02], [0.1, -0.01, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "net_j, activation_function, a, threshold, heaviside_0 = ActivationFunctionOperations.setParams()\n",
    "ActivationFunctionOperations.fit(net_j, activation_function, a, threshold, heaviside_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN - Deep Generative NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    File name: deep-gen-neural-nets.py\n",
    "    Author: Camil Abraham Hamanoiel Faure\n",
    "            Anish Thekkan\n",
    "    Date created: 26/05/2021\n",
    "    Date last modified: 30/05/2021\n",
    "    Python Version: 3.7\n",
    "'''\n",
    "\n",
    "import math as math\n",
    "import sympy as sy\n",
    "\n",
    "#\n",
    "# NOTE: This script only works for k=1. Examples with different values\n",
    "#       of k are not provided in the lecture or the tutorial.\n",
    "#\n",
    "\n",
    "# Change this variables according to the question\n",
    "training_iterations = 1 # Number of training iterations.\n",
    "x_real = [[1,2],[3,4]] # Real samples\n",
    "x_fake = [[5,6],[7,8]] # Generated samples\n",
    "    # Both x_real and x_fake must have the same number of \n",
    "    # samples for part b to work\n",
    "\n",
    "theta = [0.1,0.2] # Initial parameters of discriminator\n",
    "\n",
    "prob_real = 0.5 # Probability of real samples to be selected\n",
    "prob_fake = 0.5 # Probability of generated samples to be selected\n",
    "    # In Tutorial 7 Q.3 each sample has the same probability,\n",
    "    # that is why the current value is 0.5 for both\n",
    "\n",
    "learning_rate = 0.02 # Part b Q.3 says that learning rate is 0.02 but\n",
    "                    # it can be changed to the desired value\n",
    "\n",
    "# DO NOT change this variables\n",
    "# ----------------------------\n",
    "# Here, symbols are created for each variable so that\n",
    "# the discriminator function can be differentiated and\n",
    "# evaluated with different values\n",
    "n = len(x_real[0]) \n",
    "x = [None] * n\n",
    "t = [None] * n\n",
    "for i in range(n):\n",
    "    name = 'x'+str(i)\n",
    "    x[i] = sy.Symbol(name)\n",
    "    name = 't'+str(i)\n",
    "    t[i] = sy.Symbol(name)\n",
    "# ----------------------------\n",
    "\n",
    "# Change this according to the problem's discriminator function\n",
    "discriminator_function = 1/(1+ math.e**-(t[0]*x[0] - t[1]*x[1] - 2))\n",
    "    # The discriminator function can be changed to any function needed.\n",
    "    # \n",
    "    # Note that the variables MUST be expressed as t[i] and x[i] \n",
    "    # where:\n",
    "    # 't' is a list containig variables for the parameters of Discriminator (Theta)\n",
    "    # 'x' is a list containig variables for each attribute of each sample\n",
    "    # 'i' is the number of the attribute and parameter used in that case\n",
    "    # \n",
    "    # i.e. for the equation: \"1*x1\" \n",
    "    # then above it should be written: \"t[0]*x[0]\"\n",
    "    # because python lists start in 0\n",
    "\n",
    "# *****DO NOT CHANGE CODE BELOW THIS LINE********\n",
    "#\n",
    "m = len(x_real)\n",
    "m_fake = len(x_fake)\n",
    "\n",
    "#\n",
    "# Because it is discrete we don't need to find integrals here when computing expectations\n",
    "#\n",
    "print('************')\n",
    "print('*****GAN****')\n",
    "print('************')\n",
    "\n",
    "#\n",
    "# Tutorial 7 Question 3 part a\n",
    "#\n",
    "for iter in range(training_iterations):\n",
    "    print ('\\n--Start of',str(iter+1)+\"\",'training iteration.--')\n",
    "    V_D = 0 # Discriminator Value \n",
    "\n",
    "    # This for is to obtain the discriminator value\n",
    "    for i in range(m):\n",
    "        xx = x_real[i]\n",
    "        res_discr_funct = discriminator_function\n",
    "        for j in range(n):\n",
    "            # .subs() is a function that replaces the x[i] and t[i] variables of the \n",
    "            # discriminant function with the actual values of the theta and real samples\n",
    "            # given in the problem\n",
    "            res_discr_funct = res_discr_funct.subs(x[j],xx[j]).subs(t[j],theta[j])\n",
    "        V_D += prob_real*math.log(res_discr_funct)\n",
    "\n",
    "\n",
    "    print('\\n***** PART A ****\\n')\n",
    "\n",
    "    print('\\nThe Discriminator value is', V_D)\n",
    "\n",
    "    V_G = 0 # Generator Value\n",
    "\n",
    "    # This for is to obtain the generator value\n",
    "    for i in range(m_fake):\n",
    "        xx = x_fake[i]\n",
    "        res_discr_funct = discriminator_function\n",
    "        for j in range(n):\n",
    "            # .subs() is a function that replaces the x[i] and t[i] variables of the \n",
    "            # discriminant function with the actual values of the theta and real samples\n",
    "            # given in the problem\n",
    "            res_discr_funct = res_discr_funct.subs(x[j],xx[j]).subs(t[j],theta[j])\n",
    "        V_G += prob_fake*math.log(1 - res_discr_funct)\n",
    "        \n",
    "    print('\\nThe Generator value is', V_G)\n",
    "\n",
    "    V_DG = V_D + V_G\n",
    "\n",
    "    print('\\nThe Computed V_DG is ', V_DG)\n",
    "\n",
    "    #\n",
    "    # Tutorial 7 Question 3 part b\n",
    "    #\n",
    "\n",
    "    print('\\n\\n*********************')\n",
    "    print('***** PART B ****\\n')\n",
    "\n",
    "    alpha_beta = [[0] * n] * m\n",
    "    for i in range(m):\n",
    "        xx = x_real[i]\n",
    "        xx_bar = x_fake[i]\n",
    "        # Two different equations are needed. One with x_real[i] and another with\n",
    "        # x_fake[i]. This is explained in the tutorial solutions.\n",
    "        discriminator_function_xx = discriminator_function\n",
    "        discriminator_function_xx_bar = discriminator_function\n",
    "        for j in range(n):\n",
    "            discriminator_function_xx = discriminator_function_xx.subs(x[j],xx[j])\n",
    "            discriminator_function_xx_bar = discriminator_function_xx_bar.subs(x[j],xx_bar[j])\n",
    "\n",
    "        learning_equation = sy.log(discriminator_function_xx) \n",
    "        learning_equation += sy.log(1 - discriminator_function_xx_bar)\n",
    "        \n",
    "        differential_equation = [None] * n\n",
    "        # The learning equation is differentiated in this for\n",
    "        for j in range(n):\n",
    "            differential_equation[j] = learning_equation.diff(t[j])\n",
    "            # After differentiating the learning equation, the next for replaces\n",
    "            # thetha variables with the actual values to obtain the result\n",
    "            for k in range(n):\n",
    "                differential_equation[j] = differential_equation[j].subs(t[k],theta[k])\n",
    "        # The result of the differentiated equations is added to a list\n",
    "        alpha_beta[i] = differential_equation\n",
    "\n",
    "    # Alpha and beta are added together to obtain delta\n",
    "    delta_theta = [0] * n\n",
    "    for i in range(n):\n",
    "        delta_theta[i] = 0\n",
    "        for j in range(m):\n",
    "            print( 'Alpha and Beta', i+1,j+1, 'is', alpha_beta[j][i])\n",
    "            delta_theta[i] += (1/m)*alpha_beta[j][i]\n",
    "\n",
    "    print( '\\nDelta is ', delta_theta,'\\n')\n",
    "\n",
    "    # Lastly, delta is added to the current theta values to obtain the new\n",
    "    # theta values after training\n",
    "    for i in range(n):\n",
    "        theta[i] = theta[i] + learning_rate*delta_theta[i]\n",
    "        print('New theta',i+1   ,'is',theta[i])\n",
    "    print ('\\n--End of',str(iter+1)+\"\",'training iteration.--')\n",
    "print ('\\n\\n---------End of the script.---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 7\n",
    "\n",
    "### Oja's rule\n",
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import numpy as np\n",
    "\n",
    "# Oja's Learning rule \n",
    "# through Mizan \n",
    "# this code works out zero-mean data so please enter the raw feature vectors as given in the question\n",
    "# Tutorial 7, question 7\n",
    "\n",
    "## -----------------------------------------------------------------------\n",
    "# ONLY CHANGE THESE 3 INPUTS AND CHANGE THE NUMBER OF EPOCH YOU WANT WHEN APPLYING THE FUNCTION(Oja_learning_rule)\n",
    "input_from_question = np.array([[[0,1]],[[3,5]],[[5,4]],[[5,6]],[[8,7]],[[9,7]]])\n",
    "weight_x = np.array([[-1,0]])\n",
    "learning_rate = 0.01\n",
    "epochs = 6\n",
    "## -----------------------------------------------------------------------\n",
    "\n",
    "# we are going to perform Oja's learning on the input_vectors\n",
    "input_vectors = []\n",
    "\n",
    "mean_of_data = input_from_question.mean(axis=0)\n",
    "for i in input_from_question:\n",
    "    zero_mean_data = i - mean_of_data\n",
    "    input_vectors.append(zero_mean_data)\n",
    "    \n",
    "\n",
    "def Oja_learning_rule(epoch):\n",
    "    weight_update = np.copy(weight_x)  \n",
    "    for i in range(1,epoch+1):\n",
    "        df = pd.DataFrame({\"x\": [i for i in input_vectors]})\n",
    "        df['y'] = df['x'].apply(lambda x: np.dot(x,weight_update.T))\n",
    "        df['x - yw'] = df['x'].apply(lambda x: np.round(x, 4)) - df['y'].apply(lambda y: y * (weight_update))  \n",
    "        df['ny(x -yw)'] = learning_rate * df['y'].apply(lambda y: y) * df['x - yw'].apply(lambda x: x)\n",
    "        #Rounding the numbers         \n",
    "        df['y'] = df['y'].apply(lambda y: np.round(y,4))\n",
    "        df['x - yw'] =  df['x - yw'].apply(lambda x: np.round(x,4))\n",
    "        df['ny(x -yw)']  = df['ny(x -yw)'].apply(lambda x: np.round(x,4))\n",
    "        sum_of_weights = df['ny(x -yw)'].sum()\n",
    "        weight_update = weight_update + sum_of_weights   \n",
    "        display(df)\n",
    "        print(f'after {i} epoch Total weight change is: {sum_of_weights}')\n",
    "        print(f'after {i} epoch our weights are: {weight_update}')\n",
    "\n",
    "\n",
    "Oja_learning_rule(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "\n",
    "def _PCA(ip, n_components, data_to_project=None):\n",
    "    ip = np.array(ip)\n",
    "    ip_mean = np.mean(ip, axis=1)\n",
    "    ip_prime = ip - np.vstack(ip_mean)\n",
    "    C = (ip_prime @ ip_prime.T) / ip.shape[1]\n",
    "    V, D, VT = svd(C)\n",
    "    ans = VT @ ip_prime\n",
    "    print(\"-\"*100)\n",
    "    print(\"READ THE ROWS FROM THE TOP\")\n",
    "    print(ans[:n_components])\n",
    "    print(\"-\"*100)\n",
    "    if data_to_project:\n",
    "        data_to_project = np.array(data_to_project)\n",
    "        print(\"-\"*100)\n",
    "        print(f\"PROJECTION OF {data_to_project}\")\n",
    "        print((VT@data_to_project)[:n_components])\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE ACCORDING TO THE QUESTION\n",
    "ip = [[4, 0, 2, -2], [2, -2, 4, 0], [2, 2, 2, 2]]\n",
    "n_components = 2\n",
    "data_to_project = [3, -2, 5]\n",
    "_PCA(ip=ip, n_components=n_components, data_to_project=data_to_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher's Method\n",
    "\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def fishers(ip, weights, classes):\n",
    "    ip = np.array(ip)\n",
    "    N, D = ip.shape\n",
    "    weights = np.array(weights)\n",
    "    m1 = []\n",
    "    m2 = []\n",
    "    for idx in range(N):\n",
    "        if classes[idx] == 1:\n",
    "            m1.append(ip[idx])\n",
    "        else:\n",
    "            m2.append(ip[idx])\n",
    "    m1 = np.mean(m1, axis=0)\n",
    "    m2 = np.mean(m2, axis=0)\n",
    "\n",
    "    # between cluster distance\n",
    "    sb = []\n",
    "    sw = []\n",
    "    for w in (weights):\n",
    "        d = (w @ (m1-m2)) ** 2\n",
    "        sb.append(d)\n",
    "    # calculate within cluster distance\n",
    "    sw = []\n",
    "    for w in weights:\n",
    "        running_sw = 0\n",
    "        for idx in range(len(ip)):\n",
    "            if classes[idx] == 1:\n",
    "                running_sw += (w.T @ (ip[idx] - m1)) ** 2\n",
    "\n",
    "            elif classes[idx] == 2:\n",
    "                running_sw += (w.T @ (ip[idx] - m2)) ** 2\n",
    "        sw.append(running_sw)\n",
    "        # print(running_sw)\n",
    "    print(\"SB: \")\n",
    "    print(sb)\n",
    "    print(\"SW: \")\n",
    "    print(sw)\n",
    "    cost = []\n",
    "    for _sb, _sw in zip(sb, sw):\n",
    "        cost.append(_sb/_sw)\n",
    "    print(\"Cost: \")\n",
    "    print(cost)\n",
    "\n",
    "    print(\"-\"*100)\n",
    "    print(f\"{weights[np.argmax(cost)]} has high PROJECTION COST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = [[1, 2], [2, 1], [3, 3], [6, 5], [7, 8]]\n",
    "classes = [1, 1, 1, 2, 2]\n",
    "weights = [[-1, 5], [2, -3]]\n",
    "fishers(ip, weights, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Coding\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def main(p, VT, x, _lambda):\n",
    "    p = np.array(p)\n",
    "    VT = np.array(VT)\n",
    "    x = np.array(x)\n",
    "    r_error = []\n",
    "    for p in projections:\n",
    "        val = x - VT @ p\n",
    "        r_error.append(np.linalg.norm(val) + _lambda*np.count_nonzero(p))\n",
    "    print(\"RECONSTRUCTION ERRORS: \")\n",
    "    print(r_error)\n",
    "    print(projections[np.argmin(r_error)], \" for sparse coding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE ACCORDING TO THE QUESTION\n",
    "# projections are nothing but y\n",
    "\n",
    "projections = [[1, 2, 0, -1], [0, 0.5, 1, 0]]\n",
    "x = [[2, 3]]\n",
    "VT = [[1, 1, 2, 1], [-4, 3, 2, -1]]\n",
    "main(p=projections, VT=VT, x=x, _lambda=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SVM(to find lambda, weights and margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def fit(X, y, support_vectors, support_vector_class):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        print(\"-\"*100)\n",
    "        w = []\n",
    "        for idx in range(len(support_vectors)):\n",
    "            w.append(support_vectors[idx] * support_vector_class[idx])\n",
    "        w = np.array(w)\n",
    "        eq_arr = []\n",
    "        for idx, sv in enumerate(support_vectors):\n",
    "            tmp = ((w @ sv) * support_vector_class[idx])\n",
    "            tmp = np.append(tmp, [support_vector_class[idx]])\n",
    "            eq_arr.append(tmp)\n",
    "        eq_arr.append(np.append(support_vector_class, [0]))\n",
    "        rhs_arr = [1] * len(support_vector_class)\n",
    "        rhs_arr.extend([0])\n",
    "        rhs_arr = np.array(rhs_arr)\n",
    "        ans = rhs_arr @ np.linalg.inv(eq_arr)\n",
    "        print(\"lambda and w_0 values are \", ans)\n",
    "        final_weight = []\n",
    "        for idx in range(w.shape[0]):\n",
    "            final_weight.append(w[idx] * ans[idx])\n",
    "        final_weight = np.array(final_weight)\n",
    "        final_weight = np.sum(final_weight, axis=0)\n",
    "        print(\"Weights: \")\n",
    "        print(final_weight)\n",
    "        print(\"Margin: \")\n",
    "        print(2/np.linalg.norm(final_weight))\n",
    "        print(\"-\"*100)\n",
    "            \n",
    "    def setParams():\n",
    "        X = transformToList(input(\"Enter features eg [[3, 1], [3, -1], [7, 1], [8, 0], [1, 0], [0, 1], [-1, 0], [-2, 0]] 2D:  \"))\n",
    "        y = transformToList(input(\"Enter labels eg [1, 1, 1, 1, -1, -1, -1, -1] 1D:  \"))\n",
    "        support_vectors = np.array(transformToList(input(\"Enter features eg [[3, 1], [3, -1], [1, 0]] 2D:  \"))).astype(np.float)\n",
    "        support_vector_class = np.array(transformToList(input(\"Enter labels eg [1, 1, -1] 1D:  \"))).astype(np.float)\n",
    "            \n",
    "        return X, y, support_vectors, support_vector_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter features eg [[3, 1], [3, -1], [7, 1], [8, 0], [1, 0], [0, 1], [-1, 0], [-2, 0]] 2D:  [[3, 1], [3, -1], [7, 1], [8, 0], [1, 0], [0, 1], [-1, 0], [-2, 0]]\n",
      "Enter labels eg [1, 1, 1, 1, -1, -1, -1, -1] 1D:  [1, 1, 1, 1, -1, -1, -1, -1]\n",
      "Enter features eg [[3, 1], [3, -1], [1, 0]] 2D:  [[3, 1], [3, -1], [1, 0]]\n",
      "Enter labels eg [1, 1, -1] 1D:  [1, 1, -1]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "lambda and w_0 values are  [ 0.25  0.25  0.5  -2.  ]\n",
      "Weights: \n",
      "[1.00000000e+00 1.38777878e-16]\n",
      "Margin: \n",
      "1.9999999999999996\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X, y, support_vectors, support_vector_class = SVM.setParams()\n",
    "SVM.fit(X, y, support_vectors, support_vector_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBooster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# NOTE you can use if for Q1 c in tutorial9\n",
    "\n",
    "def run_adaBooster_setup():\n",
    "    \"\"\" Ask the user to enter the needed parameters\n",
    "    \"\"\"  \n",
    "    print(\"Please insert the dataset. Each sample should be divided by a space and each coordinate within a sample should be divided by a coma. \\nBe careful not to enter spaces after the coma that separates the samples' coordinates.\\n\")\n",
    "    print(\"I.E.: x1=[1,2], x2=[-3,4], x3=[5,3] would be ---> 1,2 -3,4 5,3 \\n\")\n",
    "    dataset_input = str(input())\n",
    "    dataset = np.array([]).reshape(0,2)\n",
    "    samples = dataset_input.split(' ')\n",
    "    for sample in samples:\n",
    "        coordinates = sample.split(',')\n",
    "        coordinates = np.array([float(c) for c in coordinates])\n",
    "        dataset = np.concatenate((dataset, [coordinates]))\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the labels (y) of each sample in the dataset. \\nYou should provide a sequential list of y where each y is separated by a single space.\")\n",
    "    print(\"I.E. +1 -1 +1 -1\")\n",
    "    labels = [ int(o) for o in str(input()).split(' ')] \n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the number of weak classifiers:\")\n",
    "    n_classifiers = int(input())\n",
    "    print(\"\\n\")\n",
    "    thresholds = np.array([])\n",
    "    for i in range(0, n_classifiers):\n",
    "        print(f\"Please insert the decision threshold for weak classifier {i+1} so that it classify a sample to be +1:\")\n",
    "        print(\"I.E.: x1 > 0\")\n",
    "        print(\"I.E.: x2 > 3\")\n",
    "        print(\"I.E.: x1 >= -4\")\n",
    "        thresholds = np.concatenate((thresholds, [str(input())]))\n",
    "        print('\\n')\n",
    "    print(\"Please enter the target training error (when the adaboost should terminate). This should be normalised to the total number of samples:\")\n",
    "    print(\"I.E.: 0 -> if you want to stop it when the classifier classifies correctly all the training samples.\")\n",
    "    print(\"I.E.: 0.25 -> if you want to stop it when the classifier classifies correctly 75\\% of the samples.\")\n",
    "    target_error = int(input())\n",
    "    print(\"\\n\")\n",
    "    print(\"Please enter the maximum number of iterations you want the algorithm to run for:\")\n",
    "    max_iterations = int(input())\n",
    "    print(\"\\n\")\n",
    "    return dataset, labels, n_classifiers, thresholds, target_error, max_iterations\n",
    "\n",
    "\n",
    "# Decision stump used as weak classifier\n",
    "class DecisionStump():\n",
    "    def __init__(self, id, threshold=None):\n",
    "        self.threshold = threshold\n",
    "        self.id = id\n",
    "\n",
    "\n",
    "    def predict(self, sample):\n",
    "        if '>' in self.threshold:\n",
    "            terms = self.threshold.split(' ')\n",
    "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
    "            if sample[axis_in_condition[0]] > float(terms[2]):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        elif '<' in self.threshold:\n",
    "            terms = self.threshold.split(' ')\n",
    "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
    "            if sample[axis_in_condition[0]] < float(terms[2]):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        elif '<=' in self.threshold:\n",
    "            terms = self.threshold.split(' ')\n",
    "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
    "            if sample[axis_in_condition[0]] <= float(terms[2]):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        elif '>=' in self.threshold:\n",
    "            terms = self.threshold.split(' ')\n",
    "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
    "            if sample[axis_in_condition[0]] >= float(terms[2]):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "\n",
    "class Adaboost():\n",
    "\n",
    "    def __init__(self, n_clf, thresholds, target_error, max_iterations=10):\n",
    "        self.n_clf = n_clf\n",
    "        self.clfs = np.array([])\n",
    "        for i in range (0, len(thresholds)):\n",
    "            self.clfs = np.concatenate(( self.clfs, [DecisionStump(i+1, thresholds[i])] ))\n",
    "        # self.alpha = 0\n",
    "        self.alpha = []\n",
    "        self.target_error = target_error\n",
    "        self.max_iterations = max_iterations\n",
    "        self.best_classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights to 1/N\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "        iteration = 1\n",
    "        while True:\n",
    "            print(f\"Iteration {iteration}: \\nWeights: {w}\")\n",
    "\n",
    "            # Iterate through classifiers and find the best one\n",
    "            lowest_error = 100000000\n",
    "            best_classifier = 0\n",
    "            for i in range(0, self.n_clf):\n",
    "                clf = self.clfs[i]\n",
    "                # Calculate Error\n",
    "                err = 0\n",
    "                for j, sample in enumerate(X):\n",
    "                    prediction = clf.predict(sample)\n",
    "                    error = 0 if prediction == y[j] else 1\n",
    "                    err += error * w[j]\n",
    "                if err < lowest_error:\n",
    "                    lowest_error = err\n",
    "                    best_classifier = i\n",
    "\n",
    "            self.best_classifiers.append(self.clfs[best_classifier])\n",
    "            print(f\"Best classifier: {best_classifier+1}\")\n",
    "\n",
    "            # Get predictions from best classifer for each sample\n",
    "            predictions = []\n",
    "            for j, sample in enumerate(X):\n",
    "                prediction = self.clfs[best_classifier].predict(sample)\n",
    "                predictions.append(prediction)\n",
    "\n",
    "            # Calculate weighted training error of best classifier\n",
    "            weighted_error = 0\n",
    "            for i, prediction in enumerate(predictions):\n",
    "                # error += 0 if prediction == y[i] else 1\n",
    "                w_e = 0 if prediction == y[i] else 1\n",
    "                w_e *= w[i]\n",
    "                weighted_error += w_e\n",
    "            error /= len(y)\n",
    "            print(f\"Best classifier's weighted training error: {weighted_error}\")\n",
    "\n",
    "\n",
    "            # Calculate Alpha\n",
    "            EPS = 1e-10\n",
    "            alpha = 0.5 * np.log((1.0 - lowest_error) / (lowest_error))\n",
    "            self.alpha.append(alpha)\n",
    "            print(f\"Alpha: {alpha}\")\n",
    "\n",
    "            # Calculate weights for next iteration\n",
    "            new_w = []\n",
    "            for i, weight in enumerate(w):\n",
    "                new_w.append(w[i] * (np.exp(- alpha * y[i] * predictions[i])))\n",
    "                print(f\"Update weight: W{iteration}(sample{i+1})*e^-alpha{iteration}*y{i+1}*h{iteration}(sample{i+1}) ----> {w[i] * (np.exp(- alpha * y[i] * predictions[i]))}\")\n",
    "            # Normalize to one\n",
    "            Z_normalisation = 0\n",
    "            for i, weight in enumerate(new_w):\n",
    "                Z_normalisation += weight\n",
    "            for i, weight in enumerate(new_w):\n",
    "                new_w[i] /= Z_normalisation\n",
    "            print(f\"Normalisation Z{iteration} when updating new weights: {Z_normalisation}\")\n",
    "            # Update weights for next iteration\n",
    "            w = new_w\n",
    "\n",
    "\n",
    "\n",
    "            # Check if the classifier has reached the desired target error\n",
    "            # Find the output*alpha of each classifier for each sample\n",
    "            tot_error = 0\n",
    "            decision_formula = ''\n",
    "            sample_classifications = np.zeros((X.shape[0], len(self.alpha)))\n",
    "            for i, alpha in enumerate(self.alpha):  \n",
    "                clf = self.best_classifiers[i]\n",
    "                for j, sample in enumerate(X):\n",
    "                    prediction = clf.predict(sample)\n",
    "                    sample_classifications[j][i] = alpha if prediction == y[j] else -alpha\n",
    "                decision_formula += f\"{alpha} * h{clf.id}(x) + \"\n",
    "            # Calculate the AdaBooster classification error in this round\n",
    "            sample_classifications = sample_classifications.sum(axis=1)\n",
    "            for i, classification in enumerate(sample_classifications):\n",
    "                classification = 1 if classification >= 0 else -1\n",
    "                tot_error += 1/X.shape[0] if classification == y[j] else 0\n",
    "            print(f\"AdaBoost Classifier in this round: {decision_formula[:-2]}\")\n",
    "            print(f\"AdaBoost Classifier (unweighted) error in this round: {tot_error}\")\n",
    "            # If the error is below our target error stop the execution\n",
    "            if tot_error <= self.target_error:\n",
    "                print('\\n')\n",
    "                print(f\"The final hard classifier is: sgn({decision_formula[:-2]})\")\n",
    "                return\n",
    "\n",
    "            # If we have reached the max iterations stop the execution\n",
    "            if iteration >= self.max_iterations:\n",
    "                return\n",
    "            iteration += 1\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, sample):\n",
    "        tot_error = 0\n",
    "        sample_classifications = np.zeros((1, len(self.alpha)))\n",
    "        for i, alpha in enumerate(self.alpha):  \n",
    "            clf = self.best_classifiers[i]\n",
    "            prediction = clf.predict(sample)\n",
    "            sample_classifications[0][i] = alpha if prediction == 1 else -alpha\n",
    "        # Calculate the AdaBooster classification error in this round\n",
    "        sample_classifications = sample_classifications.sum(axis=1)\n",
    "        return 1 if sample_classifications[0] >= 0 else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset, labels, n_classifiers, thresholds, target_error, max_iterations = run_adaBooster_setup()\n",
    "    classifier = Adaboost(n_classifiers, thresholds, target_error, max_iterations)\n",
    "    classifier.fit(dataset, labels)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nEnter a new sample (separating its coordinates with a coma and not including spaces) to predict or simply enter quit()\")\n",
    "        sample = str(input())\n",
    "        if sample == 'quit()':\n",
    "            quit()\n",
    "        result = classifier.predict([float(c) for c in sample.split(',')])\n",
    "        print(f\"The AdaBoost classifier classified it as {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Competitive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def run_competitive_learning_setup():\n",
    "    \"\"\" Ask the user to enter the needed parameters for competitive learning.\n",
    "    Returns:\n",
    "        [string, np.array, np.array, float, np.array]: \n",
    "                             mode -> Specify if using normalisation or not.\n",
    "                             dataset -> The dataset to cluster.\n",
    "                             clusters -> The coordinates of the initial centroids\n",
    "                             lr -> The learning Rate \n",
    "                             order_of_indexes -> The order to follow when selecting samples in the algorithm\n",
    "    \"\"\"    \n",
    "    print(\"How you want to run the algorithm? (Enter the corresponding number and press ENTER)\")\n",
    "    print(\"1) With normalisation and argumentation\")\n",
    "    print(\"2) Without normalisation and argumentation\")\n",
    "    mode = int(input())\n",
    "    algorithm_variants = {1: \"with norm\", 2:\"without norm\"}\n",
    "    mode = algorithm_variants.get(mode)\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the dataset. Each sample should be divided by a space and each coordinate within a sample should be divided by a coma. \\nBe careful not to enter spaces after the coma that separates the samples' coordinates.\\n\")\n",
    "    print(\"I.E.: x1=[1,2], x2=[-3,4], x3=[5,3] would be ---> 1,2 -3,4 5,3 \\n\")\n",
    "    dataset_input = str(input())\n",
    "    print(\"\\n\")\n",
    "    dataset = np.array([]).reshape(0,2)\n",
    "    samples = dataset_input.split(' ')\n",
    "    for sample in samples:\n",
    "        coordinates = sample.split(',')\n",
    "        coordinates = np.array([float(c) for c in coordinates])\n",
    "        dataset = np.concatenate((dataset, [coordinates]))\n",
    "    print(\"Please insert the initial centroids. Each centroid should be divided by a space and each coordinate within a centroid should be divided by a coma. \\nBe careful not to enter spaces after the coma that separates the centroids' coordinates.\\n\")\n",
    "    print(\"I.E.: c1=[1,2], c2=[-3,4], c3=[5,3] would be ---> 1,2 -3,4 5,3 \\n\")\n",
    "    centroids_input = str(input())\n",
    "    print(\"\\n\")\n",
    "    clusters = np.array([]).reshape(0,2)\n",
    "    centroids = centroids_input.split(' ')\n",
    "    for centroid in centroids:\n",
    "        coordinates = centroid.split(',')\n",
    "        coordinates = np.array([float(c) for c in coordinates])\n",
    "        clusters = np.concatenate((clusters, [coordinates]))\n",
    "    print(\"Please insert the value of the Learning Rate and then press ENTER (make sure it is a float number - I.E. 0.1)\")\n",
    "    lr = float(input())\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the order to follow when selecting the samples within the algorithm. \\nYou should provide a sequential list of the indexes of the samples to select separated by single spaces.\")\n",
    "    print(\"I.E. 1 2 1 5 3\")\n",
    "    print(\"NOTE: indexes start at 1!\")\n",
    "    order_of_indexes = [ int(o) - 1 for o in str(input()).split(' ')]\n",
    "    print (\"\\n\")\n",
    "    return mode, dataset, clusters, lr, order_of_indexes\n",
    "\n",
    "\n",
    "class CompetitiveLearning:\n",
    "    \"\"\" This class can be used to execute problems regarding Competitive Learning\n",
    "    \"\"\" \n",
    "    def __init__(self, mode, dataset, clusters, lr, order_of_samples):\n",
    "        self.mode = mode # Indicates wether to use normalisation or not. Either \"with norm\" or \"without norm\"\n",
    "        self.dataset = dataset # Dataset of type np.array([[x11, x12], [x12, x22],...,[x1n, x2n]])\n",
    "        self.centroids = clusters # Initial clusters of type np.array([[x11, x12], [x12, x22],...,[x1n, x2n]])\n",
    "        self.lr = lr # Learning rate. Must be a float > 0\n",
    "        self.order_of_samples = order_of_samples # Indicates what order to select the samples in the algorithm. \n",
    "                                                # Of type np.array([int, int, int]) where each int is the corresponding index to the sample in self.dataset\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs the algorithm on the samples listed in self.order_of_samples.\n",
    "            After that, the user can select some more operations to do with the updated clusters.\n",
    "        \"\"\"        \n",
    "        if self.mode == \"with norm\":\n",
    "            self.run_with_normalisation()\n",
    "        elif self.mode == \"without norm\":\n",
    "            self.run_without_normalisation()\n",
    "        user_input = 0\n",
    "        while user_input != 3:\n",
    "            # While the user doesnt selects exit show some options.\n",
    "            # NOTE only applicable when in \"without norm\" mode.\n",
    "            if user_input == 1:\n",
    "                self.classify_samples()\n",
    "            elif user_input == 2:\n",
    "                self.classify_new_data()\n",
    "\n",
    "            print(\"What do you want to do now?\")\n",
    "            print(\"1) Classify all the existing samples\")\n",
    "            print(\"2) Classify a new sample\")\n",
    "            print(\"3) Exit\")\n",
    "            user_input = int(input())\n",
    "\n",
    "    def run_with_normalisation(self):\n",
    "        augmented_dataset = np.array([np.insert(sample, 0, 1) for sample in self.dataset])\n",
    "        normalised_dataset = np.array([np.divide(sample, np.linalg.norm(sample)) for sample in augmented_dataset])\n",
    "        augmented_centroids = np.array([np.insert(sample, 0, 1) for sample in self.centroids])\n",
    "\n",
    "        print(f\"The augmented dataset is :{augmented_dataset}\")\n",
    "        print(f\"The normalised dataset is :{normalised_dataset}\\n\")\n",
    "        for iteration, i in enumerate(self.order_of_samples):\n",
    "            x = normalised_dataset[i]\n",
    "            print(f\"Iteration {iteration+1}:\")\n",
    "            print(f\"Selected x{i+1} {self.dataset[i]} which normalised is --> {x}\")\n",
    "\n",
    "            net_inner_products = np.array([np.multiply(c.transpose(), x) for c in augmented_centroids])\n",
    "            print(f\"The inner products to each centroid with respect to x{i+1} are {net_inner_products}\")\n",
    "            j = np.argmax(np.sum(net_inner_products, axis=1))\n",
    "\n",
    "            rhino_centroid = augmented_centroids[j]\n",
    "            print(f\"The selected centroid is c{j+1} {rhino_centroid}, with a net inner product to {x} of {net_inner_products[j]}\")\n",
    "            \n",
    "            # Update Rhino Centroid\n",
    "            rhino_centroid = np.add(rhino_centroid, np.multiply([self.lr], x))\n",
    "            print(f\"Updated Centroid c{j+1} with respect to x{i+1}: {rhino_centroid}\")\n",
    "            # Normalise Rhino Centroid\n",
    "            rhino_centroid = np.divide(rhino_centroid, np.linalg.norm(rhino_centroid))\n",
    "            print(f\"Normalised Centroid c{j+1} with respect to x{i+1}: {rhino_centroid} \\n\")\n",
    "            augmented_centroids[j] = rhino_centroid\n",
    "\n",
    "        self.centroids = augmented_centroids\n",
    "        print(f\"The final Centroids are: {self.centroids}\\n\")\n",
    "    \n",
    "    def run_without_normalisation(self):\n",
    "        for iteration, i in enumerate(self.order_of_samples):\n",
    "            print(f\"Iteration {iteration+1}:\")\n",
    "            x = self.dataset[i]\n",
    "\n",
    "            distances_to_centroids = np.array([np.linalg.norm(x - c) for c in self.centroids])\n",
    "            j = distances_to_centroids.argmin()\n",
    "\n",
    "            rhino_centroid = self.centroids[j]\n",
    "            print(f\"The selected centroid is c{j+1} {rhino_centroid}, with a distance of {distances_to_centroids[j]} to {x}\")\n",
    "            \n",
    "            # Update Rhino Centroid\n",
    "            rhino_centroid = np.add(rhino_centroid, np.multiply([self.lr], np.subtract(x, rhino_centroid)))\n",
    "            self.centroids[j] = rhino_centroid\n",
    "            print(f\"Updated Centroid with respect to x{i+1}: {rhino_centroid} \\n\")\n",
    "        print(f\"The final Centroids are: {self.centroids}\\n\")\n",
    "    \n",
    "    def classify_samples(self):\n",
    "        \"\"\"Prints what cluster each sample belongs to.\n",
    "            NOTE Only works in 'without norm' mode\n",
    "        \"\"\" \n",
    "        if self.mode == 'without norm':\n",
    "            for j, sample in enumerate(self.dataset):\n",
    "                minimum_distance = None\n",
    "                closest_centroid_index = None\n",
    "                for i, centroid in enumerate(self.centroids):\n",
    "                    dist = np.linalg.norm(sample - centroid)\n",
    "                    if not minimum_distance or minimum_distance > dist:\n",
    "                        minimum_distance = dist\n",
    "                        closest_centroid_index = i\n",
    "                print(f\"Sample x{j+1} {sample} belongs to cluster c{closest_centroid_index+1} {self.centroids[closest_centroid_index]}\")\n",
    "            print(\"\\n\")\n",
    "        elif self.mode == 'with norm':\n",
    "            print(\"This feature is not available in mode \\\"with normalisation\\\". No examples were given.\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    \n",
    "    def classify_new_data(self):\n",
    "        \"\"\"Asks the user to input a new sample to classify and says what cluster it belongs to.\n",
    "            NOTE Only works in 'without norm' mode\n",
    "        \"\"\"        \n",
    "        if self.mode == 'without norm':\n",
    "            print(\"Please insert the new sample to classify. Do not insert spaces and divide its coordinates with a coma\")\n",
    "            new_sample = np.array([float(c) for c in str(input()).split(',')])\n",
    "            print('\\n')\n",
    "            minimum_distance = None\n",
    "            closest_centroid_index = None\n",
    "            for i, centroid in enumerate(self.centroids):\n",
    "                dist = np.linalg.norm(new_sample - centroid)\n",
    "                if not minimum_distance or minimum_distance > dist:\n",
    "                    minimum_distance = dist\n",
    "                    closest_centroid_index = i\n",
    "            print(f\"The new sample {new_sample} belongs to cluster c{closest_centroid_index+1} {self.centroids[closest_centroid_index]}\")\n",
    "        elif self.mode == 'with norm':\n",
    "            print(\"This feature is not available in mode \\\"with normalisation\\\". No examples were given.\")\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Competitive Algorithm. NOTE youll be asked to enter all the parameters when executing the script\n",
    "    mode, dataset, clusters, lr, order_of_indexes = run_competitive_learning_setup()\n",
    "    cluster = CompetitiveLearning(mode, dataset, clusters, lr, order_of_indexes)\n",
    "    cluster.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def run_hierarchical_clustering_setup():\n",
    "    \"\"\" Ask the user to enter the needed parameters for hierarchical clustering\n",
    "    Returns:\n",
    "        [int, np.array, string]: c -> the number of classes to cluster.\n",
    "                             dataset -> The dataset to cluster.\n",
    "                             similarity_method -> the distancing method to use.\n",
    "    \"\"\"    \n",
    "    print(\"Please enter the number of clusters you want to divide the dataset in and then press ENTER: \")\n",
    "    c = int(input())\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the dataset. Each sample should be divided by a space and each coordinate within a sample should be divided by a coma. \\n Be careful not to enter spaces after the coma that separates the samples' coordinates.\")\n",
    "    print(\"I.E.: x1=[1,2], x2=[-3,4], x3=[5,3] would be ---> 1,2 -3,4 5,3\")\n",
    "    dataset_input = str(input())\n",
    "    print(\"\\n\")\n",
    "    dataset = []\n",
    "    samples = dataset_input.split(' ')\n",
    "    for sample in samples:\n",
    "        coordinates = sample.split(',')\n",
    "        coordinates = [float(c) for c in coordinates]\n",
    "        dataset.append(coordinates)\n",
    "    print(\"Please type the number corresponding to the similarity method to use and press ENTER:\")\n",
    "    print(\"1) Single-link\")\n",
    "    print(\"2) Complete-link\")\n",
    "    print(\"3) Group-average\")\n",
    "    print(\"4) Centroid\")\n",
    "    similarity_method = int(input())\n",
    "    print(\"\\n\")\n",
    "    similarity_options = {1: \"single-link\", 2:\"complete-link\", 3:\"group-average\", 4: \"centroid\"}\n",
    "    similarity_method = similarity_options.get(similarity_method)\n",
    "    return c, dataset, similarity_method\n",
    "\n",
    "\n",
    "class HierarchicalClustering():\n",
    "    \"\"\" This class can be used to execute problems regarding Hierarchical Clustering\n",
    "    \"\"\"    \n",
    "    def __init__(self, n_classes, dataset, similarity_method=\"single-link\"):\n",
    "        self.n = n_classes # Number of classes that need to be found\n",
    "        self.dataset = dataset # Dataset of type np.array([[x11, x12], [x12, x22],...,[x1n, x2n]])\n",
    "        self.similarity_method = similarity_method # either \"single-link\", \"complete-link\", \"group-average\", or \"centroid\"\n",
    "        self.clusters = [np.array([s]) for s in self.dataset] # The initial clusters of type np.array([[x11, x12], [x12, x22],...,[x1n, x2n]])\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run hierarchical clustering on the dataset to find self.n number of clusters.\n",
    "        \"\"\"        \n",
    "        iteration = 1\n",
    "        while(len(self.clusters) != self.n):\n",
    "            closest_clusters = None\n",
    "            closest_distance = None\n",
    "            # Find the two closest clusters\n",
    "            for i, cluster in enumerate(self.clusters):\n",
    "                closest_cluster_to_i, distance_to_i = self.find_closest_cluster(i) #TODO\n",
    "                if not closest_distance or distance_to_i < closest_distance:\n",
    "                    closest_clusters = [i, closest_cluster_to_i]\n",
    "                    closest_distance = distance_to_i\n",
    "            \n",
    "            # Merge closest_clusters\n",
    "            merged_clusters = [self.clusters[closest_clusters[0]], self.clusters[closest_clusters[1]]]\n",
    "            self.merge_clusters(closest_clusters[0], closest_clusters[1])\n",
    "            self.print_iteration(iteration, merged_clusters, closest_distance)\n",
    "            iteration += 1\n",
    "        self.print_final()\n",
    "\n",
    "    def find_closest_cluster(self, cluster_index):\n",
    "        \"\"\"Find the the cluster closest to self.clusters[cluster_index] according to a given similarity method.\n",
    "        Args:\n",
    "            cluster_index (int): The index of the cluster in self.cluster\n",
    "        Returns:\n",
    "            ((int, float)): A pair containing the index of the cluster closest to self.clusters[cluster_index] and its distance.\n",
    "        \"\"\"        \n",
    "        closest_cluster_index = None\n",
    "        closest_distance = None\n",
    "        for i, cluster in enumerate(self.clusters):\n",
    "            similarity_method_options = {\"single-link\": lambda: self.get_single_link_distance(self.clusters[cluster_index], self.clusters[i]),\n",
    "                                            \"complete-link\": lambda: self.get_complete_link_distance(self.clusters[cluster_index], self.clusters[i]),\n",
    "                                            \"group-average\": lambda: self.get_average_link_distance(self.clusters[cluster_index], self.clusters[i]),\n",
    "                                            \"centroid\": lambda: self.get_centroid_distance(self.clusters[cluster_index], self.clusters[i])}\n",
    "            if cluster_index != i:\n",
    "                func = similarity_method_options.get(self.similarity_method, lambda: \"Invalid\")\n",
    "                distance = func()\n",
    "                if not closest_distance or distance < closest_distance:\n",
    "                    closest_cluster_index = i\n",
    "                    closest_distance = distance\n",
    "\n",
    "        return closest_cluster_index, closest_distance\n",
    "\n",
    "\n",
    "            \n",
    "    def get_single_link_distance(self, cluster_a, cluster_b):\n",
    "        minimum_distance = None\n",
    "        for a in cluster_a:\n",
    "            for b in cluster_b:\n",
    "                dist = np.linalg.norm(a-b)\n",
    "                if not minimum_distance or minimum_distance > dist:\n",
    "                    minimum_distance = dist\n",
    "        return minimum_distance \n",
    "\n",
    "    def get_complete_link_distance(self, cluster_a, cluster_b):\n",
    "        maximum_distance = None\n",
    "        for a in cluster_a:\n",
    "            for b in cluster_b:\n",
    "                dist = np.linalg.norm(a-b)\n",
    "                if not maximum_distance or maximum_distance < dist:\n",
    "                    maximum_distance = dist\n",
    "        return maximum_distance \n",
    "    \n",
    "    def get_average_link_distance(self, cluster_a, cluster_b):\n",
    "        distances = np.array([])\n",
    "        for a in cluster_a:\n",
    "            for b in cluster_b:\n",
    "                dist = np.linalg.norm(a-b)\n",
    "                distances = np.append(distances, np.array([dist]))\n",
    "        return np.average(distances) \n",
    "\n",
    "    def get_centroid_distance(self, cluster_a, cluster_b):\n",
    "        centroid_a = np.average(cluster_a) \n",
    "        centroid_b = np.average(cluster_b) \n",
    "        dist = np.linalg.norm(centroid_a - centroid_b)\n",
    "        return dist \n",
    "    \n",
    "    def merge_clusters(self, cluster_index_a, cluster_index_b):\n",
    "        merged_cluster = np.concatenate((self.clusters[cluster_index_a], self.clusters[cluster_index_b]))\n",
    "        self.clusters = [self.clusters[i] for i in range(0, len(self.clusters)) if i != cluster_index_a and i != cluster_index_b]\n",
    "        self.clusters.append(merged_cluster)\n",
    "\n",
    "    def print_iteration(self, i, merged_clusters, distance):\n",
    "        print(f\"End of iteration {str(i)} \" )\n",
    "        print(f\"Merged clusters {merged_clusters[0]} and {merged_clusters[1]}. Distance between the clusters was {distance}.\")\n",
    "        print(f\"There are {str(len(self.clusters))} clusters:\")\n",
    "        for j, c in enumerate(self.clusters):\n",
    "            print(f\"Cluster {j} -> {self.clusters[j]}\")\n",
    "        print (\"\\n\")\n",
    "    \n",
    "    def print_final(self):\n",
    "        print(\"FINAL CLUSTERS:\")\n",
    "        for j, c in enumerate(self.clusters):\n",
    "            print(f\"Cluster {j} -> {self.clusters[j]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Hierarchical Clustering Exercise. NOTE youll be asked to enter all the parameters when executing the script\n",
    "    c, dataset, similarity_method = run_hierarchical_clustering_setup()\n",
    "    cluster = HierarchicalClustering(c, dataset, similarity_method)\n",
    "    cluster.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialisation - Parameters from question\n",
    "k = 2 # Number of clusters to form\n",
    "centers = {  # Initial clusters' centers\n",
    "    0: [-1, 3],\n",
    "    1: [5, 1],\n",
    "}\n",
    "\n",
    "x = np.array([  # Dataset\n",
    "    [-1, 3],\n",
    "    [1, 4],\n",
    "    [0, 5],\n",
    "    [4, -1],\n",
    "    [3, 0],\n",
    "    [5, 1]\n",
    "])\n",
    "\n",
    "try:\n",
    "    assert(k == len(centers))\n",
    "except AssertionError as e:\n",
    "    e.args += ('The number of initialised clusers doesn\\'t match k',\n",
    "               len(centers), k)\n",
    "    raise\n",
    "\n",
    "print(\n",
    "    f'The parameters are: k = {k} and the centers are {centers}')\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def distance(feature, centers, method='euclidian'):\n",
    "    if method == 'euclidian':\n",
    "        return [np.linalg.norm(feature-centers[center])  # euclidian norm\n",
    "                for center in centers]\n",
    "    if method == 'manhatan':\n",
    "        return [np.linalg.norm(feature-centers[center], 1)  # manhatan dist\n",
    "                for center in centers]\n",
    "\n",
    "\n",
    "previous = {}\n",
    "for i in range(5):  # Max number of iterations (I don't think we'd be expected to perform more than 5 iterations)\n",
    "    print(f'\\n Iteration number {i+1}: \\n')\n",
    "    classes = {}  # Dict to hold a list of data points that are closest to the cluster number\n",
    "    for j in range(k):\n",
    "        classes[j] = []  # Instantiate empty list at every iteration\n",
    "    for feature in x:  # For each data point do:\n",
    "        # Compute the distance to each cluster (options: euclidian or manhatan)\n",
    "        distances = distance(feature, centers, 'euclidian')\n",
    "\n",
    "        classification = distances.index(\n",
    "            min(distances))  # Find the lowest distance\n",
    "        # Assign the datapoint to that cluster\n",
    "        classes[classification].append(feature)\n",
    "\n",
    "    # Print to which cluster each data point belongs to\n",
    "    print(f'The classification is: {classes}')\n",
    "\n",
    "    previous = centers.copy()  # Copy the cluster centers dict\n",
    "    print(f'The previous centers are:{previous}')\n",
    "    for classification in classes:\n",
    "        # Compute the new cluster average by taking the mean of all the data point assigned to that cluster\n",
    "        centers[classification] = np.average(classes[classification], axis=0)\n",
    "    print(f'The new centers are:{centers}')\n",
    "\n",
    "    opti = True\n",
    "    for center in centers:\n",
    "        prev = previous[center]  # Previous cluster\n",
    "        curr = centers[center]  # Update cluster\n",
    "        # termination criteria #TODO: IMPORTANT: This can be changed in the exam question!!\n",
    "        if np.sum(curr-prev) != 0:\n",
    "            opti = False\n",
    "    if opti:\n",
    "        break\n",
    "\n",
    "print('\\n The algorithm converged!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fuzzy Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def fuzzy_k_means(data, K, b, iterations, weights):\n",
    "    data = np.array(data)\n",
    "    weights = np.array(weights)\n",
    "    centers = [None] * K\n",
    "    # centers = np.array(centers)\n",
    "    data = np.array(data)\n",
    "    for _iter in range(iterations):\n",
    "        # calculate centers\n",
    "        for idx, w in enumerate(weights):\n",
    "            new_center = [None] * data.shape[1]\n",
    "            for idy in range(data.shape[1]):\n",
    "                new_center[idy] = w[idy]**b * data[:, idy]\n",
    "            centers[idx] = (np.array(new_center).sum(axis=0)/(w**b).sum())\n",
    "        # print(\"Centers\", centers)\n",
    "        new_weight_container = [None] * data.shape[1]\n",
    "        for idx in range(data.shape[1]):\n",
    "            new_weights = []\n",
    "            for c in centers:\n",
    "                val = np.linalg.norm(c - data[:, idx])\n",
    "\n",
    "                val = (1 / val) ** (2/(b-1))\n",
    "                new_weights.append(val)\n",
    "            new_weights = np.array(new_weights)\n",
    "            nw_sum = new_weights.sum()\n",
    "            for idy, nw in enumerate(new_weights):\n",
    "                new_weights[idy] = nw/nw_sum\n",
    "            new_weight_container[idx] = new_weights\n",
    "\n",
    "        new_weight_container = np.array(new_weight_container)\n",
    "        for col_idx in range(new_weight_container.shape[1]):\n",
    "            weights[col_idx] = new_weight_container[:, col_idx]\n",
    "        print(F\"CENTERS AFTER ITERATION {_iter+1} [READ COLUMN WISE]\")\n",
    "        print(np.array(centers).T)\n",
    "        print(\"#\"*100)\n",
    "        print(f\"WEIGHTS AFTER ITERATION {_iter + 1} [READ COLUMN WISE]\")\n",
    "        print(weights.T)\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE A VALUES AS GIVEN IN THE QUESTION\n",
    "data = [[-1, 1, 0, 4, 3, 5], [3, 4, 5, -1, 0, 1]]\n",
    "fuzzy_k_means(data=data, K=2, weights=[\n",
    "              [1, 0.5, 0.5, 0.5, 0.5, 0], [0, 0.5, 0.5, 0.5, 0.5, 1]], b=2, iterations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leader-Follower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def run_leader_follower_setup():\n",
    "    \"\"\" Ask the user to enter the needed parameters\n",
    "    Returns:\n",
    "        [string, np.array, float, float, np.array]: \n",
    "                             mode -> Specify if using normalisation or not.\n",
    "                             dataset -> The dataset to cluster.\n",
    "                             lr -> The learning Rate \n",
    "                             theta -> The threshold theta.\n",
    "                             order_of_indexes -> The order to follow when selecting samples in the algorithm\n",
    "    \"\"\"    \n",
    "    print(\"How you want to run the algorithm? (Enter the corresponding number and press ENTER)\")\n",
    "    print(\"1) With normalisation and argumentation\")\n",
    "    print(\"2) Without normalisation and argumentation\")\n",
    "    mode = int(input())\n",
    "    algorithm_variants = {1: \"with norm\", 2:\"without norm\"}\n",
    "    mode = algorithm_variants.get(mode)\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the dataset. Each sample should be divided by a space and each coordinate within a sample should be divided by a coma. \\nBe careful not to enter spaces after the coma that separates the samples' coordinates.\\n\")\n",
    "    print(\"I.E.: x1=[1,2], x2=[-3,4], x3=[5,3] would be ---> 1,2 -3,4 5,3 \\n\")\n",
    "    dataset_input = str(input())\n",
    "    print(\"\\n\")\n",
    "    dataset = np.array([]).reshape(0,2)\n",
    "    samples = dataset_input.split(' ')\n",
    "    for sample in samples:\n",
    "        coordinates = sample.split(',')\n",
    "        coordinates = np.array([float(c) for c in coordinates])\n",
    "        dataset = np.concatenate((dataset, [coordinates]))\n",
    "    print(\"Please insert the value of the learning rate and then press ENTER (make sure it is a float number - I.E. 0.1)\")\n",
    "    lr = float(input())\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the value of the the threshold theta and then press ENTER (make sure it is a float number - I.E. 2.0)\")\n",
    "    theta = float(input())\n",
    "    print(\"\\n\")\n",
    "    print(\"Please insert the order to follow when selecting the samples within the algorithm. \\nYou should provide a sequential list of the indexes of the samples to select separated by single spaces.\")\n",
    "    print(\"I.E. 1 2 1 5 3\")\n",
    "    print(\"NOTE: indexes start at 1!\")\n",
    "    order_of_indexes = [ int(o) - 1 for o in str(input()).split(' ')]\n",
    "    print (\"\\n\")\n",
    "    return mode, dataset, lr, theta, order_of_indexes\n",
    "\n",
    "\n",
    "class LeaderFollower:\n",
    "    \"\"\" This class can be used to execute problems regarding Leader Follower\n",
    "    \"\"\" \n",
    "    def __init__(self, mode, dataset, lr, theta, order_of_samples):\n",
    "        self.mode = mode # Indicates wether to use normalisation or not. Either \"with norm\" or \"without norm\"\n",
    "        self.dataset = dataset # Dataset of type np.array([[x11, x12], [x12, x22],...,[x1n, x2n]])\n",
    "        self.lr = lr # Learning rate. Must be a float > 0\n",
    "        self.theta = theta # The threshold to use in the algorithm.\n",
    "        self.order_of_indexes = order_of_indexes # Indicates what order to select the samples in the algorithm. \n",
    "                                                # Of type np.array([int, int, int]) where each int is the corresponding index to the sample in self.dataset\n",
    "        self.centroids = np.array([]) # Empty np.array of initial centroids\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Runs the algorithm on the samples listed in self.order_of_samples.\n",
    "            After that, the user can select some more operations to do with the updated clusters.\n",
    "        \"\"\"        \n",
    "        if self.mode == \"with norm\":\n",
    "            self.run_with_normalisation()\n",
    "        elif self.mode == \"without norm\":\n",
    "            self.run_without_normalisation()\n",
    "        user_input = 0\n",
    "        while user_input != 3:\n",
    "            # While the user doesnt selects exit show some options.\n",
    "            # NOTE only applicable when in \"without norm\" mode.\n",
    "            if user_input == 1:\n",
    "                self.classify_samples()\n",
    "            elif user_input == 2:\n",
    "                self.classify_new_data()\n",
    "\n",
    "            print(\"What do you want to do now?\")\n",
    "            print(\"1) Classify all the existing samples\")\n",
    "            print(\"2) Classify a new sample\")\n",
    "            print(\"3) Exit\")\n",
    "            user_input = int(input())\n",
    "\n",
    "    def run_without_normalisation(self):\n",
    "        # Shape the list of centroid to the right shape\n",
    "        self.centroids = np.array([]).reshape(0, self.dataset.shape[1])\n",
    "        # Initialise first centroid\n",
    "        self.centroids = np.concatenate( (self.centroids, [self.dataset[self.order_of_indexes[0]]]) )\n",
    "        for iteration, i in enumerate(self.order_of_indexes):\n",
    "            x = self.dataset[i]\n",
    "            print(f\"Iteration {iteration}:\")\n",
    "            print(f\"Selected x{i+1} {x}\")\n",
    "\n",
    "            distances_to_centroids = np.array([np.linalg.norm(x - c) for c in self.centroids])\n",
    "            print(f\"Distances to each centroids are {distances_to_centroids}\")\n",
    "            j = distances_to_centroids.argmin()\n",
    "            rhino_centroid = self.centroids[j]\n",
    "            print(f\"The closest centroid is c{j+1} {rhino_centroid}\")\n",
    "\n",
    "            if np.linalg.norm(x - rhino_centroid) < self.theta:\n",
    "                print(f\"C{j+1} is within the threshold\")\n",
    "                self.centroids[j] = np.add( rhino_centroid, np.multiply(self.lr, np.subtract(x, rhino_centroid)) )\n",
    "                print(f\"Updated centroid c{j+1} to be {self.centroids[j]}\\n\")\n",
    "            else:\n",
    "                print(f\"C{j+1} is not within the threshold\")\n",
    "                self.centroids = np.concatenate( (self.centroids, [x]) )\n",
    "                print(f\"Added new Centroid {x}\\n\")\n",
    "        \n",
    "        print(f\"The final Centroids are: {self.centroids}\\n\")\n",
    "\n",
    "    def run_with_normalisation(self):\n",
    "\n",
    "        augmented_dataset = np.array([np.insert(sample, 0, 1) for sample in self.dataset])\n",
    "        normalised_dataset = np.array([np.divide(sample, np.linalg.norm(sample)) for sample in augmented_dataset])\n",
    "\n",
    "        print(f\"The augmented dataset is :{augmented_dataset}\")\n",
    "        print(f\"The normalised dataset is :{normalised_dataset}\\n\")\n",
    "\n",
    "        # Shape the list of centroid to the right shape (it must be augmented)\n",
    "        self.centroids = np.array([]).reshape(0, normalised_dataset.shape[1])\n",
    "\n",
    "        # Initialise first centroid\n",
    "        self.centroids = np.concatenate( (self.centroids, [normalised_dataset[self.order_of_indexes[0]]]) )\n",
    "\n",
    "        for iteration, i in enumerate(self.order_of_indexes):\n",
    "            x = normalised_dataset[i]\n",
    "            print(f\"Iteration {iteration + 1}:\")\n",
    "            print(f\"Selected x{i+1} {self.dataset[i]} which normalised is --> {x}\")\n",
    "\n",
    "            net_inner_products = np.array([np.multiply(c.transpose(), x) for c in self.centroids])\n",
    "            print(f\"The inner products to each centroid with respect to x{i+1} are {net_inner_products}\")\n",
    "            j = np.argmax(np.sum(net_inner_products, axis=1))\n",
    "            rhino_centroid = self.centroids[j]\n",
    "            print(f\"The closest centroid is c{j+1} {rhino_centroid}\")\n",
    "\n",
    "            if np.linalg.norm(x - rhino_centroid) < self.theta:\n",
    "                print(f\"C{j+1} is within the threshold, as {np.linalg.norm(x - rhino_centroid)} < {self.theta}\")\n",
    "                rhino_centroid = np.add(rhino_centroid, np.multiply([self.lr], x)) # Update cluster center\n",
    "                print(f\"Updated Centroid C{j+1} with respect to x{i+1}: {rhino_centroid}\") \n",
    "                rhino_centroid = np.divide(rhino_centroid, np.linalg.norm(rhino_centroid)) # Normalise updated cluster center\n",
    "                print(f\"Normalised Centroid c{j+1} with respect to x{i+1}: {rhino_centroid} \\n\")\n",
    "                self.centroids[j] = rhino_centroid # Actually update the new cetroid\n",
    "            else:\n",
    "                print(f\"C{j+1} is not within the threshold, as {np.linalg.norm(x - rhino_centroid)} > {self.theta}\")\n",
    "                new_centroid = np.divide(x, np.linalg.norm(x)) # Create new centroid\n",
    "                self.centroids = np.concatenate( (self.centroids, [new_centroid]) )\n",
    "                print(f\"Added new centroid c{len(self.centroids - 1)} {new_centroid}\")\n",
    "        \n",
    "        print(f\"The final Centroids are: {self.centroids}\\n\")\n",
    "\n",
    "    def classify_samples(self):\n",
    "        if self.mode == 'without norm':\n",
    "            for j, sample in enumerate(self.dataset):\n",
    "                minimum_distance = None\n",
    "                closest_centroid_index = None\n",
    "                for i, centroid in enumerate(self.centroids):\n",
    "                    dist = np.linalg.norm(sample - centroid)\n",
    "                    if not minimum_distance or minimum_distance > dist:\n",
    "                        minimum_distance = dist\n",
    "                        closest_centroid_index = i\n",
    "                print(f\"Sample x{j+1} {sample} belongs to cluster c{closest_centroid_index+1} {self.centroids[closest_centroid_index]}\")\n",
    "            print(\"\\n\")\n",
    "        elif self.mode == 'with norm':\n",
    "            print(\"This feature is not available in mode \\\"with normalisation\\\". No examples were given.\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    \n",
    "    def classify_new_data(self):\n",
    "        if self.mode == 'without norm':\n",
    "            print(\"Please insert the new sample to classify. Do not insert spaces and divide its coordinates with a coma\")\n",
    "            new_sample = np.array([float(c) for c in str(input()).split(',')])\n",
    "            print('\\n')\n",
    "            minimum_distance = None\n",
    "            closest_centroid_index = None\n",
    "            for i, centroid in enumerate(self.centroids):\n",
    "                dist = np.linalg.norm(new_sample - centroid)\n",
    "                if not minimum_distance or minimum_distance > dist:\n",
    "                    minimum_distance = dist\n",
    "                    closest_centroid_index = i\n",
    "            print(f\"The new sample {new_sample} belongs to cluster c{closest_centroid_index+1} {self.centroids[closest_centroid_index]}\")\n",
    "        elif self.mode == 'with norm':\n",
    "            print(\"This feature is not available in mode \\\"with normalisation\\\". No examples were given.\")\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    # Leader follower algorithm.  NOTE youll be asked to enter all the parameters when executing the script\n",
    "    mode, dataset, lr, theta, order_of_indexes = run_leader_follower_setup()\n",
    "    cluster = LeaderFollower(mode, dataset, lr, theta, order_of_indexes)\n",
    "    cluster.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "\n",
    "def _PCA(ip, n_components, data_to_project=None):\n",
    "    ip = np.array(ip)\n",
    "    ip_mean = np.mean(ip, axis=1)\n",
    "    ip_prime = ip - np.vstack(ip_mean)\n",
    "    C = (ip_prime @ ip_prime.T) / ip.shape[1]\n",
    "    V, D, VT = svd(C)\n",
    "    ans = VT @ ip_prime\n",
    "    print(\"-\"*100)\n",
    "    print(\"READ THE ROWS FROM THE TOP\")\n",
    "    print(ans[:n_components])\n",
    "    print(\"-\"*100)\n",
    "    if data_to_project:\n",
    "        data_to_project = np.array(data_to_project)\n",
    "        print(\"-\"*100)\n",
    "        print(f\"PROJECTION OF {data_to_project}\")\n",
    "        print((VT@data_to_project)[:n_components])\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE ACCORDING TO THE QUESTION\n",
    "ip = [[4, 0, 2, -2], [2, -2, 4, 0], [2, 2, 2, 2]]\n",
    "n_components = 2\n",
    "data_to_project = [3, -2, 5]\n",
    "_PCA(ip=ip, n_components=n_components, data_to_project=data_to_project)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
